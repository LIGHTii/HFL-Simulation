#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŒé‡è¯„ä¼°ç»“æœå¯è§†åŒ–å·¥å…·
æä¾›æœ¬åœ°vså…¨å±€æ€§èƒ½å¯¹æ¯”çš„é«˜çº§å¯è§†åŒ–åŠŸèƒ½
"""

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import rcParams
import os

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def create_dual_evaluation_plots(dual_eval_history, save_dir='./save/', timestamp=''):
    """
    åˆ›å»ºåŒé‡è¯„ä¼°çš„ç»¼åˆå¯è§†åŒ–å›¾è¡¨
    
    Args:
        dual_eval_history: åŒé‡è¯„ä¼°å†å²æ•°æ®
        save_dir: ä¿å­˜ç›®å½•
        timestamp: æ—¶é—´æˆ³
    """
    if not dual_eval_history:
        print("è­¦å‘Š: æ²¡æœ‰åŒé‡è¯„ä¼°æ•°æ®")
        return
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)
    
    # æå–æ•°æ®
    epochs = [result['epoch'] for result in dual_eval_history]
    model_names = list(dual_eval_history[0]['global_performance'].keys())
    
    # åˆ›å»ºå¤§å›¾è¡¨
    fig = plt.figure(figsize=(20, 16))
    
    # å®šä¹‰é¢œè‰²æ–¹æ¡ˆ
    colors = {
        'HFL_Random_B': '#1f77b4',   # è“è‰²
        'HFL_Cluster_B': '#ff7f0e',  # æ©™è‰²  
        'SFL': '#2ca02c'             # ç»¿è‰²
    }
    
    # å®šä¹‰ä¸­æ–‡æ ‡ç­¾
    labels_zh = {
        'HFL_Random_B': 'HFL(éšæœºBçŸ©é˜µ)',
        'HFL_Cluster_B': 'HFL(èšç±»BçŸ©é˜µ)',
        'SFL': 'SFL(å•å±‚)'
    }
    
    # 1. å…¨å±€vsæœ¬åœ°å‡†ç¡®ç‡å¯¹æ¯” (2x3 å¸ƒå±€)
    for i, model_name in enumerate(model_names):
        ax = plt.subplot(2, 3, i+1)
        
        # æå–è¯¥æ¨¡å‹çš„æ•°æ®
        global_accs = [r['global_performance'][model_name]['accuracy'] for r in dual_eval_history]
        local_mean_accs = [r['local_performance'][model_name]['mean_accuracy'] for r in dual_eval_history if model_name in r['local_performance']]
        local_std_accs = [r['local_performance'][model_name]['std_accuracy'] for r in dual_eval_history if model_name in r['local_performance']]
        
        if local_mean_accs:
            local_mean_accs = np.array(local_mean_accs)
            local_std_accs = np.array(local_std_accs)
            
            # ç»˜åˆ¶å…¨å±€å‡†ç¡®ç‡çº¿
            ax.plot(epochs, global_accs, 'o-', color=colors[model_name], 
                   linewidth=2.5, markersize=6, label='å…¨å±€æµ‹è¯•', alpha=0.9)
            
            # ç»˜åˆ¶æœ¬åœ°å¹³å‡å‡†ç¡®ç‡çº¿å’Œç½®ä¿¡åŒºé—´
            ax.plot(epochs, local_mean_accs, 's--', color=colors[model_name], 
                   linewidth=2, markersize=5, label='æœ¬åœ°å¹³å‡', alpha=0.7)
            ax.fill_between(epochs, local_mean_accs - local_std_accs, 
                           local_mean_accs + local_std_accs,
                           color=colors[model_name], alpha=0.2, label='æœ¬åœ°æ–¹å·®')
        
        ax.set_title(f'{labels_zh[model_name]} - å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax.set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
        ax.set_ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 100)
    
    # 2. æ€§èƒ½å·®è·åˆ†æ (2x3 å¸ƒå±€)
    for i, model_name in enumerate(model_names):
        ax = plt.subplot(2, 3, i+4)
        
        # æå–æ€§èƒ½å·®è·æ•°æ®
        acc_gaps = [r['performance_comparison'][model_name]['accuracy_gap'] for r in dual_eval_history if model_name in r['performance_comparison']]
        local_variances = [r['performance_comparison'][model_name]['local_variance_acc'] for r in dual_eval_history if model_name in r['performance_comparison']]
        
        if acc_gaps:
            # ç»˜åˆ¶å‡†ç¡®ç‡å·®è·
            ax.bar(epochs, acc_gaps, color=colors[model_name], alpha=0.7, 
                  label='å…¨å±€-æœ¬åœ°å·®è·', width=0.8)
            
            # æ·»åŠ é›¶çº¿
            ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)
            
            # ç»˜åˆ¶æœ¬åœ°æ–¹å·®ï¼ˆä½œä¸ºè¯¯å·®çº¿ï¼‰
            ax2 = ax.twinx()
            ax2.plot(epochs, local_variances, 'ro-', alpha=0.6, markersize=4, 
                    label='å®¢æˆ·ç«¯é—´æ–¹å·®')
            ax2.set_ylabel('æœ¬åœ°æ–¹å·® (%)', fontsize=10, color='red')
        
        ax.set_title(f'{labels_zh[model_name]} - æ€§èƒ½å·®è·', fontsize=14, fontweight='bold')
        ax.set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
        ax.set_ylabel('å‡†ç¡®ç‡å·®è· (%)', fontsize=12)
        ax.legend(loc='upper left', fontsize=10)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/dual_evaluation_analysis_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.savefig(f'{save_dir}/dual_evaluation_analysis_{timestamp}.pdf', bbox_inches='tight')
    print(f"åŒé‡è¯„ä¼°åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {save_dir}/dual_evaluation_analysis_{timestamp}.png")
    plt.close()


def create_client_performance_heatmap(dual_eval_history, save_dir='./save/', timestamp=''):
    """
    åˆ›å»ºå®¢æˆ·ç«¯æ€§èƒ½çƒ­åŠ›å›¾
    
    Args:
        dual_eval_history: åŒé‡è¯„ä¼°å†å²æ•°æ®
        save_dir: ä¿å­˜ç›®å½•
        timestamp: æ—¶é—´æˆ³
    """
    if not dual_eval_history:
        return
    
    # åªåˆ†ææœ€åå‡ è½®çš„æ•°æ®
    recent_results = dual_eval_history[-3:] if len(dual_eval_history) > 3 else dual_eval_history
    model_names = list(dual_eval_history[0]['global_performance'].keys())
    
    fig, axes = plt.subplots(1, len(model_names), figsize=(6*len(model_names), 8))
    if len(model_names) == 1:
        axes = [axes]
    
    for idx, model_name in enumerate(model_names):
        # æ”¶é›†æ‰€æœ‰å®¢æˆ·ç«¯çš„æ€§èƒ½æ•°æ®
        all_client_data = []
        
        for result in recent_results:
            if model_name in result['local_performance']:
                individual_results = result['local_performance'][model_name]['individual_results']
                for client_result in individual_results:
                    all_client_data.append({
                        'epoch': result['epoch'],
                        'client_id': client_result['client_id'],
                        'accuracy': client_result['accuracy'],
                        'test_size': client_result['test_size']
                    })
        
        if all_client_data:
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(all_client_data)
            
            # åˆ›å»ºæ•°æ®é€è§†è¡¨
            pivot_table = df.pivot(index='client_id', columns='epoch', values='accuracy')
            
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            sns.heatmap(pivot_table, annot=True, fmt='.1f', cmap='RdYlBu_r',
                       ax=axes[idx], cbar_kws={'label': 'å‡†ç¡®ç‡ (%)'})
            
            axes[idx].set_title(f'{model_name} - å®¢æˆ·ç«¯æ€§èƒ½çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
            axes[idx].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
            axes[idx].set_ylabel('å®¢æˆ·ç«¯ID', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/client_performance_heatmap_{timestamp}.png', dpi=300, bbox_inches='tight')
    print(f"å®¢æˆ·ç«¯æ€§èƒ½çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {save_dir}/client_performance_heatmap_{timestamp}.png")
    plt.close()


def create_performance_summary_report(dual_eval_history, save_dir='./save/', timestamp=''):
    """
    åˆ›å»ºæ€§èƒ½æ€»ç»“æŠ¥å‘Š
    
    Args:
        dual_eval_history: åŒé‡è¯„ä¼°å†å²æ•°æ®
        save_dir: ä¿å­˜ç›®å½•
        timestamp: æ—¶é—´æˆ³
    """
    if not dual_eval_history:
        return
    
    # åˆ†ææœ€ç»ˆæ€§èƒ½
    final_result = dual_eval_history[-1]
    model_names = list(final_result['global_performance'].keys())
    
    # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    global_accs = [final_result['global_performance'][model]['accuracy'] for model in model_names]
    local_accs = [final_result['local_performance'][model]['mean_accuracy'] for model in model_names if model in final_result['local_performance']]
    
    x = np.arange(len(model_names))
    width = 0.35
    
    ax1.bar(x - width/2, global_accs, width, label='å…¨å±€æµ‹è¯•', alpha=0.8)
    if local_accs:
        ax1.bar(x + width/2, local_accs, width, label='æœ¬åœ°å¹³å‡', alpha=0.8)
    
    ax1.set_xlabel('æ¨¡å‹ç±»å‹')
    ax1.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax1.set_title('æœ€ç»ˆæ€§èƒ½å¯¹æ¯”')
    ax1.set_xticks(x)
    ax1.set_xticklabels([name.replace('_', '\n') for name in model_names])
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. æ€§èƒ½å·®è·åˆ†æ
    acc_gaps = [final_result['performance_comparison'][model]['accuracy_gap'] for model in model_names if model in final_result['performance_comparison']]
    if acc_gaps:
        colors_list = ['#1f77b4', '#ff7f0e', '#2ca02c'][:len(model_names)]
        bars = ax2.bar(model_names, acc_gaps, color=colors_list, alpha=0.7)
        
        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        ax2.set_xlabel('æ¨¡å‹ç±»å‹')
        ax2.set_ylabel('å‡†ç¡®ç‡å·®è· (%)')
        ax2.set_title('å…¨å±€-æœ¬åœ°æ€§èƒ½å·®è·')
        ax2.set_xticklabels([name.replace('_', '\n') for name in model_names])
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, gap in zip(bars, acc_gaps):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    f'{gap:+.1f}%', ha='center', va='bottom')
    
    # 3. å®¢æˆ·ç«¯é—´æ–¹å·®åˆ†æ
    variances = [final_result['performance_comparison'][model]['local_variance_acc'] for model in model_names if model in final_result['performance_comparison']]
    if variances:
        ax3.bar(model_names, variances, color=colors_list, alpha=0.7)
        ax3.set_xlabel('æ¨¡å‹ç±»å‹')
        ax3.set_ylabel('å‡†ç¡®ç‡æ ‡å‡†å·® (%)')
        ax3.set_title('å®¢æˆ·ç«¯é—´æ€§èƒ½æ–¹å·®')
        ax3.set_xticklabels([name.replace('_', '\n') for name in model_names])
        ax3.grid(True, alpha=0.3)
    
    # 4. è®­ç»ƒè¿‡ç¨‹è¶‹åŠ¿
    epochs = [r['epoch'] for r in dual_eval_history]
    for model_name in model_names:
        global_trend = [r['global_performance'][model_name]['accuracy'] for r in dual_eval_history]
        ax4.plot(epochs, global_trend, 'o-', label=f'{model_name}(å…¨å±€)', linewidth=2)
    
    ax4.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax4.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax4.set_title('è®­ç»ƒè¿‡ç¨‹å‡†ç¡®ç‡è¶‹åŠ¿')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/performance_summary_report_{timestamp}.png', dpi=300, bbox_inches='tight')
    print(f"æ€§èƒ½æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_dir}/performance_summary_report_{timestamp}.png")
    plt.close()


def generate_dual_evaluation_report(dual_eval_history, save_dir='./save/', timestamp=''):
    """
    ç”Ÿæˆå®Œæ•´çš„åŒé‡è¯„ä¼°æŠ¥å‘Š
    
    Args:
        dual_eval_history: åŒé‡è¯„ä¼°å†å²æ•°æ®
        save_dir: ä¿å­˜ç›®å½•
        timestamp: æ—¶é—´æˆ³
    """
    print("\n" + "="*50)
    print("ç”ŸæˆåŒé‡è¯„ä¼°å¯è§†åŒ–æŠ¥å‘Š")
    print("="*50)
    
    if not dual_eval_history:
        print("è­¦å‘Š: æ²¡æœ‰åŒé‡è¯„ä¼°æ•°æ®å¯ä¾›åˆ†æ")
        return
    
    try:
        # 1. åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨
        create_dual_evaluation_plots(dual_eval_history, save_dir, timestamp)
        
        # 2. åˆ›å»ºå®¢æˆ·ç«¯æ€§èƒ½çƒ­åŠ›å›¾
        create_client_performance_heatmap(dual_eval_history, save_dir, timestamp)
        
        # 3. åˆ›å»ºæ€§èƒ½æ€»ç»“æŠ¥å‘Š
        create_performance_summary_report(dual_eval_history, save_dir, timestamp)
        
        # 4. ç”Ÿæˆæ•°å­—åŒ–åˆ†ææŠ¥å‘Š
        final_result = dual_eval_history[-1]
        
        print("\nğŸ“Š åŒé‡è¯„ä¼°æœ€ç»ˆæ€§èƒ½åˆ†æ:")
        print("-" * 40)
        
        for model_name in final_result['global_performance'].keys():
            global_perf = final_result['global_performance'][model_name]
            local_perf = final_result['local_performance'].get(model_name, {})
            comparison = final_result['performance_comparison'].get(model_name, {})
            
            print(f"\nğŸ”¸ {model_name}:")
            print(f"   å…¨å±€æµ‹è¯•å‡†ç¡®ç‡: {global_perf['accuracy']:.2f}%")
            if local_perf:
                print(f"   æœ¬åœ°å¹³å‡å‡†ç¡®ç‡: {local_perf['mean_accuracy']:.2f} Â± {local_perf['std_accuracy']:.2f}%")
                print(f"   æ€§èƒ½å·®è·: {comparison.get('accuracy_gap', 0):+.2f}%")
                print(f"   å®¢æˆ·ç«¯é—´æ–¹å·®: {comparison.get('local_variance_acc', 0):.2f}%")
        
        print("\n" + "="*50)
        print("åŒé‡è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        print(f"æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {save_dir}")
        print("="*50 + "\n")
        
    except Exception as e:
        print(f"ç”ŸæˆåŒé‡è¯„ä¼°æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("åŒé‡è¯„ä¼°å¯è§†åŒ–å·¥å…·æµ‹è¯•")