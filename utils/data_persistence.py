#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®¢æˆ·ç«¯æ•°æ®åˆ†é…ä¿å­˜å’ŒåŠ è½½å·¥å…·æ¨¡å—
ç”¨äºä¿å­˜å’Œå¤ç”¨å®¢æˆ·ç«¯æ•°æ®åˆ†é…æ–¹æ¡ˆ
"""

import os
import pickle
import json
import numpy as np
from datetime import datetime
import hashlib


def generate_data_config_hash(args):
    """
    ç”Ÿæˆæ•°æ®é…ç½®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºéªŒè¯æ•°æ®åˆ†é…æ–¹æ¡ˆçš„ä¸€è‡´æ€§
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
    
    Returns:
        str: é…ç½®çš„MD5å“ˆå¸Œå€¼
    """
    config_dict = {
        'dataset': args.dataset,
        'num_users': args.num_users,
        'partition': getattr(args, 'partition', 'noniid-labeldir'),
        'beta': getattr(args, 'beta', 0.1),
        'iid': getattr(args, 'iid', False),
        'use_sampling': getattr(args, 'use_sampling', False),
        'seed': args.seed,
        # æ–°å¢å½±å“å®¢æˆ·ç«¯æ•°é‡çš„ç½‘ç»œæ‹“æ‰‘å‚æ•°
        'graphml_file': getattr(args, 'graphml_file', None),
        'es_ratio': getattr(args, 'es_ratio', None),
        'max_capacity': getattr(args, 'max_capacity', None),
        # æ–°å¢å…¶ä»–å¯èƒ½å½±å“æ•°æ®åˆ’åˆ†çš„å‚æ•°
        'data_path': getattr(args, 'data_path', './data/'),
        'local_ep': getattr(args, 'local_ep', 5),
        'method': getattr(args, 'method', 'fedavg')
    }
    
    # å°†å­—å…¸è½¬æ¢ä¸ºæ’åºçš„å­—ç¬¦ä¸²
    config_str = json.dumps(config_dict, sort_keys=True)
    
    # ç”ŸæˆMD5å“ˆå¸Œ
    return hashlib.md5(config_str.encode()).hexdigest()


def save_client_data_distribution(dict_users, client_classes, args, custom_name=None):
    """
    ä¿å­˜å®¢æˆ·ç«¯æ•°æ®åˆ†é…ä¿¡æ¯åˆ°æ–‡ä»¶
    
    Args:
        dict_users: å®¢æˆ·ç«¯æ•°æ®ç´¢å¼•æ˜ å°„å­—å…¸
        client_classes: å®¢æˆ·ç«¯ç±»åˆ«ä¿¡æ¯å­—å…¸
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        custom_name: è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    save_dir = args.data_save_dir
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”Ÿæˆé…ç½®å“ˆå¸Œ
    config_hash = generate_data_config_hash(args)
    
    # ç”Ÿæˆæ–‡ä»¶å
    if custom_name:
        filename = f"{custom_name}.pkl"
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"client_data_{args.dataset}_{args.num_users}clients_{config_hash[:8]}_{timestamp}.pkl"
    
    filepath = os.path.join(save_dir, filename)
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'dict_users': dict_users,
        'client_classes': client_classes,
        'config_hash': config_hash,
        'config': {
            'dataset': args.dataset,
            'num_users': args.num_users,
            'partition': getattr(args, 'partition', 'noniid-labeldir'),
            'beta': getattr(args, 'beta', 0.1),
            'iid': getattr(args, 'iid', False),
            'use_sampling': getattr(args, 'use_sampling', False),
            'seed': args.seed,
            'save_timestamp': datetime.now().isoformat()
        },
        'metadata': {
            'total_clients': len(dict_users),
            'total_samples': sum(len(indices) for indices in dict_users.values()),
            'classes_per_client': {client_id: len(classes) for client_id, classes in client_classes.items()},
            'unique_classes': list(set(class_id for classes in client_classes.values() for class_id in classes))
        }
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    try:
        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        
        print(f"âœ… å®¢æˆ·ç«¯æ•°æ®åˆ†é…å·²ä¿å­˜åˆ°: {filepath}")
        print(f"   é…ç½®å“ˆå¸Œ: {config_hash}")
        print(f"   æ€»å®¢æˆ·ç«¯æ•°: {save_data['metadata']['total_clients']}")
        print(f"   æ€»æ ·æœ¬æ•°: {save_data['metadata']['total_samples']}")
        
        # ä¿å­˜ä¸€ä¸ªå¯è¯»çš„é…ç½®æ–‡ä»¶
        config_filepath = filepath.replace('.pkl', '_config.json')
        with open(config_filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data['config'], f, indent=2, ensure_ascii=False)
        
        return filepath
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å®¢æˆ·ç«¯æ•°æ®åˆ†é…å¤±è´¥: {str(e)}")
        return None


def load_client_data_distribution(filepath, args, verify_config=True):
    """
    ä»æ–‡ä»¶åŠ è½½å®¢æˆ·ç«¯æ•°æ®åˆ†é…ä¿¡æ¯
    
    Args:
        filepath: æ•°æ®æ–‡ä»¶è·¯å¾„
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        verify_config: æ˜¯å¦éªŒè¯é…ç½®ä¸€è‡´æ€§
    
    Returns:
        tuple: (dict_users, client_classes) æˆ– None (å¦‚æœåŠ è½½å¤±è´¥)
    """
    if not os.path.exists(filepath):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        return None, None
    
    try:
        with open(filepath, 'rb') as f:
            save_data = pickle.load(f)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        if not all(key in save_data for key in ['dict_users', 'client_classes', 'config']):
            print("âŒ æ•°æ®æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
            return None, None
        
        dict_users = save_data['dict_users']
        client_classes = save_data['client_classes']
        saved_config = save_data['config']
        
        print(f"âœ… æˆåŠŸåŠ è½½å®¢æˆ·ç«¯æ•°æ®åˆ†é…: {filepath}")
        print(f"   æ•°æ®é›†: {saved_config['dataset']}")
        print(f"   å®¢æˆ·ç«¯æ•°: {saved_config['num_users']}")
        print(f"   åˆ†åŒºæ–¹æ³•: {saved_config['partition']}")
        print(f"   ä¿å­˜æ—¶é—´: {saved_config.get('save_timestamp', 'Unknown')}")
        
        # è‡ªåŠ¨ä½¿ç”¨ä¿å­˜æ•°æ®æ—¶çš„éšæœºç§å­ï¼Œç¡®ä¿å®éªŒä¸€è‡´æ€§
        if 'seed' in saved_config and saved_config['seed'] != args.seed:
            old_seed = args.seed
            args.seed = saved_config['seed']
            print(f"ğŸ”„ è‡ªåŠ¨æ›´æ–°éšæœºç§å­: {old_seed} -> {args.seed} (ä½¿ç”¨ä¿å­˜æ•°æ®æ—¶çš„ç§å­)")
            
            # ç«‹å³è®¾ç½®éšæœºç§å­
            import torch
            import numpy as np
            import random
            torch.manual_seed(args.seed)
            np.random.seed(args.seed)
            random.seed(args.seed)
            print(f"   å·²è®¾ç½®æ‰€æœ‰éšæœºç§å­ä¸º: {args.seed}")
        
        # éªŒè¯é…ç½®ä¸€è‡´æ€§
        if verify_config:
            current_hash = generate_data_config_hash(args)
            saved_hash = save_data.get('config_hash', '')
            
            if current_hash != saved_hash:
                print("âš ï¸  è­¦å‘Š: å½“å‰é…ç½®ä¸ä¿å­˜çš„é…ç½®ä¸ä¸€è‡´!")
                print(f"   å½“å‰é…ç½®å“ˆå¸Œ: {current_hash}")
                print(f"   ä¿å­˜çš„é…ç½®å“ˆå¸Œ: {saved_hash}")
                
                # é‡æ–°è®¡ç®—å“ˆå¸Œï¼ˆå› ä¸ºç§å­å¯èƒ½å·²ç»è‡ªåŠ¨æ›´æ–°ï¼‰
                updated_hash = generate_data_config_hash(args)
                
                if updated_hash != saved_hash:
                    # æ˜¾ç¤ºå…·ä½“å·®å¼‚
                    current_config = {
                        'dataset': args.dataset,
                        'num_users': args.num_users,
                        'partition': getattr(args, 'partition', 'noniid-labeldir'),
                        'beta': getattr(args, 'beta', 0.1),
                        'iid': getattr(args, 'iid', False),
                        'use_sampling': getattr(args, 'use_sampling', False),
                        'seed': args.seed
                    }
                    
                    print(f"   æ›´æ–°åé…ç½®å“ˆå¸Œ: {updated_hash}")
                    print("\n   é…ç½®å·®å¼‚:")
                    for key in current_config:
                        if key in saved_config and current_config[key] != saved_config[key]:
                            print(f"   - {key}: å½“å‰={current_config[key]}, ä¿å­˜={saved_config[key]}")
                    
                    response = input("\n   æ˜¯å¦ç»§ç»­ä½¿ç”¨ä¸ä¸€è‡´çš„é…ç½®? (y/N): ")
                    if response.lower() != 'y':
                        return None, None
                else:
                    print("âœ… é…ç½®éªŒè¯é€šè¿‡ï¼ˆç§å­å·²è‡ªåŠ¨åŒæ­¥ï¼‰")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        if 'metadata' in save_data:
            metadata = save_data['metadata']
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"   æ€»æ ·æœ¬æ•°: {metadata.get('total_samples', 'Unknown')}")
            print(f"   ç±»åˆ«æ•°: {len(metadata.get('unique_classes', []))}")
            
            # æ˜¾ç¤ºå®¢æˆ·ç«¯ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
            classes_per_client = metadata.get('classes_per_client', {})
            if classes_per_client:
                class_counts = {}
                for client_id, num_classes in classes_per_client.items():
                    class_counts[num_classes] = class_counts.get(num_classes, 0) + 1
                
                print("   å®¢æˆ·ç«¯ç±»åˆ«åˆ†å¸ƒ:")
                for num_classes, count in sorted(class_counts.items()):
                    print(f"     æ‹¥æœ‰ {num_classes} ä¸ªç±»åˆ«çš„å®¢æˆ·ç«¯: {count} ä¸ª")
        
        return dict_users, client_classes
        
    except Exception as e:
        print(f"âŒ åŠ è½½å®¢æˆ·ç«¯æ•°æ®åˆ†é…å¤±è´¥: {str(e)}")
        return None, None


def list_saved_data_files(data_save_dir):
    """
    åˆ—å‡ºæ‰€æœ‰ä¿å­˜çš„æ•°æ®æ–‡ä»¶
    
    Args:
        data_save_dir: æ•°æ®ä¿å­˜ç›®å½•
    
    Returns:
        list: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    if not os.path.exists(data_save_dir):
        return []
    
    files = []
    for filename in os.listdir(data_save_dir):
        if filename.endswith('.pkl') and 'client_data_' in filename:
            files.append(os.path.join(data_save_dir, filename))
    
    return sorted(files, key=os.path.getmtime, reverse=True)


def print_available_data_files(data_save_dir):
    """
    æ‰“å°æ‰€æœ‰å¯ç”¨çš„æ•°æ®æ–‡ä»¶
    
    Args:
        data_save_dir: æ•°æ®ä¿å­˜ç›®å½•
    """
    files = list_saved_data_files(data_save_dir)
    
    if not files:
        print(f"åœ¨ç›®å½• {data_save_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„æ•°æ®æ–‡ä»¶")
        return
    
    print(f"\nğŸ“ å¯ç”¨çš„å®¢æˆ·ç«¯æ•°æ®æ–‡ä»¶ ({len(files)} ä¸ª):")
    print("-" * 80)
    
    for i, filepath in enumerate(files, 1):
        try:
            # å°è¯•è¯»å–é…ç½®ä¿¡æ¯
            with open(filepath, 'rb') as f:
                save_data = pickle.load(f)
            
            config = save_data.get('config', {})
            metadata = save_data.get('metadata', {})
            
            filename = os.path.basename(filepath)
            print(f"{i:2d}. {filename}")
            print(f"    æ•°æ®é›†: {config.get('dataset', 'Unknown')}")
            print(f"    å®¢æˆ·ç«¯æ•°: {config.get('num_users', 'Unknown')}")
            print(f"    åˆ†åŒºæ–¹æ³•: {config.get('partition', 'Unknown')}")
            print(f"    Betaå‚æ•°: {config.get('beta', 'Unknown')}")
            print(f"    æ€»æ ·æœ¬æ•°: {metadata.get('total_samples', 'Unknown')}")
            print(f"    ä¿å­˜æ—¶é—´: {config.get('save_timestamp', 'Unknown')}")
            print()
            
        except Exception as e:
            filename = os.path.basename(filepath)
            print(f"{i:2d}. {filename} (è¯»å–é…ç½®å¤±è´¥: {str(e)})")
            print()


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    print("å®¢æˆ·ç«¯æ•°æ®åˆ†é…ä¿å­˜/åŠ è½½å·¥å…·æµ‹è¯•")
    
    # ç¤ºä¾‹ç”¨æ³•
    import argparse
    
    # åˆ›å»ºç¤ºä¾‹å‚æ•°
    args = argparse.Namespace(
        dataset='mnist',
        num_users=50,
        partition='noniid-labeldir',
        beta=0.1,
        iid=False,
        use_sampling=False,
        seed=1,
        data_save_dir='./test_saved_data/'
    )
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    dict_users = {i: set(range(i*100, (i+1)*100)) for i in range(5)}
    client_classes = {i: [i % 3, (i+1) % 3] for i in range(5)}
    
    # æµ‹è¯•ä¿å­˜
    filepath = save_client_data_distribution(dict_users, client_classes, args, "test_data")
    
    if filepath:
        # æµ‹è¯•åŠ è½½
        loaded_dict_users, loaded_client_classes = load_client_data_distribution(filepath, args)
        
        if loaded_dict_users is not None:
            print("\nâœ… æµ‹è¯•æˆåŠŸ!")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥!")