#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6
import os

import numpy as np
from torchvision import datasets, transforms
from utils.data_partition import get_client_datasets
from utils.data_persistence import (
    save_client_data_distribution, 
    load_client_data_distribution,
    print_available_data_files
)


def mnist_iid(dataset, num_users):
    """
    Sample I.I.D. client data from MNIST dataset
    :param dataset:
    :param num_users:
    :return: dict of image index
    """
    num_items = int(len(dataset)/num_users)##è®¡ç®—æ¯ä¸ªç”¨æˆ·åº”è¯¥æ‹¥æœ‰çš„æ ·æœ¬æ•°é‡
    dict_users, all_idxs = {}, [i for i in range(len(dataset))] ##dict_users:æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬ç´¢å¼•ï¼›all_idxs:åŒ…å«æ‰€æœ‰æ•°æ®ç´¢å¼•çš„åˆ—è¡¨
    for i in range(num_users):  ##ç»™æ¯ä¸ªç”¨æˆ·ä¸é‡å¤åœ°åˆ†é…æ•°æ®çš„ç´¢å¼•
        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))
        all_idxs = list(set(all_idxs) - dict_users[i])
    return dict_users


def mnist_noniid(dataset, num_users):
    """
    Sample non-I.I.D client data from MNIST dataset
    :param dataset:
    :param num_users:
    :return:
    """
    num_shards, num_imgs = 200, 300 ##num_shardsï¼šå°†æ•°æ®åˆ’åˆ†ä¸ºçš„ç‰‡æ®µæ•°é‡ï¼›num_imgs:æ¯ä¸ªç‰‡æ®µçš„å›¾åƒæ•°é‡
    idx_shard = [i for i in range(num_shards)]  ##æ¯ä¸ªç‰‡æ®µçš„ç´¢å¼•
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}  ##æ¯ä¸ªç”¨æˆ·å¯¹åº”çš„æ ·æœ¬ç´¢å¼•
    idxs = np.arange(num_shards*num_imgs) ##åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å›¾åƒç´¢å¼•çš„æ•°ç»„ï¼Œå°±æ˜¯ä¸€ä¸ª0-çš„æœ‰åºæ•°ç»„
    labels = dataset.train_labels.numpy() ##å°†æ•°æ®é›†ä¸­æ ·æœ¬çš„æ ‡ç­¾è½¬æ¢ä¸ºnumpy

    # sort labels
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()] ##æ ¹æ®æ ‡ç­¾å¯¹å›¾åƒè¿›è¡Œæ’åº ï¼Œè¿”å›ä¸ºæœ‰åºæ•°åˆ—çš„åŸä½ç½®çš„åºå·ä¾‹å¦‚ï¼Œå¦‚æœæ ‡ç­¾çš„åŸå§‹é¡ºåºæ˜¯ [5, 0, 4, 1, 2]ï¼Œä½¿ç”¨ argsort() åä¼šè¿”å› [1, 4, 3, 2, 0]ï¼Œè¡¨ç¤ºæ ‡ç­¾1æ’åœ¨æœ€å‰ï¼Œæ ‡ç­¾5æ’åœ¨æœ€åã€‚
    idxs = idxs_labels[0,:] ##idxsæ’åºåçš„ç´¢å¼•ï¼Œç›¸å½“äºæ ‡ç­¾æ’åºå¾—åˆ°æœ‰åºï¼Œå¯¹åº”ç´¢å¼•è·Ÿéšæ”¹å˜ï¼Œå¾—åˆ°æ–°ä½ç½®
       ##å…¶å°†ç›¸åŒç±»åˆ«çš„å›¾åƒé›†ä¸­åœ¨ä¸€èµ·

    # divide and assign å°†æ•°æ®åˆ†é…ç»™æ¯ä¸ªç”¨æˆ·
    for i in range(num_users):
        rand_set = set(np.random.choice(idx_shard, 2, replace=False))##éšæœºé€‰æ‹©ä¸¤ä¸ªç‰‡æ®µä¸”ä¸é‡å¤
        idx_shard = list(set(idx_shard) - rand_set) ##ç§»é™¤å·²è¢«é€‰æ‹©çš„ç‰‡æ®µ
        for rand in rand_set: ##å°†æ‰€é€‰ç‰‡æ®µ
            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)
    return dict_users


def cifar_iid(dataset, num_users):
    """
    Sample I.I.D. client data from CIFAR10 dataset
    :param dataset:
    :param num_users:
    :return: dict of image index
    """
    num_items = int(len(dataset)/num_users)  ##è®¡ç®—æ¯ä¸ªç”¨æˆ·åº”è¯¥æ‹¥æœ‰çš„æ ·æœ¬æ•°é‡
    dict_users, all_idxs = {}, [i for i in range(len(dataset))]
    for i in range(num_users):  ##ç»™æ¯ä¸ªç”¨æˆ·ä¸é‡å¤åœ°åˆ†é…æ•°æ®çš„ç´¢å¼•
        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))
        all_idxs = list(set(all_idxs) - dict_users[i])
    return dict_users

# new
def cifar_noniid_adapted(dataset, num_users):
    """
    CIFARæ•°æ®é›†çš„Non-IIDåˆ’åˆ†ï¼ŒåŸºäºMNIST Non-IIDæ–¹æ³•é€‚é…
    å°†10ä¸ªç±»åˆ«åˆ’åˆ†ä¸º200ä¸ªåˆ†ç‰‡ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…2ä¸ªåˆ†ç‰‡
    """
    num_shards, num_imgs = 200, 250  # CIFARæ¯ä¸ªåˆ†ç‰‡250å¼ å›¾ç‰‡
    idx_shard = [i for i in range(num_shards)]
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}
    idxs = np.arange(num_shards * num_imgs)
    labels = np.array(dataset.targets)

    # æŒ‰æ ‡ç­¾æ’åº
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    idxs = idxs_labels[0, :]

    # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯éšæœºåˆ†é…2ä¸ªåˆ†ç‰‡
    for i in range(num_users):
        rand_set = set(np.random.choice(idx_shard, 2, replace=False))
        idx_shard = list(set(idx_shard) - rand_set)
        for rand in rand_set:
            dict_users[i] = np.concatenate(
                (dict_users[i], idxs[rand * num_imgs:(rand + 1) * num_imgs]), axis=0)

    return dict_users


def get_client_classes_from_sampling(dataset, dict_users):
    """
    ä»sampling.pyçš„æ•°æ®åˆ’åˆ†ç»“æœä¸­æå–å®¢æˆ·ç«¯ç±»åˆ«ä¿¡æ¯
    """
    client_classes = {}

    for client_id, indices in dict_users.items():
        # è·å–è¯¥å®¢æˆ·ç«¯æ•°æ®çš„æ‰€æœ‰æ ‡ç­¾
        if hasattr(dataset, 'targets'):
            labels = np.array(dataset.targets)[indices.astype(int)]
        else:
            labels = np.array([dataset[int(idx)][1] for idx in indices])

        # è·å–å”¯ä¸€çš„ç±»åˆ«
        unique_classes = np.unique(labels).tolist()
        client_classes[client_id] = unique_classes

    return client_classes

def get_data_new(dataset_type, num_clients, data_path, partition_method='homo', noniid_param=0.4):
    """
    ä½¿ç”¨æ–°çš„æ•°æ®åˆ’åˆ†å‡½æ•°è·å–æ•°æ®

    Args:
        dataset_type (str): æ•°æ®é›†ç±»å‹ ('mnist', 'cifar10', 'cifar100')
        num_clients (int): å®¢æˆ·ç«¯æ•°é‡
        data_path (str): æ•°æ®ä¿å­˜è·¯å¾„
        partition_method (str): æ•°æ®åˆ†åŒºæ–¹å¼
        noniid_param (float): non-iidåˆ†å¸ƒå‚æ•°

    Returns:
        tuple: (è®­ç»ƒæ•°æ®é›†, æµ‹è¯•æ•°æ®é›†, å®¢æˆ·ç«¯æ•°æ®æ˜ å°„, å®¢æˆ·ç«¯ç±»åˆ«æ˜ å°„)
    """

    return get_client_datasets(dataset_type, num_clients, data_path, partition_method, noniid_param)

def get_data(args):
    """å…¼å®¹åŸæœ‰æ¥å£çš„æ•°æ®è·å–å‡½æ•°ï¼Œæ”¯æŒæ•°æ®ä¿å­˜å’ŒåŠ è½½"""
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦ä»æ–‡ä»¶åŠ è½½æ•°æ®
    if hasattr(args, 'load_data') and args.load_data:
        print(f"\nğŸ”„ å°è¯•ä»æ–‡ä»¶åŠ è½½å®¢æˆ·ç«¯æ•°æ®: {args.load_data}")
        
        # å¦‚æœæŒ‡å®šäº†ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨æ•°æ®ä¿å­˜ç›®å½•ä¸­æŸ¥æ‰¾
        if not os.path.isabs(args.load_data):
            save_dir = getattr(args, 'data_save_dir', './saved_data/')
            full_path = os.path.join(save_dir, args.load_data)
            if os.path.exists(full_path):
                args.load_data = full_path
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        dataset_train, dataset_test = create_dataset_objects(args)
        
        # å°è¯•åŠ è½½æ•°æ®
        dict_users, client_classes = load_client_data_distribution(args.load_data, args)
        
        if dict_users is not None and client_classes is not None:
            print("âœ… æˆåŠŸä»æ–‡ä»¶åŠ è½½å®¢æˆ·ç«¯æ•°æ®åˆ†é…")
            return dataset_train, dataset_test, dict_users, client_classes
        else:
            print("âŒ ä»æ–‡ä»¶åŠ è½½æ•°æ®å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆæ•°æ®")
    
    # å¦‚æœæŒ‡å®šäº†--load_dataä½†æ²¡æœ‰æä¾›å…·ä½“è·¯å¾„ï¼Œæ˜¾ç¤ºå¯ç”¨æ–‡ä»¶
    if hasattr(args, 'load_data') and args.load_data == '':
        save_dir = getattr(args, 'data_save_dir', './saved_data/')
        print_available_data_files(save_dir)
        exit("è¯·æŒ‡å®šè¦åŠ è½½çš„æ•°æ®æ–‡ä»¶è·¯å¾„")

    print("\nğŸ”¨ ç”Ÿæˆæ–°çš„å®¢æˆ·ç«¯æ•°æ®åˆ†é…...")
    
    # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
    dataset_train, dataset_test = create_dataset_objects(args)
    
    # ç¡®å®šæ•°æ®é›†ç±»å‹å’Œè·¯å¾„
    if args.dataset == 'mnist':
        dataset_type = 'mnist'
        data_path = os.path.join(args.data_path, 'mnist/')
    elif args.dataset == 'cifar':
        dataset_type = 'cifar10'
        data_path = os.path.join(args.data_path, 'cifar/')
    else:
        exit('Error: unrecognized dataset')

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ sampling.py ä¸­çš„æ•°æ®åˆ’åˆ†æ–¹å¼
    use_sampling_partition = getattr(args, 'use_sampling', False)

    if use_sampling_partition:
        print("ä½¿ç”¨ sampling.py ä¸­çš„æ•°æ®åˆ’åˆ†æ–¹å¼")
        # ä½¿ç”¨ sampling.py ä¸­çš„æ•°æ®åˆ’åˆ†æ–¹å¼
        if args.dataset == 'mnist':
            if hasattr(args, 'iid') and args.iid:
                dict_users = mnist_iid(dataset_train, args.num_users)
                print("ä½¿ç”¨ MNIST IID æ•°æ®åˆ’åˆ†")
            else:
                dict_users = mnist_noniid(dataset_train, args.num_users)
                print("ä½¿ç”¨ MNIST Non-IID æ•°æ®åˆ’åˆ†")
        elif args.dataset == 'cifar':
            if hasattr(args, 'iid') and args.iid:
                dict_users = cifar_iid(dataset_train, args.num_users)
                print("ä½¿ç”¨ CIFAR IID æ•°æ®åˆ’åˆ†")
            else:
                # å¯¹äº CIFARï¼Œå¦‚æœæ²¡æœ‰ cifar_noniid å‡½æ•°ï¼Œä½¿ç”¨ä¿®æ”¹ç‰ˆçš„ mnist_noniid
                print("è­¦å‘Š: CIFAR ä½¿ç”¨ä¿®æ”¹ç‰ˆçš„ Non-IID åˆ’åˆ†")
                dict_users = cifar_noniid_adapted(dataset_train, args.num_users)

        # è®¡ç®—å®¢æˆ·ç«¯ç±»åˆ«ä¿¡æ¯ï¼ˆç”¨äºFedRSï¼‰
        client_classes = get_client_classes_from_sampling(dataset_train, dict_users)

    else:
        # ä½¿ç”¨åŸæœ‰çš„æ•°æ®åˆ’åˆ†æ–¹æ³•
        # ç¡®å®šåˆ†åŒºæ–¹æ³• - ä¼˜å…ˆä½¿ç”¨æ–°çš„partitionå‚æ•°
        if hasattr(args, 'partition'):
            partition_method = args.partition
            # å¦‚æœè¿˜è®¾ç½®äº†iidå‚æ•°ï¼Œè¦†ç›–partitionè®¾ç½®
            if hasattr(args, 'iid') and args.iid:
                partition_method = 'homo'
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬å‚æ•°
            if hasattr(args, 'iid') and args.iid:
                partition_method = 'homo'
            else:
                partition_method = 'noniid-labeldir'

        # ç¡®å®šnon-iidå‚æ•°
        noniid_param = getattr(args, 'beta', 0.4)

        print(f"ä½¿ç”¨æ•°æ®åˆ’åˆ†æ–¹æ³•: {partition_method}, non-iidå‚æ•°: {noniid_param}")

        # ä½¿ç”¨æ–°çš„æ•°æ®åˆ’åˆ†æ–¹æ³•è·å–å®¢æˆ·ç«¯æ˜ å°„
        train_data, test_data, dict_users, client_classes = get_data_new(
            dataset_type, args.num_users, data_path, partition_method, noniid_param
        )
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æ•°æ®
    if hasattr(args, 'save_data') and args.save_data:
        print("\nğŸ’¾ ä¿å­˜å®¢æˆ·ç«¯æ•°æ®åˆ†é…...")
        save_client_data_distribution(dict_users, client_classes, args)
    
    return dataset_train, dataset_test, dict_users, client_classes


def create_dataset_objects(args):
    """
    åˆ›å»ºæ•°æ®é›†å¯¹è±¡
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
    
    Returns:
        tuple: (dataset_train, dataset_test)
    """
    if args.dataset == 'mnist':
        data_path = os.path.join(args.data_path, 'mnist/')
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST(data_path, train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST(data_path, train=False, download=True, transform=trans_mnist)

    elif args.dataset == 'cifar':
        data_path = os.path.join(args.data_path, 'cifar/')
        trans_cifar = transforms.Compose(
            [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        dataset_train = datasets.CIFAR10(data_path, train=True, download=True, transform=trans_cifar)
        dataset_test = datasets.CIFAR10(data_path, train=False, download=True, transform=trans_cifar)

    else:
        exit('Error: unrecognized dataset')
    
    return dataset_train, dataset_test


if __name__ == '__main__':
    dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True,
                                   transform=transforms.Compose([
                                       transforms.ToTensor(),
                                       transforms.Normalize((0.1307,), (0.3081,))
                                   ]))
    num = 100
    d = mnist_noniid(dataset_train, num)
