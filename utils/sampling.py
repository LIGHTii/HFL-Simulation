#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6
import os
import random

import numpy as np
from torchvision import datasets, transforms
from utils.data_partition import get_client_datasets
from utils.data_persistence import (
    save_client_data_distribution, 
    load_client_data_distribution,
    print_available_data_files
)


def mnist_iid(dataset, num_users):
    """
    Sample I.I.D. client data from MNIST dataset
    :param dataset:
    :param num_users:
    :return: dict of image index
    """
    num_items = int(len(dataset)/num_users)##è®¡ç®—æ¯ä¸ªç”¨æˆ·åº”è¯¥æ‹¥æœ‰çš„æ ·æœ¬æ•°é‡
    dict_users = {}
    all_idxs = [i for i in range(len(dataset))] ##åŒ…å«æ‰€æœ‰æ•°æ®ç´¢å¼•çš„åˆ—è¡¨
    
    for i in range(num_users):  ##ç»™æ¯ä¸ªç”¨æˆ·åˆ†é…æ•°æ®çš„ç´¢å¼•
        if len(all_idxs) >= num_items:
            # å¦‚æœå‰©ä½™æ•°æ®è¶³å¤Ÿï¼Œä¸é‡å¤åˆ†é…
            dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))
            all_idxs = list(set(all_idxs) - dict_users[i])
        else:
            # å¦‚æœå‰©ä½™æ•°æ®ä¸å¤Ÿï¼Œå…è®¸é‡å¤åˆ†é…
            dict_users[i] = set(np.random.choice(len(dataset), num_items, replace=False))
    return dict_users


def mnist_noniid(dataset, num_users):
    """
    Sample non-I.I.D client data from MNIST dataset
    :param dataset:
    :param num_users:
    :return:
    """
    num_shards, num_imgs = 200, 300 ##num_shardsï¼šå°†æ•°æ®åˆ’åˆ†ä¸ºçš„ç‰‡æ®µæ•°é‡ï¼›num_imgs:æ¯ä¸ªç‰‡æ®µçš„å›¾åƒæ•°é‡
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}  ##æ¯ä¸ªç”¨æˆ·å¯¹åº”çš„æ ·æœ¬ç´¢å¼•
    idxs = np.arange(num_shards*num_imgs) ##åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å›¾åƒç´¢å¼•çš„æ•°ç»„ï¼Œå°±æ˜¯ä¸€ä¸ª0-çš„æœ‰åºæ•°ç»„
    labels = dataset.train_labels.numpy() ##å°†æ•°æ®é›†ä¸­æ ·æœ¬çš„æ ‡ç­¾è½¬æ¢ä¸ºnumpy

    # sort labels
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()] ##æ ¹æ®æ ‡ç­¾å¯¹å›¾åƒè¿›è¡Œæ’åº ï¼Œè¿”å›ä¸ºæœ‰åºæ•°åˆ—çš„åŸä½ç½®çš„åºå·ä¾‹å¦‚ï¼Œå¦‚æœæ ‡ç­¾çš„åŸå§‹é¡ºåºæ˜¯ [5, 0, 4, 1, 2]ï¼Œä½¿ç”¨ argsort() åä¼šè¿”å› [1, 4, 3, 2, 0]ï¼Œè¡¨ç¤ºæ ‡ç­¾1æ’åœ¨æœ€å‰ï¼Œæ ‡ç­¾5æ’åœ¨æœ€åã€‚
    idxs = idxs_labels[0,:] ##idxsæ’åºåçš„ç´¢å¼•ï¼Œç›¸å½“äºæ ‡ç­¾æ’åºå¾—åˆ°æœ‰åºï¼Œå¯¹åº”ç´¢å¼•è·Ÿéšæ”¹å˜ï¼Œå¾—åˆ°æ–°ä½ç½®
       ##å…¶å°†ç›¸åŒç±»åˆ«çš„å›¾åƒé›†ä¸­åœ¨ä¸€èµ·

    # divide and assign å°†æ•°æ®åˆ†é…ç»™æ¯ä¸ªç”¨æˆ· - å…è®¸é‡å åˆ†é…
    shards_per_user = 2  # æ¯ä¸ªç”¨æˆ·åˆ†é…çš„ç‰‡æ®µæ•°é‡
    for i in range(num_users):
        # å…è®¸é‡å¤é€‰æ‹©ç‰‡æ®µæ¥æ”¯æŒæ›´å¤šç”¨æˆ·
        rand_set = set(np.random.choice(num_shards, shards_per_user, replace=False))
        for rand in rand_set: ##å°†æ‰€é€‰ç‰‡æ®µ
            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)
    return dict_users


def cifar_iid(dataset, num_users):
    """
    Sample I.I.D. client data from CIFAR10 dataset
    :param dataset:
    :param num_users:
    :return: dict of image index
    """
    num_items = int(len(dataset)/num_users)  ##è®¡ç®—æ¯ä¸ªç”¨æˆ·åº”è¯¥æ‹¥æœ‰çš„æ ·æœ¬æ•°é‡
    dict_users = {}
    all_idxs = [i for i in range(len(dataset))]
    
    for i in range(num_users):  ##ç»™æ¯ä¸ªç”¨æˆ·åˆ†é…æ•°æ®çš„ç´¢å¼•
        if len(all_idxs) >= num_items:
            # å¦‚æœå‰©ä½™æ•°æ®è¶³å¤Ÿï¼Œä¸é‡å¤åˆ†é…
            dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))
            all_idxs = list(set(all_idxs) - dict_users[i])
        else:
            # å¦‚æœå‰©ä½™æ•°æ®ä¸å¤Ÿï¼Œå…è®¸é‡å¤åˆ†é…
            dict_users[i] = set(np.random.choice(len(dataset), num_items, replace=False))
    return dict_users

# new
def cifar_noniid_adapted(dataset, num_users):
    """
    CIFARæ•°æ®é›†çš„Non-IIDåˆ’åˆ†ï¼ŒåŸºäºMNIST Non-IIDæ–¹æ³•é€‚é…
    å°†10ä¸ªç±»åˆ«åˆ’åˆ†ä¸º200ä¸ªåˆ†ç‰‡ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…2ä¸ªåˆ†ç‰‡
    å…è®¸ç‰‡æ®µé‡å ä»¥æ”¯æŒæ›´å¤šç”¨æˆ·
    """
    num_shards, num_imgs = 200, 250  # CIFARæ¯ä¸ªåˆ†ç‰‡250å¼ å›¾ç‰‡
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}
    idxs = np.arange(num_shards * num_imgs)
    labels = np.array(dataset.targets)

    # æŒ‰æ ‡ç­¾æ’åº
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    idxs = idxs_labels[0, :]

    # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯éšæœºåˆ†é…2ä¸ªåˆ†ç‰‡ - å…è®¸é‡å åˆ†é…
    shards_per_user = 2  # æ¯ä¸ªç”¨æˆ·åˆ†é…çš„ç‰‡æ®µæ•°é‡
    for i in range(num_users):
        # å…è®¸é‡å¤é€‰æ‹©ç‰‡æ®µæ¥æ”¯æŒæ›´å¤šç”¨æˆ·
        rand_set = set(np.random.choice(num_shards, shards_per_user, replace=False))
        for rand in rand_set:
            dict_users[i] = np.concatenate(
                (dict_users[i], idxs[rand * num_imgs:(rand + 1) * num_imgs]), axis=0)

    return dict_users

def cifar100_iid(dataset, num_users):
    """
    Sample I.I.D. client data from CIFAR100 dataset
    """
    num_items = int(len(dataset) / num_users)
    dict_users = {}
    all_idxs = [i for i in range(len(dataset))]
    
    for i in range(num_users):
        if len(all_idxs) >= num_items:
            # å¦‚æœå‰©ä½™æ•°æ®è¶³å¤Ÿï¼Œä¸é‡å¤åˆ†é…
            dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))
            all_idxs = list(set(all_idxs) - dict_users[i])
        else:
            # å¦‚æœå‰©ä½™æ•°æ®ä¸å¤Ÿï¼Œå…è®¸é‡å¤åˆ†é…
            dict_users[i] = set(np.random.choice(len(dataset), num_items, replace=False))
    return dict_users

# æ–°å¢: CIFAR-100 Non-IID (é€‚é…è‡ª mnist_noniid)
def cifar100_noniid_adapted(dataset, num_users):
    """
    CIFAR-100æ•°æ®é›†çš„Non-IIDåˆ’åˆ†ï¼ŒåŸºäºMNIST Non-IIDæ–¹æ³•é€‚é…
    å°†100ä¸ªç±»åˆ«åˆ’åˆ†ä¸º500ä¸ªåˆ†ç‰‡ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…5ä¸ªåˆ†ç‰‡ (CIFAR-100 train: 50000å›¾åƒ)
    å…è®¸ç‰‡æ®µé‡å ä»¥æ”¯æŒæ›´å¤šç”¨æˆ·
    """
    num_shards, num_imgs = 500, 100  # 500 shards * 100 imgs = 50000
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}
    idxs = np.arange(num_shards * num_imgs)
    labels = np.array(dataset.targets)

    # æŒ‰æ ‡ç­¾æ’åº
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    idxs = idxs_labels[0, :]

    # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯éšæœºåˆ†é…5ä¸ªåˆ†ç‰‡ - å…è®¸é‡å åˆ†é…
    shards_per_user = 5  # æ¯ä¸ªç”¨æˆ·åˆ†é…çš„ç‰‡æ®µæ•°é‡
    for i in range(num_users):
        # å…è®¸é‡å¤é€‰æ‹©ç‰‡æ®µæ¥æ”¯æŒæ›´å¤šç”¨æˆ·
        rand_set = set(np.random.choice(num_shards, shards_per_user, replace=False))
        for rand in rand_set:
            dict_users[i] = np.concatenate(
                (dict_users[i], idxs[rand * num_imgs:(rand + 1) * num_imgs]), axis=0)

    return dict_users


def get_client_classes_from_sampling(dataset, dict_users):
    """
    ä»sampling.pyçš„æ•°æ®åˆ’åˆ†ç»“æœä¸­æå–å®¢æˆ·ç«¯ç±»åˆ«ä¿¡æ¯
    """
    client_classes = {}

    for client_id, indices in dict_users.items():
        # è·å–è¯¥å®¢æˆ·ç«¯æ•°æ®çš„æ‰€æœ‰æ ‡ç­¾
        if hasattr(dataset, 'targets'):
            labels = np.array(dataset.targets)[indices.astype(int)]
        else:
            labels = np.array([dataset[int(idx)][1] for idx in indices])

        # è·å–å”¯ä¸€çš„ç±»åˆ«
        unique_classes = np.unique(labels).tolist()
        client_classes[client_id] = unique_classes

    return client_classes

def get_data_new(dataset_type, num_clients, data_path, partition_method='homo', noniid_param=0.4):
    """
    ä½¿ç”¨æ–°çš„æ•°æ®åˆ’åˆ†å‡½æ•°è·å–æ•°æ®

    Args:
        dataset_type (str): æ•°æ®é›†ç±»å‹ ('mnist', 'cifar10', 'cifar100')
        num_clients (int): å®¢æˆ·ç«¯æ•°é‡
        data_path (str): æ•°æ®ä¿å­˜è·¯å¾„
        partition_method (str): æ•°æ®åˆ†åŒºæ–¹å¼
        noniid_param (float): non-iidåˆ†å¸ƒå‚æ•°

    Returns:
        tuple: (è®­ç»ƒæ•°æ®é›†, æµ‹è¯•æ•°æ®é›†, å®¢æˆ·ç«¯æ•°æ®æ˜ å°„, å®¢æˆ·ç«¯ç±»åˆ«æ˜ å°„)
    """

    return get_client_datasets(dataset_type, num_clients, data_path, partition_method, noniid_param)

def get_data(args):
    """å…¼å®¹åŸæœ‰æ¥å£çš„æ•°æ®è·å–å‡½æ•°ï¼Œæ”¯æŒæ•°æ®ä¿å­˜å’ŒåŠ è½½"""
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦ä»æ–‡ä»¶åŠ è½½æ•°æ®
    if hasattr(args, 'load_data') and args.load_data:
        print(f"\nğŸ”„ å°è¯•ä»æ–‡ä»¶åŠ è½½å®¢æˆ·ç«¯æ•°æ®: {args.load_data}")
        
        # å¦‚æœæŒ‡å®šäº†ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨æ•°æ®ä¿å­˜ç›®å½•ä¸­æŸ¥æ‰¾
        if not os.path.isabs(args.load_data):
            save_dir = getattr(args, 'data_save_dir', './saved_data/')
            full_path = os.path.join(save_dir, args.load_data)
            if os.path.exists(full_path):
                args.load_data = full_path
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        dataset_train, dataset_test = create_dataset_objects(args)
        
        # å°è¯•åŠ è½½æ•°æ®
        dict_users, client_classes = load_client_data_distribution(args.load_data, args)
        
        if dict_users is not None and client_classes is not None:
            print("âœ… æˆåŠŸä»æ–‡ä»¶åŠ è½½å®¢æˆ·ç«¯æ•°æ®åˆ†é…")
            return dataset_train, dataset_test, dict_users, client_classes
        else:
            print("âŒ ä»æ–‡ä»¶åŠ è½½æ•°æ®å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆæ•°æ®")
    
    # å¦‚æœæŒ‡å®šäº†--load_dataä½†æ²¡æœ‰æä¾›å…·ä½“è·¯å¾„ï¼Œæ˜¾ç¤ºå¯ç”¨æ–‡ä»¶
    if hasattr(args, 'load_data') and args.load_data == '':
        save_dir = getattr(args, 'data_save_dir', './saved_data/')
        print_available_data_files(save_dir)
        exit("è¯·æŒ‡å®šè¦åŠ è½½çš„æ•°æ®æ–‡ä»¶è·¯å¾„")

    print("\nğŸ”¨ ç”Ÿæˆæ–°çš„å®¢æˆ·ç«¯æ•°æ®åˆ†é…...")
    
    # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
    dataset_train, dataset_test = create_dataset_objects(args)
    
    # ç¡®å®šæ•°æ®é›†ç±»å‹å’Œè·¯å¾„
    if args.dataset == 'mnist':
        dataset_type = 'mnist'
        data_path = os.path.join(args.data_path, 'mnist/')
    elif args.dataset == 'cifar':
        dataset_type = 'cifar10'
        data_path = os.path.join(args.data_path, 'cifar/')
    elif args.dataset == 'cifar100':
        dataset_type = 'cifar100'
        data_path = os.path.join(args.data_path, 'cifar100/')
    else:
        exit('Error: unrecognized dataset')

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ sampling.py ä¸­çš„æ•°æ®åˆ’åˆ†æ–¹å¼
    use_sampling_partition = getattr(args, 'use_sampling', False)

    if use_sampling_partition:
        print("ä½¿ç”¨ sampling.py ä¸­çš„æ•°æ®åˆ’åˆ†æ–¹å¼")
        # ä½¿ç”¨ sampling.py ä¸­çš„æ•°æ®åˆ’åˆ†æ–¹å¼
        if args.dataset == 'mnist':
            if hasattr(args, 'iid') and args.iid:
                dict_users = mnist_iid(dataset_train, args.num_users)
                print("ä½¿ç”¨ MNIST IID æ•°æ®åˆ’åˆ†")
            else:
                dict_users = mnist_noniid(dataset_train, args.num_users)
                print("ä½¿ç”¨ MNIST Non-IID æ•°æ®åˆ’åˆ†")
        elif args.dataset == 'cifar':
            if hasattr(args, 'iid') and args.iid:
                dict_users = cifar_iid(dataset_train, args.num_users)
                print("ä½¿ç”¨ CIFAR IID æ•°æ®åˆ’åˆ†")
            else:
                # å¯¹äº CIFARï¼Œå¦‚æœæ²¡æœ‰ cifar_noniid å‡½æ•°ï¼Œä½¿ç”¨ä¿®æ”¹ç‰ˆçš„ mnist_noniid
                print("è­¦å‘Š: CIFAR ä½¿ç”¨ä¿®æ”¹ç‰ˆçš„ Non-IID åˆ’åˆ†")
                dict_users = cifar_noniid_adapted(dataset_train, args.num_users)
        elif args.dataset == 'cifar100':  # æ–°å¢
            if args.iid:
                dict_users = cifar100_iid(dataset_train, args.num_users)
                print("ä½¿ç”¨ CIFAR100 IID æ•°æ®åˆ’åˆ†")
            else:
                dict_users = cifar100_noniid_adapted(dataset_train, args.num_users)
                print("ä½¿ç”¨ CIFAR100 Non-IID æ•°æ®åˆ’åˆ†")


        # è®¡ç®—å®¢æˆ·ç«¯ç±»åˆ«ä¿¡æ¯ï¼ˆç”¨äºFedRSï¼‰
        client_classes = get_client_classes_from_sampling(dataset_train, dict_users)

    else:
        # ä½¿ç”¨åŸæœ‰çš„æ•°æ®åˆ’åˆ†æ–¹æ³•
        # ç¡®å®šåˆ†åŒºæ–¹æ³• - ä¼˜å…ˆä½¿ç”¨æ–°çš„partitionå‚æ•°
        if hasattr(args, 'partition'):
            partition_method = args.partition
            # å¦‚æœè¿˜è®¾ç½®äº†iidå‚æ•°ï¼Œè¦†ç›–partitionè®¾ç½®
            if hasattr(args, 'iid') and args.iid:
                partition_method = 'homo'
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬å‚æ•°
            if hasattr(args, 'iid') and args.iid:
                partition_method = 'homo'
            else:
                partition_method = 'noniid-labeldir'

        # ç¡®å®šnon-iidå‚æ•°
        noniid_param = getattr(args, 'beta', 0.4)

        print(f"ä½¿ç”¨æ•°æ®åˆ’åˆ†æ–¹æ³•: {partition_method}, non-iidå‚æ•°: {noniid_param}")

        # ä½¿ç”¨æ–°çš„æ•°æ®åˆ’åˆ†æ–¹æ³•è·å–å®¢æˆ·ç«¯æ˜ å°„
        train_data, test_data, dict_users, client_classes = get_data_new(
            dataset_type, args.num_users, data_path, partition_method, noniid_param
        )
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æ•°æ®
    if hasattr(args, 'save_data') and args.save_data:
        print("\nğŸ’¾ ä¿å­˜å®¢æˆ·ç«¯æ•°æ®åˆ†é…...")
        save_client_data_distribution(dict_users, client_classes, args)
    
    return dataset_train, dataset_test, dict_users, client_classes


def create_dataset_objects(args):
    """
    åˆ›å»ºæ•°æ®é›†å¯¹è±¡
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
    
    Returns:
        tuple: (dataset_train, dataset_test)
    """
    if args.dataset == 'mnist':
        data_path = os.path.join(args.data_path, 'mnist/')
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST(data_path, train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST(data_path, train=False, download=True, transform=trans_mnist)

    elif args.dataset == 'cifar':
        data_path = os.path.join(args.data_path, 'cifar/')
        trans_cifar = transforms.Compose(
            [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        dataset_train = datasets.CIFAR10(data_path, train=True, download=True, transform=trans_cifar)
        dataset_test = datasets.CIFAR10(data_path, train=False, download=True, transform=trans_cifar)
    
    elif args.dataset == 'cifar100':

        data_path = os.path.join(args.data_path, 'cifar100/')
        trans_cifar100 = transforms.Compose(
            [transforms.ToTensor(),
             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        dataset_train = datasets.CIFAR100(data_path, train=True, download=True, transform=trans_cifar100)
        dataset_test = datasets.CIFAR100(data_path, train=False, download=True, transform=trans_cifar100)

    else:
        exit('Error: unrecognized dataset')
    
    return dataset_train, dataset_test


def get_data_test(args):
    """
    è·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼Œæ”¯æŒä¸¤ç§å›ºå®šçš„Non-IIDæ•°æ®åˆ†å¸ƒç±»å‹
    
    Args:
        args: å‚æ•°å¯¹è±¡ï¼ŒåŒ…å«æ•°æ®é›†ã€ç”¨æˆ·æ•°é‡ç­‰é…ç½®
        
    Returns:
        tuple: (dataset_train, dataset_test, dict_users, client_classes)
    """
    import random
    import numpy as np
    
    print(f"\nğŸ§ª ä½¿ç”¨æµ‹è¯•ç‰ˆæ•°æ®åˆ†é… (get_data_test)")
    print(f"ğŸ“Š Non-IIDæ¨¡å¼ï¼šä»…ä½¿ç”¨ä¸¤ç§å›ºå®šçš„æ•°æ®åˆ†å¸ƒç±»å‹")
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦ä»æ–‡ä»¶åŠ è½½æ•°æ®
    if hasattr(args, 'load_data') and args.load_data:
        print(f"\nğŸ”„ å°è¯•ä»æ–‡ä»¶åŠ è½½å®¢æˆ·ç«¯æ•°æ®: {args.load_data}")
        
        # å¦‚æœæŒ‡å®šäº†ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨æ•°æ®ä¿å­˜ç›®å½•ä¸­æŸ¥æ‰¾
        if not os.path.isabs(args.load_data):
            save_dir = getattr(args, 'data_save_dir', './saved_data/')
            full_path = os.path.join(save_dir, args.load_data)
            if os.path.exists(full_path):
                args.load_data = full_path
        
        # åˆ›å»ºæ•°æ®é›†å¯¹è±¡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        dataset_train, dataset_test = create_dataset_objects(args)
        
        # å°è¯•åŠ è½½æ•°æ®
        dict_users, client_classes = load_client_data_distribution(args.load_data, args)
        
        if dict_users is not None and client_classes is not None:
            print("âœ… æˆåŠŸä»æ–‡ä»¶åŠ è½½å®¢æˆ·ç«¯æ•°æ®åˆ†é…")
            return dataset_train, dataset_test, dict_users, client_classes
        else:
            print("âŒ ä»æ–‡ä»¶åŠ è½½æ•°æ®å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆæ•°æ®")
    
    print("\nğŸ”¨ ç”Ÿæˆæ–°çš„æµ‹è¯•ç‰ˆå®¢æˆ·ç«¯æ•°æ®åˆ†é…...")
    
    # åˆ›å»ºæ•°æ®é›†å¯¹è±¡
    dataset_train, dataset_test = create_dataset_objects(args)
    
    # ç¡®å®šæ•°æ®é›†ç±»å‹
    if args.dataset == 'mnist':
        num_classes = 10
        class_names = list(range(10))
    elif args.dataset == 'cifar':
        num_classes = 10  
        class_names = list(range(10))
    elif args.dataset == 'cifar100':
        num_classes = 100
        class_names = list(range(100))
    else:
        raise ValueError(f'Error: unrecognized dataset {args.dataset}')
    
    # å®šä¹‰ä¸¤ç§å›ºå®šçš„æ•°æ®åˆ†å¸ƒç±»å‹
    if args.iid:
        print("ğŸ¯ IIDæ¨¡å¼ï¼šæ‰€æœ‰å®¢æˆ·ç«¯ä½¿ç”¨ç›¸åŒçš„IIDåˆ†å¸ƒ")
        # IIDæ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰å®¢æˆ·ç«¯éƒ½ä½¿ç”¨ç›¸åŒçš„åˆ†å¸ƒ
        if args.dataset == 'mnist':
            dict_users = mnist_iid(dataset_train, args.num_users)
        elif args.dataset == 'cifar':
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.dataset == 'cifar100':
            dict_users = cifar100_iid(dataset_train, args.num_users)
        
        # è®¡ç®—å®¢æˆ·ç«¯ç±»åˆ«ä¿¡æ¯
        client_classes = get_client_classes_from_sampling(dataset_train, dict_users)
        
    else:
        print("ğŸ¯ Non-IIDæ¨¡å¼ï¼šä½¿ç”¨ä¸¤ç§å›ºå®šçš„æ•°æ®åˆ†å¸ƒç±»å‹")
        
        # å®šä¹‰ä¸¤ç§ä¸åŒçš„Non-IIDåˆ†å¸ƒç±»å‹
        if num_classes >= 6:
            # ç±»å‹Aï¼šåå‘å‰åŠéƒ¨åˆ†ç±»åˆ« (0, 1, 2, ...)
            type_A_classes = class_names[:num_classes//2]
            # ç±»å‹Bï¼šåå‘ååŠéƒ¨åˆ†ç±»åˆ« (..., 7, 8, 9)
            type_B_classes = class_names[num_classes//2:]
        else:
            # å¦‚æœç±»åˆ«å¤ªå°‘ï¼Œäº¤æ›¿åˆ†é…
            type_A_classes = [class_names[i] for i in range(0, num_classes, 2)]  # å¶æ•°ç´¢å¼•
            type_B_classes = [class_names[i] for i in range(1, num_classes, 2)]  # å¥‡æ•°ç´¢å¼•
        
        print(f"ğŸ“‹ ç±»å‹Aåˆ†å¸ƒä¸»è¦ç±»åˆ«: {type_A_classes}")
        print(f"ğŸ“‹ ç±»å‹Båˆ†å¸ƒä¸»è¦ç±»åˆ«: {type_B_classes}")
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯éšæœºåˆ†é…åˆ†å¸ƒç±»å‹
        distribution_types = []
        for i in range(args.num_users):
            dist_type = random.choice(['A', 'B'])
            distribution_types.append(dist_type)
        
        type_A_count = distribution_types.count('A')
        type_B_count = distribution_types.count('B')
        print(f"ğŸ“Š åˆ†å¸ƒç±»å‹åˆ†é…ï¼šç±»å‹A {type_A_count}ä¸ªå®¢æˆ·ç«¯ï¼Œç±»å‹B {type_B_count}ä¸ªå®¢æˆ·ç«¯")
        
        # ç”Ÿæˆæ¯ç§ç±»å‹çš„æ•°æ®åˆ†é…
        dict_users = {}
        client_classes = {}
        
        # è·å–æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬ç´¢å¼•
        labels = np.array(dataset_train.targets)
        class_indices = {}
        for class_id in range(num_classes):
            class_indices[class_id] = np.where(labels == class_id)[0]
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…æ•°æ®
        samples_per_client = len(dataset_train) // args.num_users
        
        for client_id in range(args.num_users):
            dist_type = distribution_types[client_id]
            
            if dist_type == 'A':
                # ç±»å‹Aï¼š80%æ¥è‡ªtype_A_classesï¼Œ20%æ¥è‡ªtype_B_classes
                main_classes = type_A_classes
                minor_classes = type_B_classes
            else:
                # ç±»å‹Bï¼š80%æ¥è‡ªtype_B_classesï¼Œ20%æ¥è‡ªtype_A_classes
                main_classes = type_B_classes  
                minor_classes = type_A_classes
            
            # åˆ†é…æ ·æœ¬
            client_indices = []
            
            # 80%æ¥è‡ªä¸»è¦ç±»åˆ«
            main_samples = int(samples_per_client * 0.8)
            main_samples_per_class = main_samples // len(main_classes)
            
            for class_id in main_classes:
                available_indices = class_indices[class_id]
                if len(available_indices) >= main_samples_per_class:
                    selected = np.random.choice(available_indices, main_samples_per_class, replace=False)
                else:
                    selected = np.random.choice(available_indices, main_samples_per_class, replace=True)
                client_indices.extend(selected)
            
            # 20%æ¥è‡ªæ¬¡è¦ç±»åˆ«
            minor_samples = samples_per_client - len(client_indices)
            if minor_samples > 0 and len(minor_classes) > 0:
                minor_samples_per_class = minor_samples // len(minor_classes)
                for class_id in minor_classes:
                    available_indices = class_indices[class_id]
                    if len(available_indices) >= minor_samples_per_class:
                        selected = np.random.choice(available_indices, minor_samples_per_class, replace=False)
                    else:
                        selected = np.random.choice(available_indices, minor_samples_per_class, replace=True)
                    client_indices.extend(selected)
            
            # å¦‚æœè¿˜å·®ä¸€äº›æ ·æœ¬ï¼Œéšæœºè¡¥å……
            while len(client_indices) < samples_per_client:
                remaining_samples = samples_per_client - len(client_indices)
                all_available = np.concatenate([class_indices[c] for c in (main_classes + minor_classes)])
                additional = np.random.choice(all_available, min(remaining_samples, len(all_available)), replace=False)
                client_indices.extend(additional)
            
            # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
            client_indices = client_indices[:samples_per_client]
            
            dict_users[client_id] = set(client_indices)
            
            # è®¡ç®—æ­¤å®¢æˆ·ç«¯çš„ç±»åˆ«åˆ†å¸ƒ
            client_labels = labels[client_indices]
            unique_classes = np.unique(client_labels).tolist()
            client_classes[client_id] = unique_classes
        
        # æ‰“å°åˆ†å¸ƒç»Ÿè®¡
        print(f"\nğŸ“ˆ æ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
        for client_id in range(min(5, args.num_users)):  # åªæ˜¾ç¤ºå‰5ä¸ªå®¢æˆ·ç«¯
            dist_type = distribution_types[client_id]
            classes = client_classes[client_id]
            print(f"  å®¢æˆ·ç«¯{client_id} (ç±»å‹{dist_type}): {len(classes)}ä¸ªç±»åˆ« {classes}")
        
        if args.num_users > 5:
            print(f"  ... ä»¥åŠå…¶ä»– {args.num_users - 5} ä¸ªå®¢æˆ·ç«¯")
    
    # ä¿å­˜æ•°æ®åˆ†é…åˆ°æ–‡ä»¶
    if hasattr(args, 'save_data') and args.save_data:
        save_client_data_distribution(dict_users, client_classes, args)
    
    print("âœ… æµ‹è¯•ç‰ˆæ•°æ®åˆ†é…ç”Ÿæˆå®Œæˆ")
    return dataset_train, dataset_test, dict_users, client_classes


if __name__ == '__main__':
    dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True,
                                   transform=transforms.Compose([
                                       transforms.ToTensor(),
                                       transforms.Normalize((0.1307,), (0.3081,))
                                   ]))
    num = 100
    d = mnist_noniid(dataset_train, num)
