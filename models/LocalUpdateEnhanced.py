#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„æœ¬åœ°æ›´æ–°ç±» - æ”¯æŒå®¢æˆ·ç«¯æœ¬åœ°æµ‹è¯•åŠŸèƒ½
æ‰©å±•åŸæœ‰çš„LocalUpdateç±»ä»¥æ”¯æŒåŒé‡è¯„ä¼°æ¶æ„
"""

import torch
from torch import nn, autograd
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
import numpy as np
import random
from models.Update import LocalUpdate, DatasetSplit, restricted_softmax


class LocalUpdateWithLocalTest(LocalUpdate):
    """
    æ”¯æŒæœ¬åœ°æµ‹è¯•çš„å¢å¼ºç‰ˆLocalUpdateç±»
    åœ¨åŸæœ‰è®­ç»ƒåŠŸèƒ½åŸºç¡€ä¸Šï¼Œæ·»åŠ å®¢æˆ·ç«¯æœ¬åœ°æµ‹è¯•èƒ½åŠ›
    """
    
    def __init__(self, args, dataset=None, train_idxs=None, test_idxs=None, user_classes=None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆæœ¬åœ°æ›´æ–°å™¨
        
        Args:
            args: å‚æ•°é…ç½®
            dataset: å®Œæ•´æ•°æ®é›†
            train_idxs: å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒæ•°æ®ç´¢å¼•
            test_idxs: å®¢æˆ·ç«¯æœ¬åœ°æµ‹è¯•æ•°æ®ç´¢å¼•  
            user_classes: å®¢æˆ·ç«¯æ‹¥æœ‰çš„ç±»åˆ«ä¿¡æ¯ï¼ˆç”¨äºFedRSï¼‰
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆä½†éœ€è¦é‡å†™æ•°æ®åŠ è½½å™¨éƒ¨åˆ†ï¼‰
        self.args = args
        self.loss_func = nn.CrossEntropyLoss()
        self.selected_clients = []
        self.user_classes = user_classes
        
        # åˆ›å»ºæœ¬åœ°è®­ç»ƒæ•°æ®åŠ è½½å™¨
        if train_idxs is not None:
            self.ldr_train = DataLoader(
                DatasetSplit(dataset, train_idxs), 
                batch_size=self.args.local_bs, 
                shuffle=True
            )
            self.train_data_size = len(train_idxs)
        else:
            self.ldr_train = None
            self.train_data_size = 0
        
        # åˆ›å»ºæœ¬åœ°æµ‹è¯•æ•°æ®åŠ è½½å™¨
        if test_idxs is not None:
            self.ldr_test = DataLoader(
                DatasetSplit(dataset, test_idxs), 
                batch_size=self.args.bs, 
                shuffle=False
            )
            self.test_data_size = len(test_idxs)
        else:
            self.ldr_test = None
            self.test_data_size = 0
        
        print(f"å®¢æˆ·ç«¯æœ¬åœ°æ•°æ®: è®­ç»ƒé›†={self.train_data_size}, æµ‹è¯•é›†={self.test_data_size}")
    
    def train(self, net):
        """
        å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒï¼ˆç»§æ‰¿åŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            net: ç¥ç»ç½‘ç»œæ¨¡å‹
            
        Returns:
            tuple: (æ¨¡å‹å‚æ•°, å¹³å‡æŸå¤±)
        """
        if self.ldr_train is None:
            print("è­¦å‘Š: å®¢æˆ·ç«¯æ²¡æœ‰è®­ç»ƒæ•°æ®")
            return net.state_dict(), 0.0
        
        net.train()
        
        # ä¸º FedRS ç®—æ³•æ”¯æŒåŠ¨æ€ local epochs
        if self.args.method == 'fedrs' and hasattr(self.args, 'min_le') and hasattr(self.args, 'max_le'):
            local_ep = random.randint(self.args.min_le, self.args.max_le)
        else:
            local_ep = self.args.local_ep
        
        # train and update
        optimizer = torch.optim.SGD(net.parameters(), lr=self.args.lr, momentum=self.args.momentum)

        epoch_loss = []
        for iter in range(local_ep):
            batch_loss = []
            for batch_idx, (images, labels) in enumerate(self.ldr_train):
                images, labels = images.to(self.args.device), labels.to(self.args.device)
                net.zero_grad()
                log_probs = net(images)
                
                # FedRS ç®—æ³•: åº”ç”¨å—é™åˆ¶çš„ softmax
                if self.args.method == 'fedrs' and self.user_classes is not None:
                    log_probs = restricted_softmax(log_probs, self.user_classes, self.args)
                
                loss = self.loss_func(log_probs, labels)
                loss.backward()
                optimizer.step()
                
                if self.args.verbose and batch_idx % 10 == 0:
                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                        iter, batch_idx * len(images), len(self.ldr_train.dataset),
                               100. * batch_idx / len(self.ldr_train), loss.item()))
                batch_loss.append(loss.item())
            epoch_loss.append(sum(batch_loss)/len(batch_loss))
        
        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)
    
    def local_test(self, net):
        """
        å®¢æˆ·ç«¯æœ¬åœ°æµ‹è¯•åŠŸèƒ½
        
        Args:
            net: ç¥ç»ç½‘ç»œæ¨¡å‹
            
        Returns:
            tuple: (å‡†ç¡®ç‡, æŸå¤±)
        """
        if self.ldr_test is None:
            print("è­¦å‘Š: å®¢æˆ·ç«¯æ²¡æœ‰æœ¬åœ°æµ‹è¯•æ•°æ®")
            return 0.0, float('inf')
        
        net.eval()
        test_loss = 0
        correct = 0
        
        with torch.no_grad():
            for data, target in self.ldr_test:
                data, target = data.to(self.args.device), target.to(self.args.device)
                output = net(data)
                
                # sum up batch loss
                test_loss += F.cross_entropy(output, target, reduction='sum').item()
                
                # get the index of the max log-probability
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        test_loss /= len(self.ldr_test.dataset)
        accuracy = 100. * correct / len(self.ldr_test.dataset)
        
        if self.args.verbose:
            print(f'æœ¬åœ°æµ‹è¯•ç»“æœ: å‡†ç¡®ç‡: {correct}/{len(self.ldr_test.dataset)} ({accuracy:.2f}%), å¹³å‡æŸå¤±: {test_loss:.4f}')
        
        return accuracy, test_loss
    
    def get_data_statistics(self):
        """
        è·å–å®¢æˆ·ç«¯æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            dict: æ•°æ®ç»Ÿè®¡
        """
        stats = {
            'train_size': self.train_data_size,
            'test_size': self.test_data_size,
            'total_size': self.train_data_size + self.test_data_size,
            'test_ratio': self.test_data_size / (self.train_data_size + self.test_data_size) if (self.train_data_size + self.test_data_size) > 0 else 0
        }
        
        # åˆ†ææ ‡ç­¾åˆ†å¸ƒï¼ˆå¦‚æœæœ‰æœ¬åœ°è®­ç»ƒæ•°æ®ï¼‰
        if self.ldr_train is not None:
            label_counts = {}
            for _, labels in self.ldr_train:
                for label in labels:
                    label_item = label.item()
                    label_counts[label_item] = label_counts.get(label_item, 0) + 1
            stats['train_label_distribution'] = label_counts
        
        # åˆ†ææµ‹è¯•æ ‡ç­¾åˆ†å¸ƒï¼ˆå¦‚æœæœ‰æœ¬åœ°æµ‹è¯•æ•°æ®ï¼‰
        if self.ldr_test is not None:
            label_counts = {}
            for _, labels in self.ldr_test:
                for label in labels:
                    label_item = label.item()
                    label_counts[label_item] = label_counts.get(label_item, 0) + 1
            stats['test_label_distribution'] = label_counts
        
        return stats
    
    def validate_data_consistency(self):
        """
        éªŒè¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ•°æ®ä¸€è‡´æ€§
        æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒæ˜¯å¦åˆç†
        
        Returns:
            dict: éªŒè¯ç»“æœ
        """
        validation_result = {
            'is_valid': True,
            'issues': [],
            'recommendations': []
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if self.train_data_size == 0:
            validation_result['is_valid'] = False
            validation_result['issues'].append("å®¢æˆ·ç«¯ç¼ºå°‘è®­ç»ƒæ•°æ®")
        
        if self.test_data_size == 0:
            validation_result['issues'].append("å®¢æˆ·ç«¯ç¼ºå°‘æµ‹è¯•æ•°æ®")
            validation_result['recommendations'].append("å»ºè®®å¢åŠ æµ‹è¯•æ•°æ®æ¯”ä¾‹")
        
        # æ£€æŸ¥æ•°æ®æ¯”ä¾‹æ˜¯å¦åˆç†
        total_size = self.train_data_size + self.test_data_size
        if total_size > 0:
            test_ratio = self.test_data_size / total_size
            if test_ratio < 0.1:
                validation_result['recommendations'].append(f"æµ‹è¯•é›†æ¯”ä¾‹è¾ƒä½ ({test_ratio:.2%})")
            elif test_ratio > 0.4:
                validation_result['recommendations'].append(f"æµ‹è¯•é›†æ¯”ä¾‹è¾ƒé«˜ ({test_ratio:.2%})")
        
        return validation_result


def create_enhanced_local_updates(args, dataset, dict_users_train, dict_users_test, client_classes):
    """
    æ‰¹é‡åˆ›å»ºå¢å¼ºç‰ˆæœ¬åœ°æ›´æ–°å™¨
    
    Args:
        args: å‚æ•°é…ç½®
        dataset: å®Œæ•´æ•°æ®é›†
        dict_users_train: å®¢æˆ·ç«¯è®­ç»ƒæ•°æ®æ˜ å°„
        dict_users_test: å®¢æˆ·ç«¯æµ‹è¯•æ•°æ®æ˜ å°„  
        client_classes: å®¢æˆ·ç«¯ç±»åˆ«æ˜ å°„
        
    Returns:
        dict: {client_id: LocalUpdateWithLocalTest}
    """
    enhanced_local_updates = {}
    
    print(f"\nåˆ›å»º {len(dict_users_train)} ä¸ªå¢å¼ºç‰ˆæœ¬åœ°æ›´æ–°å™¨...")
    
    for client_id in dict_users_train.keys():
        train_idxs = dict_users_train.get(client_id, set())
        test_idxs = dict_users_test.get(client_id, set())
        user_classes = client_classes.get(client_id, [])
        
        enhanced_local_updates[client_id] = LocalUpdateWithLocalTest(
            args=args,
            dataset=dataset,
            train_idxs=train_idxs,
            test_idxs=test_idxs,
            user_classes=user_classes
        )
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§ï¼ˆåªå¯¹å‰å‡ ä¸ªå®¢æˆ·ç«¯æ˜¾ç¤ºè¯¦æƒ…ï¼‰
        if client_id < 3:
            validation = enhanced_local_updates[client_id].validate_data_consistency()
            if not validation['is_valid'] or validation['issues']:
                print(f"  å®¢æˆ·ç«¯ {client_id} æ•°æ®éªŒè¯:")
                for issue in validation['issues']:
                    print(f"    âš ï¸  {issue}")
                for rec in validation['recommendations']:
                    print(f"    ğŸ’¡ {rec}")
    
    print("å¢å¼ºç‰ˆæœ¬åœ°æ›´æ–°å™¨åˆ›å»ºå®Œæˆ\n")
    return enhanced_local_updates


if __name__ == "__main__":
    print("LocalUpdateWithLocalTest æµ‹è¯•")
    # è¿™é‡Œå¯ä»¥æ·»åŠ å•å…ƒæµ‹è¯•ä»£ç 