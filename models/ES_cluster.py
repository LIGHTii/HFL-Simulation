import numpy as np
import matplotlib.pyplot as plt
import torch
import copy
import torch.multiprocessing as mp
from tqdm import tqdm
from models.Update import LocalUpdate  # å¯¼å…¥LocalUpdate
from models.cluster2 import cluster2
from models.Nets import MLP, CNNMnist, CNNCifar, LR, ResNet18, VGG11, MobileNetCifar, LeNet5
# ==================================== ESèšç±» ========================================
def model_to_vector(model_params):
    """å°†æ¨¡å‹å‚æ•°å­—å…¸è½¬æ¢ä¸ºå‘é‡"""
    vectors = []
    for param in model_params.values():
        # å°†å‚æ•°è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å±•å¹³
        vectors.append(param.cpu().numpy().flatten())
    return np.concatenate(vectors)

def calculate_es_label_distributions(A, client_label_distributions):
    """
    è®¡ç®—æ¯ä¸ªESçš„æ ‡ç­¾åˆ†å¸ƒï¼Œé€šè¿‡æ±‡æ€»è¿æ¥åˆ°è¯¥ESçš„æ‰€æœ‰å®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒ

    å‚æ•°:
        A: å®¢æˆ·ç«¯-ESå…³è”çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_clients, n_es)
        client_label_distributions: æ¯ä¸ªå®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒï¼Œå½¢çŠ¶ä¸º(n_clients, n_classes)

    è¿”å›:
        es_label_distributions: æ¯ä¸ªESçš„æ ‡ç­¾åˆ†å¸ƒï¼Œå½¢çŠ¶ä¸º(n_es, n_classes)
    """
    n_es = A.shape[1]
    n_classes = client_label_distributions.shape[1]  # åŠ¨æ€è·å–ç±»åˆ«æ•°
    es_label_distributions = np.zeros((n_es, n_classes))

    for es_idx in range(n_es):
        # æ‰¾åˆ°è¿æ¥åˆ°å½“å‰ESçš„æ‰€æœ‰å®¢æˆ·ç«¯
        client_indices = np.where(A[:, es_idx] == 1)[0]

        if len(client_indices) > 0:
            # æ±‡æ€»è¿™äº›å®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒ
            es_label_distributions[es_idx] = np.sum(
                client_label_distributions[client_indices], axis=0
            )

    return es_label_distributions

def visualize_clustering_comparison(es_label_distributions, cluster_labels,
                                    save_path='./save/clustering_comparison.png'):
    """
    å¯¹æ¯”è°±èšç±»åˆ†ç°‡å’Œéšæœºåˆ†ç°‡çš„æ•ˆæœï¼Œä½¿ç”¨æ ‡ç­¾åˆ†å¸ƒçš„å †å æŸ±çŠ¶å›¾
    """
    n_es = len(es_label_distributions)
    n_clusters = len(np.unique(cluster_labels))
    n_classes = es_label_distributions.shape[1]

    print(f"[å¯è§†åŒ–] ESæ•°é‡: {n_es}, ç°‡æ•°: {n_clusters}, ç±»åˆ«æ•°: {n_classes}")

    # éšæœºåˆ†ç°‡ï¼Œæ¯ä¸ª ES éšæœºåˆ†åˆ° 0~n_clusters-1 çš„ç°‡
    np.random.seed(42)  # å›ºå®šç§å­ä»¥ä¾¿é‡ç°
    random_cluster_labels = np.random.randint(0, n_clusters, size=n_es)

    # åˆ›å»ºå›¾å½¢ï¼ŒåŒ…å«ä¸¤ä¸ªå­å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), constrained_layout=True)
    fig.suptitle('Comparison of Spectral Clustering vs Random Clustering', fontsize=16)

    cmap = plt.cm.viridis

    # ã€ä¿®å¤ã€‘é¢„å…ˆç”Ÿæˆé¢œè‰²æ˜ å°„
    class_colors = [cmap(i / max(n_classes - 1, 1)) for i in range(n_classes)]
    print(f"[å¯è§†åŒ–] ç”Ÿæˆ {n_classes} ç§ä¸åŒé¢œè‰²")

    # ç»˜åˆ¶è°±èšç±»ç»“æœï¼ˆå·¦ä¾§å­å›¾ï¼‰
    _plot_gradient_clustering_result(
        ax1, es_label_distributions, cluster_labels,
        "Spectral Clustering Result", class_colors, n_classes
    )

    # ç»˜åˆ¶éšæœºåˆ†ç°‡ç»“æœï¼ˆå³ä¾§å­å›¾ï¼‰
    _plot_gradient_clustering_result(
        ax2, es_label_distributions, random_cluster_labels,
        "Random Clustering Result", class_colors, n_classes
    )

    # åœ¨æ‰€æœ‰å­å›¾ä¸‹æ–¹æ·»åŠ è¿ç»­é¢œè‰²æ¡
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=n_classes-1))
    sm.set_array([])
    cbar = fig.colorbar(sm, ax=[ax1, ax2], orientation='horizontal', fraction=0.05, pad=0.12)
    cbar.ax.tick_params(labelsize=8)

    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"Clustering comparison visualization saved to: {save_path}")


def _plot_gradient_clustering_result(ax, distributions, labels, title, class_colors, n_classes):
    """
    ç»˜åˆ¶å †å æŸ±çŠ¶å›¾ï¼Œæ¯ä¸ªæ ‡ç­¾ä½¿ç”¨ä¸åŒçš„æ¸å˜è‰²ï¼Œå®ç°çœŸæ­£çš„100ç§é¢œè‰²æ•ˆæœ

    å‚æ•°:
        ax: matplotlibè½´å¯¹è±¡
        distributions: æ ‡ç­¾åˆ†å¸ƒæ•°æ®ï¼Œå½¢çŠ¶(n_es, n_classes)
        labels: èšç±»æ ‡ç­¾
        title: å­å›¾æ ‡é¢˜
        class_colors: é¢„ç”Ÿæˆçš„100ç§é¢œè‰²åˆ—è¡¨
        n_classes: ç±»åˆ«æ•°
    """
    unique_clusters = np.unique(labels)

    # æŒ‰èšç±»ç»“æœæ’åº
    sorted_indices = np.argsort(labels)
    sorted_labels = labels[sorted_indices]
    sorted_distributions = distributions[sorted_indices]

    # è®¡ç®—æ¯ä¸ªèšç±»çš„è¾¹ç•Œ
    cluster_boundaries = []
    cluster_sizes = {}
    start_idx = 0
    for cluster_id in unique_clusters:
        cluster_size = np.sum(sorted_labels == cluster_id)
        cluster_boundaries.append((start_idx, start_idx + cluster_size - 1))
        cluster_sizes[cluster_id] = cluster_size
        start_idx += cluster_size

    # åˆ›å»ºxåæ ‡ï¼ˆæ¯ä¸ªç°‡ä¹‹é—´æœ‰é—´éš™ï¼‰
    x_positions = []
    current_x = 0
    for cluster_id in unique_clusters:
        for _ in range(cluster_sizes[cluster_id]):
            x_positions.append(current_x)
            current_x += 1
        current_x += 0.5  # ç°‡é—´ é—´éš™
    x = np.array(x_positions)

    # è®¡ç®—æ¯ä¸ªESçš„æ€»æ ·æœ¬æ•°
    total_samples = np.sum(sorted_distributions, axis=1)

    # ç»˜åˆ¶å †å æŸ±çŠ¶å›¾ - æ¯ä¸ªæ ‡ç­¾ä¸€ä¸ªé¢œè‰²
    bottom = np.zeros(len(sorted_distributions))

    for class_idx in range(n_classes):
        # å½“å‰æ ‡ç­¾åœ¨æ‰€æœ‰ESä¸­çš„æ ·æœ¬æ•°
        class_samples = sorted_distributions[:, class_idx]

        # åªç»˜åˆ¶æœ‰æ•°æ®çš„æ ‡ç­¾
        if np.sum(class_samples) > 0:
            # ä¸ºæ¯ä¸ªESç»˜åˆ¶è¯¥æ ‡ç­¾çš„å †å éƒ¨åˆ†
            for es_idx, (x_pos, samples, bottom_pos) in enumerate(zip(x, class_samples, bottom)):
                if samples > 0:
                    # ä½¿ç”¨é¢„ç”Ÿæˆçš„é¢œè‰²
                    color = class_colors[class_idx]
                    # ç»˜åˆ¶å †å æ¡å½¢å›¾
                    bar = ax.bar(x_pos, samples, bottom=bottom_pos,
                                 color=color, width=0.8, alpha=0.85,
                                 edgecolor='none', linewidth=0)
            # æ›´æ–°åº•éƒ¨ä½ç½®
            bottom += class_samples

    # è®¾ç½®xè½´ï¼ˆç°‡æ ‡ç­¾ï¼‰
    cluster_centers = []
    for boundary in cluster_boundaries:
        left_idx = boundary[0]
        right_idx = boundary[1]
        if left_idx <= right_idx:
            center = (x[left_idx] + x[right_idx]) / 2
        else:
            center = x[left_idx]
        cluster_centers.append(center)

    ax.set_xticks(cluster_centers)
    ax.set_xticklabels([f'Cluster {int(i)}' for i in unique_clusters], fontsize=10)

    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title(title, fontsize=12, pad=20)
    ax.set_xlabel('Edge Server Cluster', fontsize=10)
    ax.set_ylabel('Number of Samples', fontsize=10)

    # è®¾ç½®yè½´èŒƒå›´
    max_height = np.max(bottom) * 1.05
    ax.set_ylim(0, max_height)

    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, linestyle='--', alpha=0.3, axis='y')


def FedAvg_weighted(models, sizes=None):
    """
    æ”¯æŒåŠ æƒçš„è”é‚¦å¹³å‡ç®—æ³•

    å‚æ•°:
        models: æ¨¡å‹å‚æ•°åˆ—è¡¨
        sizes: æ¯ä¸ªæ¨¡å‹å¯¹åº”çš„æ•°æ®é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç®€å•å¹³å‡

    è¿”å›:
        w_avg: å¹³å‡åçš„æ¨¡å‹å‚æ•°
    """
    if sizes is None:
        sizes = [1] * len(models)

    total_size = sum(sizes)
    w_avg = copy.deepcopy(models[0])

    for k in w_avg.keys():
        w_avg[k] = w_avg[k] * sizes[0]
        for i in range(1, len(models)):
            w_avg[k] += models[i][k] * sizes[i]
        w_avg[k] = torch.div(w_avg[k], total_size)

    return w_avg


def train_initial_models(args, dataset_train, dict_users, net_glob, num_users):
    """è®­ç»ƒåˆå§‹æœ¬åœ°æ¨¡å‹ï¼Œç”¨äºæ„å»ºESç›¸ä¼¼åº¦å›¾ """
    w_locals = []
    client_label_distributions = []  # å­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒ

    print("Training initial local models for graph construction...")

    for user_idx in range(num_users):
        # åˆ›å»ºæœ¬åœ°æ›´æ–°å®ä¾‹
        local = LocalUpdate(
            args=args,
            dataset=dataset_train,
            idxs=dict_users[user_idx]
        )

        # å¤åˆ¶å…¨å±€æ¨¡å‹ä½œä¸ºæœ¬åœ°æ¨¡å‹çš„åˆå§‹çŠ¶æ€
        local_net = copy.deepcopy(net_glob)

        # è®­ç»ƒæœ¬åœ°æ¨¡å‹
        w_local, loss_local = local.train(net=local_net.to(args.device))

        w_locals.append(copy.deepcopy(w_local))

        # è®¡ç®—è¯¥å®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒ
        labels = [dataset_train[i][1] for i in dict_users[user_idx]]
        label_count = np.zeros(args.num_classes)  # åŠ¨æ€ç±»åˆ«æ•°
        for label in labels:
            label_count[label] += 1
        client_label_distributions.append(label_count)

    return w_locals, np.array(client_label_distributions)

def aggregate_es_models(w_locals, A, dict_users, net_glob):
    """
    æ ¹æ®å®¢æˆ·ç«¯-ESå…³è”çŸ©é˜µAï¼Œèšåˆæ¯ä¸ªESçš„æ¨¡å‹

    å‚æ•°:
        w_locals: æ‰€æœ‰å®¢æˆ·ç«¯çš„æ¨¡å‹å‚æ•°åˆ—è¡¨
        A: å®¢æˆ·ç«¯-ESå…³è”çŸ©é˜µ
        dict_users: å®¢æˆ·ç«¯æ•°æ®ç´¢å¼•å­—å…¸
        net_glob: å…¨å±€æ¨¡å‹ï¼ˆç”¨äºåˆå§‹åŒ–ç©ºESï¼‰

    è¿”å›:
        es_models: æ¯ä¸ªESçš„èšåˆæ¨¡å‹å‚æ•°åˆ—è¡¨
    """
    num_ESs = A.shape[1]
    es_models = [None] * num_ESs

    for es_idx in range(num_ESs):
        # æ‰¾åˆ°è¿æ¥åˆ°å½“å‰ESçš„æ‰€æœ‰å®¢æˆ·ç«¯
        client_indices = np.where(A[:, es_idx] == 1)[0]

        if len(client_indices) > 0:
            # è·å–è¿™äº›å®¢æˆ·ç«¯çš„æ¨¡å‹
            client_models = [w_locals[i] for i in client_indices]

            # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®é‡ï¼ˆç”¨äºåŠ æƒå¹³å‡ï¼‰
            client_sizes = [len(dict_users[i]) for i in client_indices]

            # ä½¿ç”¨åŠ æƒå¹³å‡è¿›è¡Œèšåˆ
            w_es = FedAvg_weighted(client_models, client_sizes)
            es_models[es_idx] = w_es
        else:
            # å¦‚æœESæ²¡æœ‰è¿æ¥ä»»ä½•å®¢æˆ·ç«¯ï¼Œè®¾ä¸ºNone
            es_models[es_idx] = None #copy.deepcopy(net_glob.state_dict())

    return es_models


def spectral_clustering_es(es_models, epsilon=None):
    """
    å¯¹è¾¹ç¼˜æœåŠ¡å™¨æ¨¡å‹è¿›è¡Œè°±èšç±»

    å‚æ•°:
        es_models: è¾¹ç¼˜æœåŠ¡å™¨æ¨¡å‹åˆ—è¡¨
        epsilon: ç°‡å†…è·ç¦»é˜ˆå€¼

    è¿”å›:
        B: ES-EHå…³è”çŸ©é˜µ
        cluster_labels: èšç±»æ ‡ç­¾
    """
    print("[è°±èšç±»] å¼€å§‹å¯¹ESæ¨¡å‹è¿›è¡Œè°±èšç±»...")

    # å°†æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºå‘é‡
    model_vectors = []
    for i, model in enumerate(es_models):
        vec = model_to_vector(model)
        model_vectors.append(vec)
        print(f"[è°±èšç±»] ES {i} æ¨¡å‹å‘é‡ç»´åº¦: {vec.shape}")

    model_vectors = np.array(model_vectors)
    print(f"[è°±èšç±»] æ¨¡å‹å‘é‡çŸ©é˜µå½¢çŠ¶: {model_vectors.shape}")

    # è¿›è¡Œè°±èšç±»
    cluster_num, cluster_labels = cluster2(model_vectors, epsilon=epsilon)
    print(f"[è°±èšç±»] è‡ªåŠ¨ç¡®å®šçš„æœ€ä½³ç°‡æ•°: {cluster_num}")

    # æ„å»ºBçŸ©é˜µ
    num_ESs = len(es_models)
    B = np.zeros((num_ESs, cluster_num), dtype=int)
    for es_idx, label in enumerate(cluster_labels):
        B[es_idx, label] = 1

    # æ‰“å°èšç±»åˆ†é…ç»“æœ
    print("[è°±èšç±»] ESèšç±»åˆ†é…ç»“æœ:")
    for es_idx, label in enumerate(cluster_labels):
        print(f"  ES {es_idx} -> EH {label}")

    return B, cluster_labels

def train_single_client_for_es(args, user_idx, dataset_train, dict_users, w_input, net_glob):
    """
    å•ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒå‡½æ•°ï¼Œç”¨äºESåˆå§‹åŒ–é˜¶æ®µçš„å¤šè¿›ç¨‹è®­ç»ƒ
    
    Args:
        args: è®­ç»ƒå‚æ•°
        user_idx: å®¢æˆ·ç«¯ç´¢å¼•
        dataset_train: è®­ç»ƒæ•°æ®é›†
        dict_users: å®¢æˆ·ç«¯æ•°æ®ç´¢å¼•å­—å…¸
        w_input: è¾“å…¥æ¨¡å‹æƒé‡
        net_glob: å…¨å±€æ¨¡å‹æ¶æ„ï¼ˆç”¨äºé‡æ–°æ„å»ºæ¨¡å‹ï¼‰
    
    Returns:
        tuple: (user_idx, w_local, loss_local)
    """
    # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°æ„å»ºæ¨¡å‹æ¶æ„
    if hasattr(dataset_train, '__getitem__'):
        img_size = dataset_train[0][0].shape
    else:
        # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if args.dataset == 'mnist':
            img_size = (1, 28, 28)
        elif args.dataset in ['cifar', 'cifar100']:
            img_size = (3, 32, 32)
        else:
            img_size = (1, 28, 28)  # é»˜è®¤å€¼

    if args.model == 'cnn':
        if args.dataset in ['cifar', 'cifar100']:
            local_net = CNNCifar(args=args).to(args.device)
        elif args.dataset == 'mnist':
            local_net = CNNMnist(args=args).to(args.device)
    elif args.model == 'mlp':
        len_in = 1
        for x in img_size:
            len_in *= x
        local_net = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)
    elif args.model == 'lr' and args.dataset == 'mnist':
        len_in = 1
        for x in img_size:
            len_in *= x
        local_net = LR(dim_in=len_in, dim_out=args.num_classes).to(args.device)
    elif args.model == 'lenet5' and args.dataset == 'mnist':
        local_net = LeNet5(args=args).to(args.device)
    elif args.model == 'vgg11' and args.dataset in ['cifar', 'cifar100']:
        local_net = VGG11(args=args).to(args.device)
    elif args.model == 'resnet18' and args.dataset in ['cifar', 'cifar100']:
        local_net = ResNet18(args=args).to(args.device)
    else:
        # é»˜è®¤ä½¿ç”¨CNN
        if args.dataset in ['cifar', 'cifar100']:
            local_net = CNNCifar(args=args).to(args.device)
        else:
            local_net = CNNMnist(args=args).to(args.device)
    
    # åˆ›å»ºæœ¬åœ°æ›´æ–°å®ä¾‹
    local = LocalUpdate(
        args=args,
        dataset=dataset_train,
        idxs=dict_users[user_idx]
    )
    
    # åŠ è½½è¾“å…¥æƒé‡
    local_net.load_state_dict(w_input)
    
    # è®­ç»ƒæœ¬åœ°æ¨¡å‹
    w_local, loss_local = local.train(net=local_net.to(args.device))
    
    # è¿”å›ç»“æœï¼ŒåŒ…æ‹¬ user_idx ä»¥ä¾¿åç»­æ’åº
    return (user_idx, copy.deepcopy(w_local), loss_local)

def train_initial_models_with_es_aggregation(args, dataset_train, dict_users, net_glob, A_design, num_users):
    """
    è®­ç»ƒåˆå§‹æœ¬åœ°æ¨¡å‹å¹¶èšåˆåˆ°ESå±‚ï¼Œéµå¾ªè”é‚¦å­¦ä¹ æœºåˆ¶
    
    Args:
        args: è®­ç»ƒå‚æ•°
        dataset_train: è®­ç»ƒæ•°æ®é›†
        dict_users: å®¢æˆ·ç«¯æ•°æ®ç´¢å¼•å­—å…¸
        net_glob: å…¨å±€æ¨¡å‹
        A_design: å®¢æˆ·ç«¯-ESå…³è”çŸ©é˜µ
        num_users: å®¢æˆ·ç«¯æ•°é‡
    
    Returns:
        es_models: ESå±‚èšåˆåçš„æ¨¡å‹åˆ—è¡¨
        client_label_distributions: å®¢æˆ·ç«¯æ ‡ç­¾åˆ†å¸ƒ
    """
    from models.Fed import FedAvg_layered
    
    print("Training initial local models with ES aggregation for clustering...")
    print(f"æ¯ä¸ªå®¢æˆ·ç«¯è®­ç»ƒ {args.local_ep} è½®æœ¬åœ°æ›´æ–°")
    print(f"ESå±‚èšåˆ {args.ES_k2} è½®")
    print("ğŸ“Š ESå±‚èšåˆå°†ä½¿ç”¨åŸºäºæ•°æ®é‡çš„åŠ æƒå¹³å‡")
    
    # æ„å»ºC1å±‚çº§ç»“æ„ï¼ˆå®¢æˆ·ç«¯->ESæ˜ å°„ï¼‰
    num_ESs = A_design.shape[1]
    C1 = {j: [] for j in range(num_ESs)}
    for i in range(num_users):
        for j in range(num_ESs):
            if A_design[i][j] == 1:
                C1[j].append(i)
    
    print(f"å®¢æˆ·ç«¯-ESæ˜ å°„å…³ç³»: {dict(C1)}")
    
    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®é‡
    client_data_counts = {}
    for client_id, data_indices in dict_users.items():
        if isinstance(data_indices, set):
            client_data_counts[client_id] = len(data_indices)
        elif isinstance(data_indices, np.ndarray):
            client_data_counts[client_id] = len(data_indices)
        else:
            client_data_counts[client_id] = len(list(data_indices))
    
    print(f"ğŸ“Š å®¢æˆ·ç«¯æ•°æ®é‡ç»Ÿè®¡: æ€»æ•°={sum(client_data_counts.values())}, å¹³å‡={np.mean(list(client_data_counts.values())):.1f}")
    print(f"ğŸ“Š æ•°æ®é‡èŒƒå›´: [{min(client_data_counts.values())}, {max(client_data_counts.values())}]")
    
    # åˆå§‹åŒ–ESå±‚æ¨¡å‹æƒé‡ï¼ˆä»å…¨å±€æ¨¡å‹å¼€å§‹ï¼‰
    w_glob = net_glob.state_dict()
    ESs_ws = [copy.deepcopy(w_glob) for _ in range(num_ESs)]
    
    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æ ‡ç­¾åˆ†å¸ƒ
    client_label_distributions = []
    for user_idx in range(num_users):
        labels = [dataset_train[i][1] for i in dict_users[user_idx]]
        label_count = np.zeros(args.num_classes)
        for label in labels:
            label_count[label] += 1
        client_label_distributions.append(label_count)
    
    # ESå±‚èšåˆk2è½®
    for es_round in range(args.ES_k2):
        print(f"\n--- ESèšåˆè½®æ¬¡ {es_round + 1}/{args.ES_k2} ---")
        
        # ESå±‚->å®¢æˆ·ç«¯å±‚æƒé‡åˆ†å‘
        w_locals_input = [None] * num_users
        for ES_idx, user_indices in C1.items():
            for user_idx in user_indices:
                w_locals_input[user_idx] = copy.deepcopy(ESs_ws[ES_idx])
        
        # å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ - ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ
        w_locals_output = [None] * num_users
        
        # å‡†å¤‡å¹¶è¡Œè®­ç»ƒä»»åŠ¡
        print(f"  å¼€å§‹å¹¶è¡Œè®­ç»ƒ {num_users} ä¸ªå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ 5 ä¸ªè¿›ç¨‹...")
        
        # å‡†å¤‡ä¼ é€’ç»™æ¯ä¸ªå­è¿›ç¨‹çš„å‚æ•°
        tasks = []
        for user_idx in range(num_users):
            task_args = (
                args, user_idx, dataset_train, dict_users,
                w_locals_input[user_idx], net_glob
            )
            tasks.append(task_args)
        
        # åˆ›å»ºè¿›ç¨‹æ± å¹¶åˆ†å‘ä»»åŠ¡
        with mp.Pool(processes=5) as pool:
            results = pool.starmap(train_single_client_for_es, tqdm(tasks, desc=f"ESèšåˆè½®æ¬¡ {es_round + 1} å®¢æˆ·ç«¯è®­ç»ƒ"))
        
        # æ”¶é›†å¹¶æ•´ç†æ‰€æœ‰å®¢æˆ·ç«¯çš„è®­ç»ƒç»“æœï¼ŒæŒ‰user_idxæ’åº
        for result in results:
            u_idx, w_local, loss_local = result
            w_locals_output[u_idx] = w_local
            
            if u_idx < 5:  # åªæ‰“å°å‰5ä¸ªå®¢æˆ·ç«¯çš„æŸå¤±
                print(f"  å®¢æˆ·ç«¯ {u_idx}: æŸå¤± {loss_local:.4f}")
        
        # å®¢æˆ·ç«¯->ESå±‚èšåˆ - ä½¿ç”¨åŠ æƒå¹³å‡
        print(f"  ğŸ“Š å¼€å§‹ESå±‚åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(client_data_counts)}ä¸ªå®¢æˆ·ç«¯æ•°æ®é‡)")
        ESs_ws = FedAvg_layered(w_locals_output, C1, client_data_counts)
        print(f"  ESèšåˆå®Œæˆï¼Œå…±èšåˆåˆ° {len([es for es in ESs_ws if es is not None])} ä¸ªES")
    
    print(f"\nâœ… åˆå§‹è®­ç»ƒå®Œæˆï¼ç»è¿‡ {args.ES_k2} è½®ESåŠ æƒèšåˆï¼Œç”Ÿæˆ {len(ESs_ws)} ä¸ªESæ¨¡å‹ç”¨äºè°±èšç±»")
    print("ğŸ“Š æ‰€æœ‰ESèšåˆå‡å·²ä½¿ç”¨åŸºäºæ•°æ®é‡çš„åŠ æƒå¹³å‡ï¼Œç¡®ä¿è®­ç»ƒè´¨é‡")
    
    return ESs_ws, np.array(client_label_distributions)