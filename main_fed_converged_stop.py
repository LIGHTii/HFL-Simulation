#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6
import os
import matplotlib

from utils.visualization_tool import create_enhanced_visualizations

matplotlib.use('Agg')
# è®¾ç½® matplotlib ä½¿ç”¨è‹±æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡å­—ä½“è­¦å‘Š
matplotlib.rcParams.update({
    'font.family': 'DejaVu Sans',
    'axes.unicode_minus': True  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
})
import matplotlib.pyplot as plt
import copy
import numpy as np
from torchvision import datasets, transforms
import torch
from tqdm import tqdm
import torch.multiprocessing as mp
import os
os.environ["OMP_NUM_THREADS"] = "1"

import csv
from datetime import datetime
import pandas as pd

from utils.sampling import get_data
from utils.options import args_parser
from utils.data_partition import get_client_datasets
from utils.visualize_client_data import visualize_client_data_distribution
from utils.eh_test_utils import EHTestsetGenerator, test_eh_model
from utils.bipartite_bandwidth import run_bandwidth_allocation, calculate_distance
from utils.comm_utils import calculate_transmission_time, get_model_size_in_bits, select_eh
from models.Nets import MLP, CNNMnist, CNNCifar, LR, ResNet18, VGG11, MobileNetCifar, LeNet5
from models.Fed import FedAvg, FedAvg_layered
from models.test import test_img
from models.Update import LocalUpdate
from models.ES_cluster import (
    train_initial_models,
    aggregate_es_models, spectral_clustering_es,
    calculate_es_label_distributions,
    visualize_clustering_comparison
)
from utils.conver_check import ConvergenceChecker
import numpy as np

def build_model(args, dataset_train):
    img_size = dataset_train[0][0].shape

    if args.model == 'cnn':
        if args.dataset in ['cifar', 'cifar100']:  # æ”¯æŒ cifar å’Œ cifar100
            net_glob = CNNCifar(args=args).to(args.device)  # CNNCifar éœ€è¦æ”¯æŒ args.num_classes=100
        elif args.dataset == 'mnist':
            net_glob = CNNMnist(args=args).to(args.device)
    elif args.model == 'mlp':
        # è®¡ç®—å°†å›¾ç‰‡å±•å¹³åçš„è¾“å…¥å±‚ç»´åº¦
        len_in = 1
        for x in img_size:
            len_in *= x
        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)

    # ======================= æ–° =======================
    elif args.model == 'lr' and args.dataset == 'mnist':
        len_in = 1
        for x in img_size:
            len_in *= x
        net_glob = LR(dim_in=len_in, dim_out=args.num_classes).to(args.device)

    elif args.model == 'lenet5' and args.dataset == 'mnist':
        net_glob = LeNet5(args=args).to(args.device)

    elif args.model == 'vgg11' and args.dataset in ['cifar', 'cifar100']:
        net_glob = VGG11(args=args).to(args.device)

    elif args.model == 'resnet18' and args.dataset in ['cifar', 'cifar100']:
        net_glob = ResNet18(args=args).to(args.device)  # ResNet18 éœ€è¦æ”¯æŒ args.num_classes=100

    else:
        exit('é”™è¯¯ï¼šæ— æ³•è¯†åˆ«çš„æ¨¡å‹')

    # print("--- æ¨¡å‹æ¶æ„ ---")
    # print(net_glob)
    # print("--------------------")
    return net_glob

def get_A_random(num_users, num_ESs):
    A = np.zeros((num_users, num_ESs), dtype=int)

    # æ¯ä¸ª ES è‡³å°‘è¦åˆ†åˆ°çš„ç”¨æˆ·æ•°
    base = num_users // num_ESs
    # å¤šå‡ºæ¥çš„ç”¨æˆ·æ•°é‡
    extra = num_users % num_ESs

    # ç”¨æˆ·ç´¢å¼•
    users = np.arange(num_users)
    np.random.shuffle(users)  # æ‰“ä¹±é¡ºåºï¼Œä¿è¯éšæœºæ€§

    start = 0
    for es in range(num_ESs):
        count = base + (1 if es < extra else 0)
        assigned_users = users[start:start+count]
        for u in assigned_users:
            A[u, es] = 1
        start += count

    return A

def get_B(num_ESs, num_EHs):
    B = np.zeros((num_ESs, num_EHs), dtype=int)

    # å¯¹æ¯ä¸€è¡Œéšæœºé€‰æ‹©ä¸€ä¸ªç´¢å¼•ï¼Œå°†è¯¥ä½ç½®è®¾ä¸º 1
    for i in range(num_ESs):
        random_index = np.random.randint(0, num_EHs)
        B[i, random_index] = 1

    return B

def get_B_cluster(args, w_locals, A, dict_users, net_glob, client_label_distributions):
    """
    ä½¿ç”¨è°±èšç±»ç”Ÿæˆ ES-EH å…³è”çŸ©é˜µ Bï¼Œå¹¶å¯è§†åŒ–èšç±»ç»“æœ
    """
    print("å¼€å§‹è°±èšç±»ç”ŸæˆBçŸ©é˜µ...")

    # 1. èšåˆESæ¨¡å‹
    es_models = aggregate_es_models(w_locals, A, dict_users, net_glob)

    # 2. ä½¿ç”¨è°±èšç±»è·å–ES-EHå…³è”çŸ©é˜µB
    B, cluster_labels = spectral_clustering_es(
        es_models,
        epsilon=args.epsilon  # ä»å‚æ•°ä¸­è·å–
    )

    # 3. è®¡ç®—ESçš„æ ‡ç­¾åˆ†å¸ƒå¹¶å¯è§†åŒ–
    es_label_distributions = calculate_es_label_distributions(A, client_label_distributions)

    #labels1, labels2, labels3 = run_all_clusterings(es_models, epsilon=args.epsilon)
    # åœ¨å®Œæˆè°±èšç±»åæ·»åŠ å¯¹æ¯”å¯è§†åŒ–
    visualize_clustering_comparison(
        es_label_distributions=es_label_distributions,
        cluster_labels=cluster_labels,
        save_path='./save/clustering_comparison.png'
    )
    return B

# ===== æ ¹æ® Aã€B æ„é€  C1 å’Œ C2 =====
def build_hierarchy(A, B):
    num_users, num_ESs = A.shape
    _, num_EHs = B.shape

    # client -> ES
    C1 = {j: [] for j in range(num_ESs)}
    for i in range(num_users):
        for j in range(num_ESs):
            if A[i][j] == 1:
                C1[j].append(i)

    # ES -> EH
    C2 = {k: [] for k in range(num_EHs)}
    for j in range(num_ESs):
        for k in range(num_EHs):
            if B[j][k] == 1:
                C2[k].append(j)

    return C1, C2

def train_client(args, user_idx, dataset_train, dict_users, w_input_hfl_random, w_input_hfl_cluster, w_sfl_global, 
                 client_classes=None, train_hfl_random=True, train_hfl_cluster=True, train_sfl=True):
    """
    å•ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒå‡½æ•°ï¼Œç”¨äºè¢«å¤šè¿›ç¨‹è°ƒç”¨ã€‚
    ç°åœ¨æ”¯æŒä¸‰ç§æ¨¡å‹ï¼šSFLã€HFL(éšæœºBçŸ©é˜µ)ã€HFL(èšç±»BçŸ©é˜µ)

    æ³¨æ„ï¼šä¸ºäº†å…¼å®¹å¤šè¿›ç¨‹ï¼Œæˆ‘ä»¬ä¸ç›´æ¥ä¼ é€’å¤§å‹æ¨¡å‹å¯¹è±¡ï¼Œ
    è€Œæ˜¯ä¼ é€’æ¨¡å‹æƒé‡(state_dict)å’Œæ¨¡å‹æ¶æ„ä¿¡æ¯(args)ï¼Œåœ¨å­è¿›ç¨‹ä¸­é‡æ–°æ„å»ºæ¨¡å‹ã€‚
    
    Args:
        train_hfl_random: æ˜¯å¦è®­ç»ƒHFLéšæœºBçŸ©é˜µæ¨¡å‹
        train_hfl_cluster: æ˜¯å¦è®­ç»ƒHFLèšç±»BçŸ©é˜µæ¨¡å‹
        train_sfl: æ˜¯å¦è®­ç»ƒSFLæ¨¡å‹
    """
    # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°æ„å»ºæ¨¡å‹
    local_net_hfl_random = build_model(args, dataset_train)
    local_net_hfl_cluster = build_model(args, dataset_train)
    local_net_sfl = build_model(args, dataset_train)
    
    # è·å–å½“å‰å®¢æˆ·ç«¯çš„ç±»åˆ«ä¿¡æ¯
    user_classes = client_classes.get(user_idx, None) if client_classes else None
    
    # --- è®­ç»ƒHFLæ¨¡å‹ (ä½¿ç”¨éšæœºBçŸ©é˜µ) - ä»…åœ¨æœªæ”¶æ•›æ—¶è®­ç»ƒ ---
    if train_hfl_random:
        local_random = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[user_idx], user_classes=user_classes)
        local_net_hfl_random.load_state_dict(w_input_hfl_random)
        w_hfl_random, loss_hfl_random = local_random.train(net=local_net_hfl_random.to(args.device))
    else:
        # å¦‚æœå·²æ”¶æ•›ï¼Œç›´æ¥è¿”å›è¾“å…¥æƒé‡å’Œé›¶æŸå¤±
        w_hfl_random, loss_hfl_random = copy.deepcopy(w_input_hfl_random), 0.0
    
    # --- è®­ç»ƒHFLæ¨¡å‹ (ä½¿ç”¨èšç±»BçŸ©é˜µ) - ä»…åœ¨æœªæ”¶æ•›æ—¶è®­ç»ƒ ---
    if train_hfl_cluster:
        local_cluster = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[user_idx], user_classes=user_classes)
        local_net_hfl_cluster.load_state_dict(w_input_hfl_cluster)
        w_hfl_cluster, loss_hfl_cluster = local_cluster.train(net=local_net_hfl_cluster.to(args.device))
    else:
        # å¦‚æœå·²æ”¶æ•›ï¼Œç›´æ¥è¿”å›è¾“å…¥æƒé‡å’Œé›¶æŸå¤±
        w_hfl_cluster, loss_hfl_cluster = copy.deepcopy(w_input_hfl_cluster), 0.0
    
    # --- è®­ç»ƒå•å±‚æ¨¡å‹ (SFL) - ä»…åœ¨æœªæ”¶æ•›æ—¶è®­ç»ƒ ---
    if train_sfl:
        local_sfl = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[user_idx], user_classes=user_classes)
        local_net_sfl.load_state_dict(w_sfl_global)
        w_sfl, loss_sfl = local_sfl.train(net=local_net_sfl.to(args.device))
    else:
        # å¦‚æœå·²æ”¶æ•›ï¼Œç›´æ¥è¿”å›è¾“å…¥æƒé‡å’Œé›¶æŸå¤±
        w_sfl, loss_sfl = copy.deepcopy(w_sfl_global), 0.0

    # è¿”å›ç»“æœï¼ŒåŒ…æ‹¬ user_idx ä»¥ä¾¿åç»­æ’åº
    return (user_idx, 
            copy.deepcopy(w_hfl_random), loss_hfl_random,
            copy.deepcopy(w_hfl_cluster), loss_hfl_cluster, 
            copy.deepcopy(w_sfl), loss_sfl)

def summarize_results(net_glob_hfl_bipartite, net_glob_hfl_random, net_glob_sfl, dataset_train, dataset_test, args,
                     total_comm_overhead_bipartite_upload, total_comm_overhead_bipartite_download,
                     total_comm_overhead_random_upload, total_comm_overhead_random_download,
                     total_comm_overhead_sfl_upload, total_comm_overhead_sfl_download):
    print("=== Final Communication Overhead Summary ===")
    print(f"SFL Total Overhead: {total_comm_overhead_sfl_upload + total_comm_overhead_sfl_download:.6f}s "
          f"(Upload: {total_comm_overhead_sfl_upload:.6f}s, Download: {total_comm_overhead_sfl_download:.6f}s)")
    print(f"HFL Bipartite Total Overhead: {total_comm_overhead_bipartite_upload + total_comm_overhead_bipartite_download:.6f}s "
          f"(Upload: {total_comm_overhead_bipartite_upload:.6f}s, Download: {total_comm_overhead_bipartite_download:.6f}s)")
    print(f"HFL Random Total Overhead: {total_comm_overhead_random_upload + total_comm_overhead_random_download:.6f}s "
          f"(Upload: {total_comm_overhead_random_upload:.6f}s, Download: {total_comm_overhead_random_download:.6f}s)")

def save_results_to_csv(results, filename):
    """Save results to CSV file for three models, including EH-level testing results"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['epoch', 'eh_round', 'es_round', 'train_loss', 'test_loss', 'test_acc', 
                     'model_type', 'level', 'eh_idx']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for result in results:
            writer.writerow(result)

if __name__ == '__main__':
    mp.set_start_method('spawn', force=True)
    # parse args
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

    # å…ˆè¿è¡Œå¸¦å®½åˆ†é…ç®—æ³•è·å–å®é™…çš„å®¢æˆ·ç«¯æ•°é‡
    print("æ­£åœ¨åˆ†æç½‘ç»œæ‹“æ‰‘å¹¶ç¡®å®šå®¢æˆ·ç«¯æ•°é‡...")
    bipartite_graph, client_nodes, active_es_nodes, A_design, r_client_to_es, r_es, r_es_to_cloud, r_client_to_cloud = run_bandwidth_allocation(
        graphml_file=args.graphml_file, 
        es_ratio=args.es_ratio, 
        max_capacity=args.max_capacity, 
        visualize=True)
    
    if bipartite_graph is None:
        print("Failed to build bipartite graph, exiting.")
        exit(1)
    
    # æ ¹æ®å®é™…å®¢æˆ·ç«¯æ•°é‡æ›´æ–°args.num_users
    actual_num_users = len(client_nodes)
    print(f"ç½‘ç»œæ‹“æ‰‘åˆ†æå®Œæˆï¼šå®é™…å®¢æˆ·ç«¯æ•°é‡ä¸º {actual_num_users}")
    print(f"åŸå§‹å‚æ•°è®¾ç½®ï¼šargs.num_users = {args.num_users}")
    
    # æ›´æ–°å‚æ•°ä»¥åŒ¹é…å®é™…å®¢æˆ·ç«¯æ•°é‡
    args.num_users = actual_num_users
    print(f"å·²æ›´æ–°å‚æ•°ï¼šargs.num_users = {args.num_users}")

    # ç°åœ¨ä½¿ç”¨æ›´æ–°åçš„å‚æ•°ç”Ÿæˆæ•°æ®åˆ†é…
    dataset_train, dataset_test, dict_users, client_classes = get_data(args)

    # æ‰“å° FedRS é…ç½®ä¿¡æ¯
    if args.method == 'fedrs':
        print("\n" + "="*50)
        print("FedRS ç®—æ³•é…ç½®ä¿¡æ¯")
        print("="*50)
        print(f"è”é‚¦å­¦ä¹ æ–¹æ³•: {args.method}")
        print(f"FedRS Alpha å‚æ•°: {args.fedrs_alpha}")
        print(f"æœ€å°æœ¬åœ°è®­ç»ƒè½®æ¬¡: {args.min_le}")
        print(f"æœ€å¤§æœ¬åœ°è®­ç»ƒè½®æ¬¡: {args.max_le}")
        
        # ç»Ÿè®¡å®¢æˆ·ç«¯ç±»åˆ«åˆ†å¸ƒ
        class_counts = {}
        for client_id, classes in client_classes.items():
            num_classes = len(classes)
            class_counts[num_classes] = class_counts.get(num_classes, 0) + 1
        
        print("\nå®¢æˆ·ç«¯ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
        for num_classes, count in sorted(class_counts.items()):
            print(f"  æ‹¥æœ‰ {num_classes} ä¸ªç±»åˆ«çš„å®¢æˆ·ç«¯æ•°é‡: {count}")
        print("="*50 + "\n")
    else:
        print(f"\nä½¿ç”¨è”é‚¦å­¦ä¹ æ–¹æ³•: {args.method}\n")

    net_glob = build_model(args, dataset_train)
    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    print(f"\n=== æ•°æ®ä¸€è‡´æ€§éªŒè¯ ===")
    print(f"å®¢æˆ·ç«¯èŠ‚ç‚¹æ€»æ•°: {len(client_nodes)}")
    print(f"æ´»è·ƒè¾¹ç¼˜æœåŠ¡å™¨èŠ‚ç‚¹æ€»æ•°: {len(active_es_nodes)}")
    print(f"args.num_users: {args.num_users}")
    print(f"dict_usersé”®çš„æ•°é‡: {len(dict_users)}")
    print(f"dict_usersé”®çš„èŒƒå›´: {min(dict_users.keys())} - {max(dict_users.keys())}")
    print("=" * 25)

    net_glob.train()

    # åˆå§‹åŒ–å…¨å±€æƒé‡
    w_glob = net_glob.state_dict()
    num_users = len(client_nodes)
    num_ESs = len(active_es_nodes)
    k2 = args.ES_k2
    k3 = args.EH_k3
    num_processes = args.num_processes

    A_random = get_A_random(num_users, num_ESs)

    # ä½¿ç”¨è°±èšç±»ç”ŸæˆBçŸ©é˜µï¼ˆæ›¿æ¢åŸæ¥çš„éšæœºBçŸ©é˜µï¼‰
    print("å¼€å§‹åˆå§‹è®­ç»ƒå’Œè°±èšç±»...")

    # 1. è®­ç»ƒåˆå§‹æœ¬åœ°æ¨¡å‹
    w_locals, client_label_distributions = train_initial_models(
        args, dataset_train, dict_users, net_glob, num_users
    )

    # 2. ä½¿ç”¨è°±èšç±»ç”ŸæˆBçŸ©é˜µ
    B_cluster = get_B_cluster(
        args, w_locals, A_design, dict_users, net_glob, client_label_distributions
    )
    num_EHs = B_cluster.shape[1]
    
    # 3. åŒæ—¶ç”ŸæˆéšæœºBçŸ©é˜µç”¨äºå¯¹æ¯”
    B_random = get_B(num_ESs, num_EHs)

    # æ„å»ºä¸¤å¥—å±‚çº§ç»“æ„ï¼ˆç”¨äºè”é‚¦å­¦ä¹ èšåˆï¼‰
    C1_random, C2_random = build_hierarchy(A_random, B_random)
    C1_cluster, C2_cluster = build_hierarchy(A_design, B_cluster)

    # æ„å»ºé€šä¿¡å®é™…çš„å…³è”çŸ©é˜µï¼ˆç”¨äºé€šä¿¡å¼€é”€è®¡ç®—ï¼‰
    # ä¸¤ç§Aå…³è”çŸ©é˜µç›´æ¥å¯ç”¨ï¼ŒBå…³è”çŸ©é˜µä¸ºï¼ˆesï¼Œç°‡ï¼‰å½¢å¼ï¼Œéœ€è½¬åŒ–ä¸ºes-esï¼Œè¿˜éœ€ç”Ÿæˆes-cloud
    model_size = get_model_size_in_bits(w_glob)
    B_random_comm, C_random_comm = select_eh(B_random, r_es, r_es_to_cloud, model_size)
    B_cluster_comm, C_cluster_comm = select_eh(B_cluster, r_es, r_es_to_cloud, model_size)
    print("C1_random (ä¸€çº§->å®¢æˆ·ç«¯):", C1_random)
    print("C2_random (äºŒçº§->ä¸€çº§):", C2_random)
    print("C1_cluster (ä¸€çº§->å®¢æˆ·ç«¯):", C1_cluster)
    print("C2_cluster (äºŒçº§->ä¸€çº§):", C2_cluster)
    t_client_to_es_random = calculate_transmission_time(model_size, r_client_to_es, A_random)
    t_client_to_es_design = calculate_transmission_time(model_size, r_client_to_es, A_design)
    t_es_to_eh_random = calculate_transmission_time(model_size, r_es, B_random_comm)
    t_es_to_eh_design = calculate_transmission_time(model_size, r_es, B_cluster_comm)
    t_eh_to_cloud_random = calculate_transmission_time(model_size, r_es_to_cloud, C_random_comm)
    t_eh_to_cloud_design = calculate_transmission_time(model_size, r_es_to_cloud, C_cluster_comm)
    t_client_to_cloud_sfl = calculate_transmission_time(model_size, r_client_to_cloud, np.ones((num_users, 1), dtype=int))
    print(f"random:{t_client_to_es_random}, {t_es_to_eh_random}, {t_eh_to_cloud_random}")
    print(f"design:{t_client_to_es_design}, {t_es_to_eh_design}, {t_eh_to_cloud_design}")
    print(f"sfl:{t_client_to_cloud_sfl}")
    t_hfl_random_sig = t_client_to_es_random * k2 + t_es_to_eh_random * k3 + t_eh_to_cloud_random
    t_hfl_design_sig = t_client_to_es_design * k2 + t_es_to_eh_design * k3 + t_eh_to_cloud_design
    t_sfl_sig = t_client_to_cloud_sfl * k2 * k3  # SFL ç›´æ¥é€šä¿¡åˆ°äº‘ç«¯ï¼Œä¹˜ä»¥ k2*k3 æ¬¡
    print(f"hfl_random é¢„è®¡å•è½®é€šä¿¡æ—¶é—´: {t_hfl_random_sig:.6f}s")
    print(f"hfl_design é¢„è®¡å•è½®é€šä¿¡æ—¶é—´: {t_hfl_design_sig:.6f}s")
    print(f"sfl é¢„è®¡å•è½®é€šä¿¡æ—¶é—´: {t_sfl_sig:.6f}s")
    # ç”ŸæˆEHä¸“å±æµ‹è¯•é›†
    print("\n--- ç”ŸæˆEHä¸“å±æµ‹è¯•é›† ---")
    print("é‡‡ç”¨æ”¹è¿›çš„èµ„æºåˆ†é…ç­–ç•¥ï¼šå…è®¸æµ‹è¯•æ ·æœ¬åœ¨å¤šä¸ªEHæµ‹è¯•é›†ä¸­é‡å¤å‡ºç°")
    print("è¿™ç¡®ä¿æ¯ä¸ªEHéƒ½èƒ½è·å¾—ä¸å…¶ä¸‹æ¸¸å®¢æˆ·ç«¯åˆ†å¸ƒåŒ¹é…çš„ä¸ªæ€§åŒ–æµ‹è¯•é›†")
    
    # ä¸ºéšæœºBçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†
    print("\nğŸ² ä¸ºéšæœºBçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†...")
    eh_testsets_random, eh_label_distributions_random = EHTestsetGenerator.create_eh_testsets(
        dataset_test, A_random, B_random, C1_random, C2_random, dataset_train, dict_users, visualize=True
    )
    
    # ä¸ºèšç±»BçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†
    print("\nğŸ§© ä¸ºèšç±»BçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†...")
    eh_testsets_cluster, eh_label_distributions_cluster = EHTestsetGenerator.create_eh_testsets(
        dataset_test, A_design, B_cluster, C1_cluster, C2_cluster, dataset_train, dict_users, visualize=True
    )
    
    print(f"\nâœ… æµ‹è¯•é›†ç”Ÿæˆå®Œæˆ!")
    print(f"éšæœºBçŸ©é˜µ: å·²ç”Ÿæˆ {len(eh_testsets_random)} ä¸ªEHä¸“å±æµ‹è¯•é›†")
    print(f"èšç±»BçŸ©é˜µ: å·²ç”Ÿæˆ {len(eh_testsets_cluster)} ä¸ªEHä¸“å±æµ‹è¯•é›†")
    
    # æ‰“å°æ¯ä¸ªEHæµ‹è¯•é›†çš„è¯¦ç»†ä¿¡æ¯
    print(f"\nğŸ“Š éšæœºBçŸ©é˜µ - EHæµ‹è¯•é›†ç»Ÿè®¡:")
    for eh_idx, testset in eh_testsets_random.items():
        unique_samples = len(np.unique(testset))
        total_samples = len(testset)
        print(f"  EH {eh_idx}: æ€»æ ·æœ¬={total_samples}, å”¯ä¸€æ ·æœ¬={unique_samples}, é‡å¤ç‡={1-unique_samples/total_samples:.1%}")
    
    print(f"\nğŸ“Š èšç±»BçŸ©é˜µ - EHæµ‹è¯•é›†ç»Ÿè®¡:")
    for eh_idx, testset in eh_testsets_cluster.items():
        unique_samples = len(np.unique(testset))
        total_samples = len(testset)
        print(f"  EH {eh_idx}: æ€»æ ·æœ¬={total_samples}, å”¯ä¸€æ ·æœ¬={unique_samples}, é‡å¤ç‡={1-unique_samples/total_samples:.1%}")

    # æ‰“å°FedRSé…ç½®ä¿¡æ¯
    print(f"\n--- FedRS Configuration ---")
    print(f"Method: {args.method}")
    if args.method == 'fedrs':
        print(f"FedRS Alpha: {args.fedrs_alpha}")
        print(f"Min Local Epochs: {args.min_le}")
        print(f"Max Local Epochs: {args.max_le}")
        print("FedRS Restricted Softmax: Enabled")
        # æ‰“å°å‰å‡ ä¸ªå®¢æˆ·ç«¯çš„ç±»åˆ«ä¿¡æ¯
        print("Sample Client Class Distributions:")
        for i in range(min(5, len(client_classes))):
            print(f"  Client {i}: Classes {client_classes[i]}")
    else:
        print("FedRS: Disabled (using standard FedAvg)")
    print("-----------------------------\n")

    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    if not os.path.exists('./results'):
        os.makedirs('./results')

    # ç”Ÿæˆå”¯ä¸€çš„æ—¶é—´æˆ³ç”¨äºæ–‡ä»¶åï¼ŒåŒ…å«é‡è¦å‚æ•°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    param_str = f"e{args.epochs}_u{args.num_users}_le{args.local_ep}_{args.dataset}_{args.model}_k2{args.ES_k2}_k3{args.EH_k3}_p{args.num_processes}"
    if not args.iid:
        param_str += f"_beta{args.beta}"
    csv_filename = f'./results/training_results_{param_str}_{timestamp}.csv'

    # åˆå§‹åŒ–ç»“æœè®°å½•åˆ—è¡¨ - æ–°æ ¼å¼æ”¯æŒä¸‰ç§æ¨¡å‹
    results_history = []

    # è®­ç»ƒå‰æµ‹è¯•åˆå§‹æ¨¡å‹
    print("\n--- Testing Initial Global Models ---")
    net_glob.eval()

    # æµ‹è¯•åˆå§‹æ¨¡å‹
    acc_init, loss_init = test_img(net_glob, dataset_test, args)
    print(f"Initial Model - Testing accuracy: {acc_init:.2f}%, Loss: {loss_init:.4f}")

    # è®°å½•åˆå§‹ç»“æœ - ä¸‰ç§æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„åˆå§‹æƒé‡
    for model_name in ['HFL_Random_B', 'HFL_Cluster_B', 'SFL']:
        results_history.append({
            'epoch': -1,
            'eh_round': 0,
            'es_round': 0,
            'train_loss': 0.0,  # åˆå§‹è®­ç»ƒæŸå¤±æš‚æ—¶è®¾ä¸º0
            'test_loss': loss_init,
            'test_acc': acc_init,
            'model_type': model_name,
            'level': 'Global',
            'eh_idx': -1
        })

    # ä¿å­˜åˆå§‹ç»“æœåˆ°CSV
    save_results_to_csv(results_history, csv_filename)

    # training
    # --- åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡å‹ï¼Œç¡®ä¿åˆå§‹æƒé‡ç›¸åŒ ---
    # net_glob_hfl_random æ˜¯ä½¿ç”¨éšæœºBçŸ©é˜µçš„ HFL å…¨å±€æ¨¡å‹
    net_glob_hfl_random = copy.deepcopy(net_glob)
    w_glob_hfl_random = net_glob_hfl_random.state_dict()

    # net_glob_hfl_cluster æ˜¯ä½¿ç”¨èšç±»BçŸ©é˜µçš„ HFL å…¨å±€æ¨¡å‹
    net_glob_hfl_cluster = copy.deepcopy(net_glob)
    w_glob_hfl_cluster = net_glob_hfl_cluster.state_dict()

    # net_glob_sfl æ˜¯ SFL çš„å…¨å±€æ¨¡å‹
    net_glob_sfl = copy.deepcopy(net_glob)
    w_glob_sfl = net_glob_sfl.state_dict()

    # --- åˆ†åˆ«è®°å½•ä¸‰ç§æ¨¡å‹çš„æŒ‡æ ‡ ---
    loss_train_hfl_random = []
    loss_train_hfl_cluster = []
    loss_train_sfl = []
    loss_test_hfl_random = []
    loss_test_hfl_cluster = []
    loss_test_sfl = []
    acc_test_hfl_random = []
    acc_test_hfl_cluster = []
    acc_test_sfl = []
    t_hfl_random = 0
    t_hfl_design = 0
    t_sfl = 0

    # è®°å½•å®é™…è¿è¡Œçš„epochæ•°
    final_epoch = args.epochs

    # --- åˆå§‹åŒ–æ”¶æ•›æ£€æŸ¥å™¨ ---
    print("\n=== åˆå§‹åŒ–æ”¶æ•›æ£€æŸ¥å™¨ ===")
    # ä¸ºæ¯ä¸ªEHåˆ›å»ºæ”¶æ•›æ£€æŸ¥å™¨ - HFLéšæœºBçŸ©é˜µ
    eh_checkers_random = {}
    for eh_idx in range(num_EHs):
        eh_checkers_random[eh_idx] = ConvergenceChecker(patience = args.EH_k3)
    
    # ä¸ºæ¯ä¸ªEHåˆ›å»ºæ”¶æ•›æ£€æŸ¥å™¨ - HFLèšç±»BçŸ©é˜µ  
    eh_checkers_cluster = {}
    for eh_idx in range(num_EHs):
        eh_checkers_cluster[eh_idx] = ConvergenceChecker(patience = args.EH_k3)
    
    # ä¸ºSFLåˆ›å»ºæ”¶æ•›æ£€æŸ¥å™¨
    sfl_checker = ConvergenceChecker(patience = args.EH_k3)

    # è®°å½•å„æœºåˆ¶çš„æ”¶æ•›çŠ¶æ€
    converged_hfl_random = False
    converged_hfl_cluster = False
    converged_sfl = False
    
    print(f"å·²ä¸ºHFLéšæœºBçŸ©é˜µåˆ›å»º {len(eh_checkers_random)} ä¸ªEHæ”¶æ•›æ£€æŸ¥å™¨")
    print(f"å·²ä¸ºHFLèšç±»BçŸ©é˜µåˆ›å»º {len(eh_checkers_cluster)} ä¸ªEHæ”¶æ•›æ£€æŸ¥å™¨")
    print(f"å·²ä¸ºSFLåˆ›å»ºå…¨å±€æ”¶æ•›æ£€æŸ¥å™¨")
    print("=" * 30)

    for epoch in range(args.epochs):
        # HFL éšæœºBçŸ©é˜µæ¨¡å‹æƒé‡åˆ†å‘ (Cloud -> EH)
        EHs_ws_hfl_random = [copy.deepcopy(w_glob_hfl_random) for _ in range(num_EHs)]
        
        # HFL èšç±»BçŸ©é˜µæ¨¡å‹æƒé‡åˆ†å‘ (Cloud -> EH)
        EHs_ws_hfl_cluster = [copy.deepcopy(w_glob_hfl_cluster) for _ in range(num_EHs)]

        # EH å±‚èšåˆ k3 è½®
        for t3 in range(k3):
            # HFLéšæœº: EH å±‚ -> ES å±‚
            ESs_ws_input_hfl_random = [None] * num_ESs
            for EH_idx, ES_indices in C2_random.items():
                for ES_idx in ES_indices:
                    ESs_ws_input_hfl_random[ES_idx] = copy.deepcopy(EHs_ws_hfl_random[EH_idx])
            
            # HFLèšç±»: EH å±‚ -> ES å±‚
            ESs_ws_input_hfl_cluster = [None] * num_ESs
            for EH_idx, ES_indices in C2_cluster.items():
                for ES_idx in ES_indices:
                    ESs_ws_input_hfl_cluster[ES_idx] = copy.deepcopy(EHs_ws_hfl_cluster[EH_idx])

            # ES å±‚èšåˆ k2 è½®
            for t2 in range(k2):
                # --- æœ¬åœ°è®­ç»ƒ (ä¸‰ç§æ¨¡å‹å¹¶è¡Œ) ---
                # HFLéšæœº: ES å±‚ -> Client å±‚
                w_locals_input_hfl_random = [None] * num_users
                for ES_idx, user_indices in C1_random.items():
                    for user_idx in user_indices:
                        w_locals_input_hfl_random[user_idx] = copy.deepcopy(ESs_ws_input_hfl_random[ES_idx])
                
                # HFLèšç±»: ES å±‚ -> Client å±‚
                w_locals_input_hfl_cluster = [None] * num_users
                for ES_idx, user_indices in C1_cluster.items():
                    for user_idx in user_indices:
                        w_locals_input_hfl_cluster[user_idx] = copy.deepcopy(ESs_ws_input_hfl_cluster[ES_idx])

                # ç”¨äºå­˜å‚¨ä¸‰ç§æ¨¡å‹æœ¬åœ°è®­ç»ƒçš„è¾“å‡º
                w_locals_output_hfl_random = [None] * num_users
                w_locals_output_hfl_cluster = [None] * num_users
                w_locals_output_sfl = [None] * num_users
                loss_locals_hfl_random = []
                loss_locals_hfl_cluster = []
                loss_locals_sfl = []

                # æ˜¾ç¤ºè®­ç»ƒçŠ¶æ€ä¿¡æ¯
                active_models = []
                if not converged_hfl_random:
                    active_models.append("HFL_Random")
                if not converged_hfl_cluster:
                    active_models.append("HFL_Cluster")
                if not converged_sfl:
                    active_models.append("SFL")
                
                if not active_models:
                    print(f"\n[Skip Training] æ‰€æœ‰æ¨¡å‹å·²æ”¶æ•›ï¼Œè·³è¿‡å®¢æˆ·ç«¯è®­ç»ƒ")
                    # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½æ”¶æ•›äº†ï¼Œè·³è¿‡è®­ç»ƒä½†ä»éœ€è¦è¿”å›ç»“æœ
                    results = []
                    for user_idx in range(num_users):
                        results.append((user_idx, 
                                      w_locals_input_hfl_random[user_idx], 0.0,
                                      w_locals_input_hfl_cluster[user_idx], 0.0,
                                      w_glob_sfl, 0.0))
                else:
                    print(f"\n[Parallel Training] ä¸º {len(active_models)} ç§æ´»è·ƒæ¨¡å‹è®­ç»ƒ {args.num_users} ä¸ªå®¢æˆ·ç«¯")
                    print(f"æ´»è·ƒæ¨¡å‹: {', '.join(active_models)}")
                    print(f"ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ...")

                    # å‡†å¤‡ä¼ é€’ç»™æ¯ä¸ªå­è¿›ç¨‹çš„å‚æ•°
                    tasks = []
                    for user_idx in range(num_users):
                        task_args = (
                            args, user_idx, dataset_train, dict_users,
                            w_locals_input_hfl_random[user_idx], w_locals_input_hfl_cluster[user_idx], 
                            w_glob_sfl, client_classes,
                            not converged_hfl_random,  # train_hfl_random
                            not converged_hfl_cluster,  # train_hfl_cluster  
                            not converged_sfl  # train_sfl
                        )
                        tasks.append(task_args)
                    print("æˆåŠŸåˆ›å»ºå¤šçº¿ç¨‹ï¼")

                    # åˆ›å»ºè¿›ç¨‹æ± å¹¶åˆ†å‘ä»»åŠ¡
                    # ä½¿ç”¨ with è¯­å¥å¯ä»¥è‡ªåŠ¨ç®¡ç†è¿›ç¨‹æ± çš„å…³é—­
                    with mp.Pool(processes=num_processes) as pool:
                        results = pool.starmap(train_client, tqdm(tasks, desc=f"Epoch {epoch}|{t3 + 1}|{t2 + 1} Training Clients"))

                print("è®­ç»ƒç»“æŸ")
                # æ”¶é›†å¹¶æ•´ç†æ‰€æœ‰å®¢æˆ·ç«¯çš„è®­ç»ƒç»“æœ
                for result in results:
                    u_idx, w_hr, l_hr, w_hc, l_hc, w_s, l_s = result
                    w_locals_output_hfl_random[u_idx] = w_hr
                    loss_locals_hfl_random.append(l_hr)
                    w_locals_output_hfl_cluster[u_idx] = w_hc
                    loss_locals_hfl_cluster.append(l_hc)
                    w_locals_output_sfl[u_idx] = w_s
                    loss_locals_sfl.append(l_s)
                
                print("æ’åºç»“æŸ")
                if active_models:
                    print(f"[Parallel Training] æ‰€æœ‰ {args.num_users} ä¸ªå®¢æˆ·ç«¯å·²å®Œæˆ {len(active_models)} ç§æ¨¡å‹çš„è®­ç»ƒ")
                    print(f"è®­ç»ƒçš„æ¨¡å‹: {', '.join(active_models)}")
                else:
                    print(f"[Skip Training] æ‰€æœ‰æ¨¡å‹å·²æ”¶æ•›ï¼Œæœªè¿›è¡Œå®é™…è®­ç»ƒ")
                print(f'\nEpoch {epoch} | EH_R {t3 + 1}/{k3} | ES_R {t2 + 1}/{k2} | å¼€å§‹èšåˆ')

                # --- HFL èšåˆ (Client -> ES) - åªå¯¹æœªæ”¶æ•›çš„æœºåˆ¶è¿›è¡Œèšåˆ ---
                if not converged_hfl_random:
                    ESs_ws_input_hfl_random = FedAvg_layered(w_locals_output_hfl_random, C1_random)
                    t_hfl_random += t_client_to_es_random
                else:
                    print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡ESå±‚èšåˆ")
                
                if not converged_hfl_cluster:
                    ESs_ws_input_hfl_cluster = FedAvg_layered(w_locals_output_hfl_cluster, C1_cluster)
                    t_hfl_design += t_client_to_es_design
                else:
                    print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡ESå±‚èšåˆ")

                # --- è®°å½•æŸå¤± ---
                # åªä¸ºå®é™…è®­ç»ƒçš„æ¨¡å‹è®¡ç®—å¹³å‡æŸå¤±ï¼Œå·²æ”¶æ•›çš„æ¨¡å‹æŸå¤±ä¸º0
                if not converged_hfl_random:
                    loss_avg_hfl_random = sum(loss_locals_hfl_random) / len(loss_locals_hfl_random) if loss_locals_hfl_random else 0.0
                else:
                    loss_avg_hfl_random = 0.0  # å·²æ”¶æ•›ï¼ŒæŸå¤±ä¸º0
                    
                if not converged_hfl_cluster:
                    loss_avg_hfl_cluster = sum(loss_locals_hfl_cluster) / len(loss_locals_hfl_cluster) if loss_locals_hfl_cluster else 0.0
                else:
                    loss_avg_hfl_cluster = 0.0  # å·²æ”¶æ•›ï¼ŒæŸå¤±ä¸º0
                    
                if not converged_sfl:
                    loss_avg_sfl = sum(loss_locals_sfl) / len(loss_locals_sfl) if loss_locals_sfl else 0.0
                else:
                    loss_avg_sfl = 0.0  # å·²æ”¶æ•›ï¼ŒæŸå¤±ä¸º0
                
                loss_train_hfl_random.append(loss_avg_hfl_random)
                loss_train_hfl_cluster.append(loss_avg_hfl_cluster)
                loss_train_sfl.append(loss_avg_sfl)

                # æ˜¾ç¤ºæŸå¤±ä¿¡æ¯ï¼ŒåŒºåˆ†è®­ç»ƒå’Œæ”¶æ•›çŠ¶æ€
                print(f'\nEpoch {epoch} | EH_R {t3 + 1}/{k3} | ES_R {t2 + 1}/{k2}')
                loss_info = []
                if not converged_hfl_random:
                    loss_info.append(f'HFL_Random Loss: {loss_avg_hfl_random:.4f}')
                else:
                    loss_info.append(f'HFL_Random: å·²æ”¶æ•› âœ…')
                    
                if not converged_hfl_cluster:
                    loss_info.append(f'HFL_Cluster Loss: {loss_avg_hfl_cluster:.4f}')
                else:
                    loss_info.append(f'HFL_Cluster: å·²æ”¶æ•› âœ…')
                    
                if not converged_sfl:
                    loss_info.append(f'SFL Loss: {loss_avg_sfl:.4f}')
                else:
                    loss_info.append(f'SFL: å·²æ”¶æ•› âœ…')
                    
                print(' | '.join(loss_info))

            # HFL èšåˆ (ES -> EH) - åªå¯¹æœªæ”¶æ•›çš„æœºåˆ¶è¿›è¡Œèšåˆ
            if not converged_hfl_random:
                EHs_ws_hfl_random = FedAvg_layered(ESs_ws_input_hfl_random, C2_random)
                t_hfl_random += t_es_to_eh_random
            else:
                print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHå±‚èšåˆ")
            
            if not converged_hfl_cluster:
                EHs_ws_hfl_cluster = FedAvg_layered(ESs_ws_input_hfl_cluster, C2_cluster)
                t_hfl_design += t_es_to_eh_design
            else:
                print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHå±‚èšåˆ")
            
            # --- SFL å…¨å±€èšåˆ (Client -> Cloud) - åœ¨EHå±‚èšåˆæ—¶æœºæ‰§è¡Œ ---
            if not converged_sfl:
                w_glob_sfl = FedAvg(w_locals_output_sfl)
                net_glob_sfl.load_state_dict(w_glob_sfl)
                t_sfl += t_client_to_cloud_sfl
            else:
                print(f"  [Skip] SFLå·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€èšåˆ")
            
            # --- åœ¨æ¯æ¬¡EHèšåˆåæµ‹è¯•EHæ¨¡å‹åœ¨ä¸“å±æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ ---
            print(f"\n[EH Testing] Epoch {epoch} | EH_Round {t3+1}/{k3} - æµ‹è¯•EHæ¨¡å‹æ€§èƒ½...")
            
            # åˆ›å»ºä¸´æ—¶æ¨¡å‹å¯¹è±¡æ¥åŠ è½½EHæƒé‡å¹¶è¿›è¡Œæµ‹è¯•
            eh_results_random = []
            eh_results_cluster = []
            
            # æµ‹è¯•æ¯ä¸ªEHçš„æ¨¡å‹æ€§èƒ½ï¼ˆåœ¨éšæœºBçŸ©é˜µæƒ…å†µä¸‹ï¼‰- åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶
            if not converged_hfl_random:
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_random):
                    if eh_weights is not None and eh_idx in eh_testsets_random:
                        # åˆ›å»ºä¸´æ—¶æ¨¡å‹å¹¶åŠ è½½æƒé‡
                        temp_model = build_model(args, dataset_train)
                        temp_model.load_state_dict(eh_weights)
                        temp_model.eval()
                        
                        # åœ¨EHä¸“å±æµ‹è¯•é›†ä¸Šæµ‹è¯•æ¨¡å‹
                        eh_acc, eh_loss = test_eh_model(temp_model, dataset_test, eh_testsets_random[eh_idx], args)
                        
                        # è®°å½•ç»“æœ
                        eh_results_random.append({
                            'epoch': epoch,
                            'eh_round': t3 + 1,
                            'es_round': k2,  # ESè½®æ¬¡å·²ç»“æŸ
                            'train_loss': 0.0,  # EHçº§åˆ«æ²¡æœ‰è®­ç»ƒæŸå¤±
                            'test_loss': eh_loss,
                            'test_acc': eh_acc,
                            'model_type': 'HFL_Random_B',
                            'level': 'EH',
                            'eh_idx': eh_idx
                        })
                        
                        print(f"  [Random] EH {eh_idx}: Acc {eh_acc:.2f}%, Loss {eh_loss:.4f}")
            else:
                print("  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHæ¨¡å‹æµ‹è¯•")
            
            # æµ‹è¯•æ¯ä¸ªEHçš„æ¨¡å‹æ€§èƒ½ï¼ˆåœ¨èšç±»BçŸ©é˜µæƒ…å†µä¸‹ï¼‰- åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶
            if not converged_hfl_cluster:
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_cluster):
                    if eh_weights is not None and eh_idx in eh_testsets_cluster:
                        # åˆ›å»ºä¸´æ—¶æ¨¡å‹å¹¶åŠ è½½æƒé‡
                        temp_model = build_model(args, dataset_train)
                        temp_model.load_state_dict(eh_weights)
                        temp_model.eval()
                        
                        # åœ¨EHä¸“å±æµ‹è¯•é›†ä¸Šæµ‹è¯•æ¨¡å‹
                        eh_acc, eh_loss = test_eh_model(temp_model, dataset_test, eh_testsets_cluster[eh_idx], args)
                        
                        # è®°å½•ç»“æœ
                        eh_results_cluster.append({
                            'epoch': epoch,
                            'eh_round': t3 + 1,
                            'es_round': k2,  # ESè½®æ¬¡å·²ç»“æŸ
                            'train_loss': 0.0,  # EHçº§åˆ«æ²¡æœ‰è®­ç»ƒæŸå¤±
                            'test_loss': eh_loss,
                            'test_acc': eh_acc,
                            'model_type': 'HFL_Cluster_B',
                            'level': 'EH',
                            'eh_idx': eh_idx
                        })
                        
                        print(f"  [Cluster] EH {eh_idx}: Acc {eh_acc:.2f}%, Loss {eh_loss:.4f}")
            else:
                print("  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHæ¨¡å‹æµ‹è¯•")
            
            # æµ‹è¯•SFLå…¨å±€æ¨¡å‹ï¼ˆåœ¨å…¨å±€æµ‹è¯•é›†ä¸Šï¼‰- åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶
            if not converged_sfl:
                net_glob_sfl.eval()
                acc_sfl, loss_sfl = test_img(net_glob_sfl, dataset_test, args)
            else:
                print("  [Skip] SFLå·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€æ¨¡å‹æµ‹è¯•")
                # ä½¿ç”¨ä¸Šä¸€è½®çš„ç»“æœä½œä¸ºå ä½ç¬¦
                acc_sfl, loss_sfl = acc_test_sfl[-1] if acc_test_sfl else 0.0, loss_test_sfl[-1] if loss_test_sfl else 0.0
            
            # è®°å½•SFLæ¨¡å‹ç»“æœ
            sfl_result = {
                'epoch': epoch,
                'eh_round': t3 + 1,
                'es_round': k2,
                'train_loss': 0.0,  # ä½¿ç”¨0.0ä½œä¸ºå ä½ç¬¦
                'test_loss': loss_sfl,
                'test_acc': acc_sfl,
                'model_type': 'SFL',
                'level': 'Global',
                'eh_idx': -1  # å…¨å±€æ¨¡å‹æ²¡æœ‰EHç´¢å¼•
            }
            
            print(f"  [SFL Global]: Acc {acc_sfl:.2f}%, Loss {loss_sfl:.4f}")
            
            # --- æ”¶æ•›æ€§æ£€æŸ¥ ---
            print(f"\n[Convergence Check] Epoch {epoch} | EH_Round {t3+1}/{k3}")
            
            # æ£€æŸ¥HFLéšæœºBçŸ©é˜µçš„æ”¶æ•›æ€§
            if not converged_hfl_random:
                hfl_random_converged_count = 0
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_random):
                    if eh_weights is not None and eh_idx in eh_testsets_random:
                        # æ‰¾åˆ°å¯¹åº”çš„æµ‹è¯•ç»“æœ
                        eh_loss = None
                        eh_acc = None
                        for result in eh_results_random:
                            if result['eh_idx'] == eh_idx:
                                eh_loss = result['test_loss']
                                eh_acc = result['test_acc']
                                break
                        
                        if eh_loss is not None and eh_acc is not None:
                            should_stop, reason = eh_checkers_random[eh_idx].check(eh_loss, eh_acc, epoch)
                            print(f"  [Random] EH {eh_idx}: {reason}")
                            if should_stop:
                                hfl_random_converged_count += 1
                
                # å¦‚æœæ‰€æœ‰EHéƒ½æ”¶æ•›ï¼Œåˆ™æ•´ä¸ªHFLéšæœºBçŸ©é˜µæœºåˆ¶æ”¶æ•›
                active_ehs_random = len([eh for eh in EHs_ws_hfl_random if eh is not None])
                if hfl_random_converged_count == active_ehs_random and active_ehs_random > 0:
                    converged_hfl_random = True
                    print(f"  ğŸ¯ [Random] HFLéšæœºBçŸ©é˜µæœºåˆ¶å·²æ”¶æ•›ï¼æ‰€æœ‰ {active_ehs_random} ä¸ªEHéƒ½æ»¡è¶³æ”¶æ•›æ¡ä»¶")
                else:
                    print(f"  [Random] æ”¶æ•›è¿›åº¦: {hfl_random_converged_count}/{active_ehs_random} EHå·²æ”¶æ•›")
            
            # æ£€æŸ¥HFLèšç±»BçŸ©é˜µçš„æ”¶æ•›æ€§
            if not converged_hfl_cluster:
                hfl_cluster_converged_count = 0
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_cluster):
                    if eh_weights is not None and eh_idx in eh_testsets_cluster:
                        # æ‰¾åˆ°å¯¹åº”çš„æµ‹è¯•ç»“æœ
                        eh_loss = None
                        eh_acc = None
                        for result in eh_results_cluster:
                            if result['eh_idx'] == eh_idx:
                                eh_loss = result['test_loss']
                                eh_acc = result['test_acc']
                                break
                        
                        if eh_loss is not None and eh_acc is not None:
                            should_stop, reason = eh_checkers_cluster[eh_idx].check(eh_loss, eh_acc, epoch)
                            print(f"  [Cluster] EH {eh_idx}: {reason}")
                            if should_stop:
                                hfl_cluster_converged_count += 1
                
                # å¦‚æœæ‰€æœ‰EHéƒ½æ”¶æ•›ï¼Œåˆ™æ•´ä¸ªHFLèšç±»BçŸ©é˜µæœºåˆ¶æ”¶æ•›
                active_ehs_cluster = len([eh for eh in EHs_ws_hfl_cluster if eh is not None])
                if hfl_cluster_converged_count == active_ehs_cluster and active_ehs_cluster > 0:
                    converged_hfl_cluster = True
                    print(f"  ğŸ¯ [Cluster] HFLèšç±»BçŸ©é˜µæœºåˆ¶å·²æ”¶æ•›ï¼æ‰€æœ‰ {active_ehs_cluster} ä¸ªEHéƒ½æ»¡è¶³æ”¶æ•›æ¡ä»¶")
                else:
                    print(f"  [Cluster] æ”¶æ•›è¿›åº¦: {hfl_cluster_converged_count}/{active_ehs_cluster} EHå·²æ”¶æ•›")
            
            # æ£€æŸ¥SFLçš„æ”¶æ•›æ€§
            if not converged_sfl:
                should_stop, reason = sfl_checker.check(loss_sfl, acc_sfl, epoch)
                print(f"  [SFL] {reason}")
                if should_stop:
                    converged_sfl = True
                    print(f"  ğŸ¯ [SFL] SFLæœºåˆ¶å·²æ”¶æ•›ï¼")
            
            # å°†EHæµ‹è¯•ç»“æœæ·»åŠ åˆ°ç»“æœå†å²ä¸­
            results_history.extend(eh_results_random)
            results_history.extend(eh_results_cluster)
            results_history.append(sfl_result)
            
            # æ¯æ¬¡EHæµ‹è¯•åæ›´æ–°CSVæ–‡ä»¶
            save_results_to_csv(results_history, csv_filename)

        # HFL å…¨å±€èšåˆ (EH -> Cloud) - åªå¯¹æœªæ”¶æ•›çš„æœºåˆ¶è¿›è¡Œèšåˆ
        if not converged_hfl_random:
            w_glob_hfl_random = FedAvg(EHs_ws_hfl_random)
            net_glob_hfl_random.load_state_dict(w_glob_hfl_random)
            t_hfl_random += t_eh_to_cloud_random
        else:
            print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€èšåˆ")
        
        if not converged_hfl_cluster:
            w_glob_hfl_cluster = FedAvg(EHs_ws_hfl_cluster)
            net_glob_hfl_cluster.load_state_dict(w_glob_hfl_cluster)
            t_hfl_design += t_eh_to_cloud_design
        else:
            print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€èšåˆ")

        # --- åœ¨æ¯ä¸ª EPOCH ç»“æŸæ—¶è¿›è¡Œæµ‹è¯• - åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶ ---
        print(f"\n[End of Epoch {epoch}] æµ‹è¯•å…¨å±€æ¨¡å‹æ€§èƒ½...")
        
        # è¯„ä¼° HFL éšæœºBæ¨¡å‹
        if not converged_hfl_random:
            net_glob_hfl_random.eval()
            acc_hfl_random, loss_hfl_random = test_img(net_glob_hfl_random, dataset_test, args)
            acc_test_hfl_random.append(acc_hfl_random)
            loss_test_hfl_random.append(loss_hfl_random)
        else:
            # ä½¿ç”¨ä¸Šä¸€è½®ç»“æœä½œä¸ºå ä½ç¬¦
            acc_hfl_random = acc_test_hfl_random[-1] if acc_test_hfl_random else 0.0
            loss_hfl_random = loss_test_hfl_random[-1] if loss_test_hfl_random else 0.0
            acc_test_hfl_random.append(acc_hfl_random)
            loss_test_hfl_random.append(loss_hfl_random)
            print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œä½¿ç”¨ä¸Šä¸€è½®ç»“æœ")

        # è¯„ä¼° HFL èšç±»Bæ¨¡å‹
        if not converged_hfl_cluster:
            net_glob_hfl_cluster.eval()
            acc_hfl_cluster, loss_hfl_cluster = test_img(net_glob_hfl_cluster, dataset_test, args)
            acc_test_hfl_cluster.append(acc_hfl_cluster)
            loss_test_hfl_cluster.append(loss_hfl_cluster)
        else:
            # ä½¿ç”¨ä¸Šä¸€è½®ç»“æœä½œä¸ºå ä½ç¬¦
            acc_hfl_cluster = acc_test_hfl_cluster[-1] if acc_test_hfl_cluster else 0.0
            loss_hfl_cluster = loss_test_hfl_cluster[-1] if loss_test_hfl_cluster else 0.0
            acc_test_hfl_cluster.append(acc_hfl_cluster)
            loss_test_hfl_cluster.append(loss_hfl_cluster)
            print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œä½¿ç”¨ä¸Šä¸€è½®ç»“æœ")

        # è¯„ä¼° SFL æ¨¡å‹
        if not converged_sfl:
            net_glob_sfl.eval()
            acc_sfl, loss_sfl = test_img(net_glob_sfl, dataset_test, args)
            acc_test_sfl.append(acc_sfl)
            loss_test_sfl.append(loss_sfl)
        else:
            # ä½¿ç”¨ä¸Šä¸€è½®ç»“æœä½œä¸ºå ä½ç¬¦
            acc_sfl = acc_test_sfl[-1] if acc_test_sfl else 0.0
            loss_sfl = loss_test_sfl[-1] if loss_test_sfl else 0.0
            acc_test_sfl.append(acc_sfl)
            loss_test_sfl.append(loss_sfl)
            print(f"  [Skip] SFLå·²æ”¶æ•›ï¼Œä½¿ç”¨ä¸Šä¸€è½®ç»“æœ")

        # è®°å½•å½“å‰epochçš„ç»“æœ - æ–°æ ¼å¼
        current_epoch_results = [
            {
                'epoch': epoch,
                'eh_round': k3,  # å®Œæ•´çš„EHè½®æ¬¡
                'es_round': k2,  # å®Œæ•´çš„ESè½®æ¬¡
                'train_loss': loss_avg_hfl_random,
                'test_loss': loss_hfl_random,
                'test_acc': acc_hfl_random,
                'model_type': 'HFL_Random_B',
                'level': 'Global',
                'eh_idx': -1
            },
            {
                'epoch': epoch,
                'eh_round': k3,  # å®Œæ•´çš„EHè½®æ¬¡
                'es_round': k2,  # å®Œæ•´çš„ESè½®æ¬¡
                'train_loss': loss_avg_hfl_cluster,
                'test_loss': loss_hfl_cluster,
                'test_acc': acc_hfl_cluster,
                'model_type': 'HFL_Cluster_B',
                'level': 'Global',
                'eh_idx': -1
            },
            {
                'epoch': epoch,
                'eh_round': k3,  # å®Œæ•´çš„EHè½®æ¬¡
                'es_round': k2,  # å®Œæ•´çš„ESè½®æ¬¡
                'train_loss': loss_avg_sfl,
                'test_loss': loss_sfl,
                'test_acc': acc_sfl,
                'model_type': 'SFL',
                'level': 'Global',
                'eh_idx': -1
            }
        ]
        
        results_history.extend(current_epoch_results)

        # ä¿å­˜ç»“æœåˆ°CSV
        save_results_to_csv(results_history, csv_filename)

        # æ‰“å°å½“å‰ EPOCH ç»“æŸæ—¶çš„æµ‹è¯•ç»“æœ
        print(f'\nEpoch {epoch} [END OF EPOCH TEST]')
        print(f'HFL_Random: Acc {acc_hfl_random:.2f}%, Loss {loss_hfl_random:.4f}')
        print(f'HFL_Cluster: Acc {acc_hfl_cluster:.2f}%, Loss {loss_hfl_cluster:.4f}')
        print(f'SFL: Acc {acc_sfl:.2f}%, Loss {loss_sfl:.4f}')

        # --- æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æœºåˆ¶éƒ½å·²æ”¶æ•› ---
        if converged_hfl_random and converged_hfl_cluster and converged_sfl:
            print(f"\nğŸ‰ æ‰€æœ‰è”é‚¦å­¦ä¹ æœºåˆ¶éƒ½å·²æ”¶æ•›ï¼æå‰ç»“æŸè®­ç»ƒã€‚")
            print(f"å®é™…è®­ç»ƒè½®æ¬¡: {epoch + 1}/{args.epochs}")
            final_epoch = epoch + 1
            break
        else:
            print(f"\nğŸ“Š æ”¶æ•›çŠ¶æ€: HFL_Random={'âœ…' if converged_hfl_random else 'âŒ'}, "
                  f"HFL_Cluster={'âœ…' if converged_hfl_cluster else 'âŒ'}, "
                  f"SFL={'âœ…' if converged_sfl else 'âŒ'}")

        net_glob_hfl_random.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
        net_glob_hfl_cluster.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
        net_glob_sfl.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼

    # =====================================================================================
    # Final Testing - æµ‹è¯•ä¸‰ç§æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½
    # =====================================================================================
    print("\n--- Final Model Evaluation ---")
    print(f"æœ€ç»ˆæ”¶æ•›çŠ¶æ€:")
    print(f"  HFL_Random: {'å·²æ”¶æ•› âœ…' if converged_hfl_random else 'æœªæ”¶æ•› âŒ'}")
    print(f"  HFL_Cluster: {'å·²æ”¶æ•› âœ…' if converged_hfl_cluster else 'æœªæ”¶æ•› âŒ'}")
    print(f"  SFL: {'å·²æ”¶æ•› âœ…' if converged_sfl else 'æœªæ”¶æ•› âŒ'}")
    print(f"å®é™…è®­ç»ƒè½®æ¬¡: {final_epoch}/{args.epochs}")
    
    # æµ‹è¯• HFL éšæœºBçŸ©é˜µæ¨¡å‹
    net_glob_hfl_random.eval()
    acc_train_hfl_random, loss_train_final_hfl_random = test_img(net_glob_hfl_random, dataset_train, args)
    acc_test_final_hfl_random, loss_test_final_hfl_random = test_img(net_glob_hfl_random, dataset_test, args)
    converged_str = "å·²æ”¶æ•›" if converged_hfl_random else "æœªæ”¶æ•›"
    print(f"HFL Model (Random B Matrix) [{converged_str}] - Training accuracy: {acc_train_hfl_random:.2f}%, Testing accuracy: {acc_test_final_hfl_random:.2f}%")
    
    # æµ‹è¯• HFL èšç±»BçŸ©é˜µæ¨¡å‹
    net_glob_hfl_cluster.eval()
    acc_train_hfl_cluster, loss_train_final_hfl_cluster = test_img(net_glob_hfl_cluster, dataset_train, args)
    acc_test_final_hfl_cluster, loss_test_final_hfl_cluster = test_img(net_glob_hfl_cluster, dataset_test, args)
    converged_str = "å·²æ”¶æ•›" if converged_hfl_cluster else "æœªæ”¶æ•›"
    print(f"HFL Model (Clustered B Matrix) [{converged_str}] - Training accuracy: {acc_train_hfl_cluster:.2f}%, Testing accuracy: {acc_test_final_hfl_cluster:.2f}%")

    # æµ‹è¯• SFL æ¨¡å‹
    net_glob_sfl.eval()
    acc_train_sfl, loss_train_final_sfl = test_img(net_glob_sfl, dataset_train, args)
    acc_test_final_sfl, loss_test_final_sfl = test_img(net_glob_sfl, dataset_test, args)
    converged_str = "å·²æ”¶æ•›" if converged_sfl else "æœªæ”¶æ•›"
    print(f"SFL Model (Single Layer) [{converged_str}] - Training accuracy: {acc_train_sfl:.2f}%, Testing accuracy: {acc_test_final_sfl:.2f}%")

    # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆæ·»åŠ åˆ°ç»“æœå†å²åˆ—è¡¨ä¸­ï¼‰
    # final_epoch å·²åœ¨å‰é¢å®šä¹‰ï¼Œæ­¤å¤„ä¸éœ€è¦é‡å¤å®šä¹‰
    
    # æ·»åŠ æœ€ç»ˆç»“æœåˆ°ç»“æœå†å²åˆ—è¡¨
    final_results = [
        {
            'epoch': final_epoch,
            'eh_round': k3,
            'es_round': k2,
            'train_loss': loss_train_final_hfl_random,
            'test_loss': loss_test_final_hfl_random,
            'test_acc': acc_test_final_hfl_random,
            'model_type': 'HFL_Random_B',
            'level': 'Final',
            'eh_idx': -1
        },
        {
            'epoch': final_epoch,
            'eh_round': k3,
            'es_round': k2,
            'train_loss': loss_train_final_hfl_cluster,
            'test_loss': loss_test_final_hfl_cluster,
            'test_acc': acc_test_final_hfl_cluster,
            'model_type': 'HFL_Cluster_B',
            'level': 'Final',
            'eh_idx': -1
        },
        {
            'epoch': final_epoch,
            'eh_round': k3,
            'es_round': k2,
            'train_loss': loss_train_final_sfl,
            'test_loss': loss_test_final_sfl,
            'test_acc': acc_test_final_sfl,
            'model_type': 'SFL',
            'level': 'Final',
            'eh_idx': -1
        }
    ]
    
    # å°†æœ€ç»ˆç»“æœæ·»åŠ åˆ°ç»“æœå†å²ä¸­
    results_history.extend(final_results)
    
    # é‡æ–°ä¿å­˜æ•´ä¸ªç»“æœå†å²åˆ°CSVæ–‡ä»¶
    save_results_to_csv(results_history, csv_filename)
    
    # å°†æœ€ç»ˆç»“æœå•ç‹¬ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
    final_summary = [
        {
            'model_type': 'HFL_Random_B',
            'final_train_acc': acc_train_hfl_random,
            'final_train_loss': loss_train_final_hfl_random,
            'final_test_acc': acc_test_final_hfl_random,
            'final_test_loss': loss_test_final_hfl_random
        },
        {
            'model_type': 'HFL_Cluster_B',
            'final_train_acc': acc_train_hfl_cluster,
            'final_train_loss': loss_train_final_hfl_cluster,
            'final_test_acc': acc_test_final_hfl_cluster,
            'final_test_loss': loss_test_final_hfl_cluster
        },
        {
            'model_type': 'SFL',
            'final_train_acc': acc_train_sfl,
            'final_train_loss': loss_train_final_sfl,
            'final_test_acc': acc_test_final_sfl,
            'final_test_loss': loss_test_final_sfl
        }
    ]

    # å°†æœ€ç»ˆæ±‡æ€»ç»“æœè¿½åŠ åˆ°CSVæ–‡ä»¶
    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([])  # ç©ºè¡Œåˆ†éš”
        writer.writerow(['Final Summary'])
        writer.writerow(['model_type', 'final_train_acc', 'final_train_loss', 'final_test_acc', 'final_test_loss'])
        for result in final_summary:
            writer.writerow([result['model_type'], result['final_train_acc'], 
                            result['final_train_loss'], result['final_test_acc'], result['final_test_loss']])

    print(f"\n=== è®­ç»ƒå®Œæˆ ===")
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {csv_filename}")
    
    try:
        print("\næ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        comprehensive_file, summary_file = create_enhanced_visualizations(csv_filename)
        print(f"ç»¼åˆå¯¹æ¯”å›¾: {comprehensive_file}")
        print(f"æ€§èƒ½æ€»ç»“è¡¨: {summary_file}")
    except Exception as e:
        print(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–æ‰‹åŠ¨è¿è¡Œvisualization_tool.py")
    
    # ä¿å­˜é€šä¿¡æ—¶é—´ç»“æœåˆ°å•ç‹¬çš„CSVæ–‡ä»¶
    communication_filename = f"communication_results_{timestamp}.csv"
    communication_csv_path = f'./results/{communication_filename}'
    
    with open(communication_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['model_type', 'total_communication_time', 'epochs', 'avg_communication_per_epoch'])
        writer.writerow(['HFL_Random_B', f"{t_hfl_random:.6f}", final_epoch, f"{t_hfl_random/final_epoch:.6f}"])
        writer.writerow(['HFL_Cluster_B', f"{t_hfl_design:.6f}", final_epoch, f"{t_hfl_design/final_epoch:.6f}"])
        writer.writerow(['SFL', f"{t_sfl:.6f}", final_epoch, f"{t_sfl/final_epoch:.6f}"])
    
    print(f"é€šä¿¡æ—¶é—´ç»“æœå·²ä¿å­˜åˆ°: {communication_csv_path}")

    print(f"\n=== å®éªŒæ€»ç»“ ===")
    print("æœ¬æ¬¡å®éªŒå¯¹æ¯”äº†ä¸‰ç§è”é‚¦å­¦ä¹ æ–¹æ³•:")
    print("1. HFL (Random B Matrix) - ä½¿ç”¨éšæœºç”Ÿæˆçš„ES-EHå…³è”çŸ©é˜µ")
    print("2. HFL (Clustered B Matrix) - ä½¿ç”¨è°±èšç±»ç”Ÿæˆçš„ES-EHå…³è”çŸ©é˜µ") 
    print("3. SFL (Single Layer) - ä¼ ç»Ÿå•å±‚è”é‚¦å­¦ä¹ ")
    print(f"è®­ç»ƒå‚æ•°: è®¾å®šepochs={args.epochs}, å®é™…epochs={final_epoch}, clients={args.num_users}, local_epochs={args.local_ep}")
    print(f"å±‚çº§å‚æ•°: k2={args.ES_k2} (ESå±‚èšåˆè½®æ•°), k3={args.EH_k3} (EHå±‚èšåˆè½®æ•°)")
    print(f"å¹¶è¡Œå‚æ•°: num_processes={args.num_processes}")
    print(f"æ•°æ®é›†: {args.dataset}, æ¨¡å‹: {args.model}, IID: {args.iid}")
    if not args.iid:
        print(f"éIIDå‚æ•°: beta={args.beta}")
    
    print(f"\n=== é€šä¿¡å¼€é”€åˆ†æ ===")
    print(f"æ€»é€šä¿¡æ—¶é—´å¯¹æ¯”:")
    print(f"  â€¢ HFL_Random: {t_hfl_random:.6f}s (å¹³å‡æ¯è½®: {t_hfl_random/final_epoch:.6f}s)")
    print(f"  â€¢ HFL_Cluster: {t_hfl_design:.6f}s (å¹³å‡æ¯è½®: {t_hfl_design/final_epoch:.6f}s)")
    print(f"  â€¢ SFL: {t_sfl:.6f}s (å¹³å‡æ¯è½®: {t_sfl/final_epoch:.6f}s)")
    
    print(f"\n=== æ”¶æ•›æ€§åˆ†æ ===")
    print(f"æ”¶æ•›æ£€æŸ¥å™¨å‚æ•°: patience=5, min_delta=0.001")
    print(f"æœ€ç»ˆæ”¶æ•›çŠ¶æ€:")
    print(f"  â€¢ HFL_Random: {'âœ… å·²æ”¶æ•›' if converged_hfl_random else 'âŒ æœªæ”¶æ•›'}")
    print(f"  â€¢ HFL_Cluster: {'âœ… å·²æ”¶æ•›' if converged_hfl_cluster else 'âŒ æœªæ”¶æ•›'}")  
    print(f"  â€¢ SFL: {'âœ… å·²æ”¶æ•›' if converged_sfl else 'âŒ æœªæ”¶æ•›'}")
    
    if converged_hfl_random and converged_hfl_cluster and converged_sfl:
        print(f"ğŸ‰ æ‰€æœ‰æœºåˆ¶å‡æ”¶æ•›ï¼Œè®­ç»ƒåœ¨ç¬¬{final_epoch}è½®æå‰ç»“æŸ")
    elif final_epoch < args.epochs:
        print(f"âš ï¸ éƒ¨åˆ†æœºåˆ¶æ”¶æ•›ï¼Œè®­ç»ƒåœ¨ç¬¬{final_epoch}è½®æå‰ç»“æŸ")
    else:
        print(f"â° è®­ç»ƒå®Œæˆè®¾å®šçš„{args.epochs}è½®ï¼Œéƒ¨åˆ†æœºåˆ¶å¯èƒ½æœªå®Œå…¨æ”¶æ•›")
    
    # æ˜¾ç¤ºæœ€ç»ˆæ€§èƒ½å¯¹æ¯”
    try:
        df = pd.read_csv(csv_filename, encoding='utf-8')
        final_results = df[df['epoch'] == df['epoch'].max()]
        print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”:")
        for _, row in final_results.iterrows():
            model_name = row['model_type'].replace('_', ' ')
            print(f"  {model_name}: {row['test_acc']:.2f}%")
    except:
        pass
