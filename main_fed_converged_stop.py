#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6
import os
import matplotlib

from utils.visualization_tool import create_enhanced_visualizations

matplotlib.use('Agg')
# è®¾ç½® matplotlib ä½¿ç”¨è‹±æ–‡å­—ä½“ï¼Œé¿å…ä¸­æ–‡å­—ä½“è­¦å‘Š
matplotlib.rcParams.update({
    'font.family': 'DejaVu Sans',
    'axes.unicode_minus': True  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
})
import matplotlib.pyplot as plt
import copy
import numpy as np
from torchvision import datasets, transforms
import torch
from tqdm import tqdm
import torch.multiprocessing as mp
import os
os.environ["OMP_NUM_THREADS"] = "1"

import csv
from datetime import datetime
import pandas as pd

from utils.sampling import get_data
from utils.options import args_parser
from utils.data_partition import get_client_datasets
from utils.visualize_client_data import visualize_client_data_distribution
from utils.eh_test_utils import EHTestsetGenerator, test_eh_model
from utils.bipartite_bandwidth import run_bandwidth_allocation, calculate_distance
from utils.comm_utils import calculate_transmission_time, get_model_size_in_bits, select_eh, select_eh_random
from models.Nets import MLP, CNNMnist, CNNCifar, LR, ResNet18, VGG11, MobileNetCifar, LeNet5
from models.Fed import FedAvg, FedAvg_layered
from models.test import test_img
from models.Update import LocalUpdate
from models.ES_cluster import (
    train_initial_models,
    train_initial_models_with_es_aggregation,
    aggregate_es_models, spectral_clustering_es,
    calculate_es_label_distributions,
    visualize_clustering_comparison
)
from utils.conver_check import ConvergenceChecker
import numpy as np

def validate_data_distribution(dict_users, dataset_train, args):
    """
    éªŒè¯å®¢æˆ·ç«¯æ•°æ®åˆ†é…çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§
    
    Args:
        dict_users: å®¢æˆ·ç«¯æ•°æ®ç´¢å¼•åˆ†é…å­—å…¸
        dataset_train: è®­ç»ƒæ•°æ®é›†
        args: å‚æ•°å¯¹è±¡
    
    Returns:
        bool: éªŒè¯æ˜¯å¦é€šè¿‡
    """
    print(f"\n=== è¯¦ç»†æ•°æ®åˆ†é…éªŒè¯ ===")
    
    # 1. æ£€æŸ¥å®¢æˆ·ç«¯æ•°é‡
    if len(dict_users) != args.num_users:
        print(f"âŒ å®¢æˆ·ç«¯æ•°é‡ä¸åŒ¹é…: æœŸæœ›{args.num_users}, å®é™…{len(dict_users)}")
        return False
    
    # 2. æ£€æŸ¥å®¢æˆ·ç«¯IDçš„è¿ç»­æ€§
    expected_ids = set(range(args.num_users))
    actual_ids = set(dict_users.keys())
    if expected_ids != actual_ids:
        missing_ids = expected_ids - actual_ids
        extra_ids = actual_ids - expected_ids
        print(f"âŒ å®¢æˆ·ç«¯IDä¸è¿ç»­:")
        if missing_ids:
            print(f"   ç¼ºå¤±ID: {missing_ids}")
        if extra_ids:
            print(f"   å¤šä½™ID: {extra_ids}")
        return False
    
    # 3. æ£€æŸ¥æ•°æ®ç´¢å¼•çš„æœ‰æ•ˆæ€§å’Œç»Ÿè®¡ä¿¡æ¯
    total_assigned_samples = 0
    empty_clients = []
    invalid_indices_clients = []
    
    for client_id, data_indices in dict_users.items():
        # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿ç»Ÿä¸€å¤„ç†
        if isinstance(data_indices, set):
            data_indices = list(data_indices)
        elif isinstance(data_indices, np.ndarray):
            data_indices = data_indices.tolist()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if len(data_indices) == 0:
            empty_clients.append(client_id)
            continue
        
        # æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
        max_index = max(data_indices)
        min_index = min(data_indices)
        
        if max_index >= len(dataset_train) or min_index < 0:
            invalid_indices_clients.append({
                'client_id': client_id,
                'max_index': max_index,
                'min_index': min_index,
                'dataset_size': len(dataset_train)
            })
        
        total_assigned_samples += len(data_indices)
    
    # æŠ¥å‘Šé—®é¢˜
    if empty_clients:
        print(f"âš ï¸  å‘ç°ç©ºå®¢æˆ·ç«¯: {empty_clients}")
    
    if invalid_indices_clients:
        print(f"âŒ å‘ç°æ— æ•ˆæ•°æ®ç´¢å¼•çš„å®¢æˆ·ç«¯:")
        for client_info in invalid_indices_clients:
            print(f"   å®¢æˆ·ç«¯{client_info['client_id']}: ç´¢å¼•èŒƒå›´[{client_info['min_index']}, {client_info['max_index']}], æ•°æ®é›†å¤§å°{client_info['dataset_size']}")
        return False
    
    # 4. ç»Ÿè®¡ä¿¡æ¯
    sample_counts = [len(dict_users[i]) if isinstance(dict_users[i], (list, set)) else len(dict_users[i].tolist()) 
                     for i in range(args.num_users)]
    
    print(f"âœ… æ•°æ®åˆ†é…éªŒè¯é€šè¿‡:")
    print(f"   æ€»å®¢æˆ·ç«¯æ•°: {len(dict_users)}")
    print(f"   æ€»åˆ†é…æ ·æœ¬æ•°: {total_assigned_samples}")
    print(f"   æ•°æ®é›†æ€»å¤§å°: {len(dataset_train)}")
    print(f"   å¹³å‡æ¯å®¢æˆ·ç«¯æ ·æœ¬æ•°: {total_assigned_samples/len(dict_users):.1f}")
    print(f"   æ ·æœ¬æ•°èŒƒå›´: [{min(sample_counts)}, {max(sample_counts)}]")
    
    if empty_clients:
        print(f"   ç©ºå®¢æˆ·ç«¯æ•°: {len(empty_clients)}")
    
    print("=" * 30)
    return True

def save_communication_results_to_csv(network_scale, hfl_cluster_time, hfl_random_time, sfl_time,
                                    hfl_cluster_power, hfl_random_power, sfl_power, 
                                    dataset, model, lr=None):
    """
    ä¿å­˜é€šä¿¡æ—¶é—´å’Œèƒ½è€—ç»“æœåˆ°CSVæ–‡ä»¶
    
    Args:
        network_scale (int): ç½‘ç»œè§„æ¨¡ï¼ˆç”¨æˆ·æ•°é‡ï¼‰
        hfl_cluster_time (float): HFLèšç±»æ–¹æ³•çš„é€šä¿¡æ—¶é—´
        hfl_random_time (float): HFLéšæœºæ–¹æ³•çš„é€šä¿¡æ—¶é—´
        sfl_time (float): SFLæ–¹æ³•çš„é€šä¿¡æ—¶é—´
        hfl_cluster_power (float): HFLèšç±»æ–¹æ³•çš„é€šä¿¡èƒ½è€—
        hfl_random_power (float): HFLéšæœºæ–¹æ³•çš„é€šä¿¡èƒ½è€—
        sfl_power (float): SFLæ–¹æ³•çš„é€šä¿¡èƒ½è€—
        dataset (str): æ•°æ®é›†åç§°
        model (str): æ¨¡å‹åç§°
        lr (float, optional): å­¦ä¹ ç‡å‚æ•°
    """
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç”Ÿæˆæ–‡ä»¶åï¼šç½‘ç»œè§„æ¨¡_æ•°æ®é›†_æ¨¡å‹_å­¦ä¹ ç‡_æ—¶é—´æˆ³
    lr_str = f"_lr{lr}" if lr is not None else ""
    filename = f"./results/comm_results_scale{network_scale}_{dataset}_{model}{lr_str}_{timestamp}.csv"
    
    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    if not os.path.exists('./results'):
        os.makedirs('./results')
    
    # å‡†å¤‡æ•°æ®
    data = []
    
    # æ·»åŠ æ—¶é—´ç»“æœè¡Œ
    data.append({
        'Network Scale': network_scale,
        'hfl_cluster': hfl_cluster_time,
        'hfl_random': hfl_random_time,
        'sfl': sfl_time,
        'type': 't'
    })
    
    # æ·»åŠ èƒ½è€—ç»“æœè¡Œ
    data.append({
        'Network Scale': network_scale,
        'hfl_cluster': hfl_cluster_power,
        'hfl_random': hfl_random_power,
        'sfl': sfl_power,
        'type': 'p'
    })
    
    # å†™å…¥CSVæ–‡ä»¶
    try:
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['Network Scale', 'hfl_cluster', 'hfl_random', 'sfl', 'type']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            # å†™å…¥è¡¨å¤´
            writer.writeheader()
            
            # å†™å…¥æ•°æ®
            writer.writerows(data)
        
        print(f"\nâœ… é€šä¿¡ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š æ•°æ®æ ¼å¼:")
        print(f"   ç½‘ç»œè§„æ¨¡: {network_scale} ç”¨æˆ·")
        print(f"   æ—¶é—´ç»“æœ - HFLèšç±»: {hfl_cluster_time:.6f}s, HFLéšæœº: {hfl_random_time:.6f}s, SFL: {sfl_time:.6f}s")
        print(f"   èƒ½è€—ç»“æœ - HFLèšç±»: {hfl_cluster_power:.6f}J, HFLéšæœº: {hfl_random_power:.6f}J, SFL: {sfl_power:.6f}J")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜é€šä¿¡ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")

def build_model(args, dataset_train):
    img_size = dataset_train[0][0].shape

    if args.model == 'cnn':
        if args.dataset in ['cifar', 'cifar100']:  # æ”¯æŒ cifar å’Œ cifar100
            net_glob = CNNCifar(args=args).to(args.device)  # CNNCifar éœ€è¦æ”¯æŒ args.num_classes=100
        elif args.dataset == 'mnist':
            net_glob = CNNMnist(args=args).to(args.device)
    elif args.model == 'mlp':
        # è®¡ç®—å°†å›¾ç‰‡å±•å¹³åçš„è¾“å…¥å±‚ç»´åº¦
        len_in = 1
        for x in img_size:
            len_in *= x
        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)

    # ======================= æ–° =======================
    elif args.model == 'lr' and args.dataset == 'mnist':
        len_in = 1
        for x in img_size:
            len_in *= x
        net_glob = LR(dim_in=len_in, dim_out=args.num_classes).to(args.device)

    elif args.model == 'lenet5' and args.dataset == 'mnist':
        net_glob = LeNet5(args=args).to(args.device)

    elif args.model == 'vgg11' and args.dataset in ['cifar', 'cifar100']:
        net_glob = VGG11(args=args).to(args.device)

    elif args.model == 'resnet18' and args.dataset in ['cifar', 'cifar100']:
        net_glob = ResNet18(args=args).to(args.device)  # ResNet18 éœ€è¦æ”¯æŒ args.num_classes=100

    else:
        exit('é”™è¯¯ï¼šæ— æ³•è¯†åˆ«çš„æ¨¡å‹')

    # print("--- æ¨¡å‹æ¶æ„ ---")
    # print(net_glob)
    # print("--------------------")
    return net_glob

def get_A_random(num_users, num_ESs):
    A = np.zeros((num_users, num_ESs), dtype=int)

    # æ¯ä¸ª ES è‡³å°‘è¦åˆ†åˆ°çš„ç”¨æˆ·æ•°
    base = num_users // num_ESs
    # å¤šå‡ºæ¥çš„ç”¨æˆ·æ•°é‡
    extra = num_users % num_ESs

    # ç”¨æˆ·ç´¢å¼•
    users = np.arange(num_users)
    np.random.shuffle(users)  # æ‰“ä¹±é¡ºåºï¼Œä¿è¯éšæœºæ€§

    start = 0
    for es in range(num_ESs):
        count = base + (1 if es < extra else 0)
        assigned_users = users[start:start+count]
        for u in assigned_users:
            A[u, es] = 1
        start += count

    return A

def get_B(num_ESs, num_EHs):
    B = np.zeros((num_ESs, num_EHs), dtype=int)

    # å¯¹æ¯ä¸€è¡Œéšæœºé€‰æ‹©ä¸€ä¸ªç´¢å¼•ï¼Œå°†è¯¥ä½ç½®è®¾ä¸º 1
    for i in range(num_ESs):
        random_index = np.random.randint(0, num_EHs)
        B[i, random_index] = 1

    return B

def get_B_cluster(args, w_locals, A, dict_users, net_glob, client_label_distributions):
    """
    ä½¿ç”¨è°±èšç±»ç”Ÿæˆ ES-EH å…³è”çŸ©é˜µ Bï¼Œå¹¶å¯è§†åŒ–èšç±»ç»“æœ
    """
    print("å¼€å§‹è°±èšç±»ç”ŸæˆBçŸ©é˜µ...")

    # 1. èšåˆESæ¨¡å‹
    es_models = aggregate_es_models(w_locals, A, dict_users, net_glob)

    # 2. ä½¿ç”¨è°±èšç±»è·å–ES-EHå…³è”çŸ©é˜µB
    B, cluster_labels = spectral_clustering_es(
        es_models,
        epsilon=args.epsilon  # ä»å‚æ•°ä¸­è·å–
    )

    # 3. è®¡ç®—ESçš„æ ‡ç­¾åˆ†å¸ƒå¹¶å¯è§†åŒ–
    es_label_distributions = calculate_es_label_distributions(A, client_label_distributions)

    #labels1, labels2, labels3 = run_all_clusterings(es_models, epsilon=args.epsilon)
    # åœ¨å®Œæˆè°±èšç±»åæ·»åŠ å¯¹æ¯”å¯è§†åŒ–
    visualize_clustering_comparison(
        es_label_distributions=es_label_distributions,
        cluster_labels=cluster_labels,
        save_path='./save/clustering_comparison.png'
    )
    return B

def get_B_cluster_from_es_models(args, es_models, A_design, client_label_distributions):
    """
    ä»ESå±‚èšåˆæ¨¡å‹ç›´æ¥ç”Ÿæˆ ES-EH å…³è”çŸ©é˜µ Bï¼Œå¹¶å¯è§†åŒ–èšç±»ç»“æœ
    
    Args:
        args: å‚æ•°é…ç½®
        es_models: ESå±‚èšåˆåçš„æ¨¡å‹åˆ—è¡¨
        A_design: å®¢æˆ·ç«¯-ESå…³è”çŸ©é˜µ
        client_label_distributions: å®¢æˆ·ç«¯æ ‡ç­¾åˆ†å¸ƒ
    
    Returns:
        B: ES-EHå…³è”çŸ©é˜µ
    """
    print("å¼€å§‹ä»ESæ¨¡å‹è¿›è¡Œè°±èšç±»ç”ŸæˆBçŸ©é˜µ...")

    # 1. ç›´æ¥ä½¿ç”¨ESæ¨¡å‹è¿›è¡Œè°±èšç±»ï¼ˆè·³è¿‡èšåˆæ­¥éª¤ï¼‰
    B, cluster_labels = spectral_clustering_es(
        es_models,
        epsilon=args.epsilon  # ä»å‚æ•°ä¸­è·å–
    )

    # 2. è®¡ç®—ESçš„æ ‡ç­¾åˆ†å¸ƒå¹¶å¯è§†åŒ–
    es_label_distributions = calculate_es_label_distributions(A_design, client_label_distributions)
    
    # 3. åœ¨å®Œæˆè°±èšç±»åæ·»åŠ å¯¹æ¯”å¯è§†åŒ–
    visualize_clustering_comparison(
        es_label_distributions=es_label_distributions,
        cluster_labels=cluster_labels,
        save_path='./save/clustering_comparison.png'
    )
    
    print(f"è°±èšç±»å®Œæˆï¼Œç”Ÿæˆ {B.shape[1]} ä¸ªEHç°‡")
    
    # 4. æ‰“å°èšç±»ç»“æœæ‘˜è¦
    print("[ES-EHèšç±»åˆ†é…æ‘˜è¦]:")
    for cluster_id in range(B.shape[1]):
        es_in_cluster = [es_idx for es_idx in range(B.shape[0]) if B[es_idx, cluster_id] == 1]
        print(f"  EHç°‡ {cluster_id}: åŒ…å«ES {es_in_cluster}")
    
    return B

def get_numlist_from_dict_users(hierarchy_dict, device_data_counts):
    """
    è®¡ç®—æ¯ä¸ªè´Ÿè´£è®¾å¤‡ç®¡ç†çš„æ‰€æœ‰è®¾å¤‡çš„æ•°æ®é‡æ€»å’Œ
    
    Args:
        hierarchy_dict: å…³è”å­—å…¸ï¼Œæ ¼å¼ä¸º {è´Ÿè´£è®¾å¤‡idx: [ç®¡ç†è®¾å¤‡idxåˆ—è¡¨]}
        device_data_counts: ç®¡ç†è®¾å¤‡å†…éƒ¨çš„æ•°æ®é‡ï¼Œæ ¼å¼ä¸º {è®¾å¤‡idx: æ•°æ®é‡} æˆ– [æ•°æ®é‡åˆ—è¡¨]
    
    Returns:
        list: æ¯ä¸ªè´Ÿè´£è®¾å¤‡å†…éƒ¨çš„æ•°æ®é‡æ€»å’Œæ•°ç»„
    """
    # å¦‚æœ device_data_counts æ˜¯åˆ—è¡¨å½¢å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸
    if isinstance(device_data_counts, list):
        device_data_dict = {idx: count for idx, count in enumerate(device_data_counts)}
    else:
        device_data_dict = device_data_counts
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„
    num_supervisors = len(hierarchy_dict)
    supervisor_data_counts = [0] * num_supervisors
    
    # è®¡ç®—æ¯ä¸ªè´Ÿè´£è®¾å¤‡ç®¡ç†çš„æ•°æ®é‡æ€»å’Œ
    for supervisor_idx, managed_devices in hierarchy_dict.items():
        total_data = 0
        for device_idx in managed_devices:
            if device_idx in device_data_dict:
                total_data += device_data_dict[device_idx]
            else:
                print(f"Warning: è®¾å¤‡ {device_idx} çš„æ•°æ®é‡æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
        
        supervisor_data_counts[supervisor_idx] = total_data
    
    return supervisor_data_counts
    
# ===== æ ¹æ® Aã€B æ„é€  C1 å’Œ C2 =====
def build_hierarchy(A, B):
    num_users, num_ESs = A.shape
    _, num_EHs = B.shape

    # client -> ES
    C1 = {j: [] for j in range(num_ESs)}
    for i in range(num_users):
        for j in range(num_ESs):
            if A[i][j] == 1:
                C1[j].append(i)

    # ES -> EH
    C2 = {k: [] for k in range(num_EHs)}
    for j in range(num_ESs):
        for k in range(num_EHs):
            if B[j][k] == 1:
                C2[k].append(j)

    return C1, C2

def train_client(args, user_idx, dataset_train, dict_users, w_input_hfl_random, w_input_hfl_cluster, w_input_hfl, 
                 client_classes=None, train_hfl_random=True, train_hfl_cluster=True, train_hfl=True):
    """
    å•ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒå‡½æ•°ï¼Œç”¨äºè¢«å¤šè¿›ç¨‹è°ƒç”¨ã€‚
    ç°åœ¨æ”¯æŒä¸‰ç§æ¨¡å‹ï¼šHFL(ä¸¤å±‚)ã€HFL(éšæœºBçŸ©é˜µä¸‰å±‚)ã€HFL(èšç±»BçŸ©é˜µä¸‰å±‚)

    æ³¨æ„ï¼šä¸ºäº†å…¼å®¹å¤šè¿›ç¨‹ï¼Œæˆ‘ä»¬ä¸ç›´æ¥ä¼ é€’å¤§å‹æ¨¡å‹å¯¹è±¡ï¼Œ
    è€Œæ˜¯ä¼ é€’æ¨¡å‹æƒé‡(state_dict)å’Œæ¨¡å‹æ¶æ„ä¿¡æ¯(args)ï¼Œåœ¨å­è¿›ç¨‹ä¸­é‡æ–°æ„å»ºæ¨¡å‹ã€‚
    
    Args:
        train_hfl_random: æ˜¯å¦è®­ç»ƒHFLéšæœºBçŸ©é˜µæ¨¡å‹ï¼ˆä¸‰å±‚ï¼‰
        train_hfl_cluster: æ˜¯å¦è®­ç»ƒHFLèšç±»BçŸ©é˜µæ¨¡å‹ï¼ˆä¸‰å±‚ï¼‰
        train_hfl: æ˜¯å¦è®­ç»ƒHFLæ¨¡å‹ï¼ˆä¸¤å±‚ï¼‰
    """
    # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°æ„å»ºæ¨¡å‹
    local_net_hfl_random = build_model(args, dataset_train)
    local_net_hfl_cluster = build_model(args, dataset_train)
    local_net_hfl = build_model(args, dataset_train)
    
    # è·å–å½“å‰å®¢æˆ·ç«¯çš„ç±»åˆ«ä¿¡æ¯
    user_classes = client_classes.get(user_idx, None) if client_classes else None
    
    # --- è®­ç»ƒHFLæ¨¡å‹ (ä½¿ç”¨éšæœºBçŸ©é˜µï¼Œä¸‰å±‚) - ä»…åœ¨æœªæ”¶æ•›æ—¶è®­ç»ƒ ---
    if train_hfl_random:
        local_random = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[user_idx], user_classes=user_classes)
        local_net_hfl_random.load_state_dict(w_input_hfl_random)
        w_hfl_random, loss_hfl_random = local_random.train(net=local_net_hfl_random.to(args.device))
    else:
        # å¦‚æœå·²æ”¶æ•›ï¼Œç›´æ¥è¿”å›è¾“å…¥æƒé‡å’Œé›¶æŸå¤±
        w_hfl_random, loss_hfl_random = copy.deepcopy(w_input_hfl_random), 0.0
    
    # --- è®­ç»ƒHFLæ¨¡å‹ (ä½¿ç”¨èšç±»BçŸ©é˜µï¼Œä¸‰å±‚) - ä»…åœ¨æœªæ”¶æ•›æ—¶è®­ç»ƒ ---
    if train_hfl_cluster:
        local_cluster = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[user_idx], user_classes=user_classes)
        local_net_hfl_cluster.load_state_dict(w_input_hfl_cluster)
        w_hfl_cluster, loss_hfl_cluster = local_cluster.train(net=local_net_hfl_cluster.to(args.device))
    else:
        # å¦‚æœå·²æ”¶æ•›ï¼Œç›´æ¥è¿”å›è¾“å…¥æƒé‡å’Œé›¶æŸå¤±
        w_hfl_cluster, loss_hfl_cluster = copy.deepcopy(w_input_hfl_cluster), 0.0
    
    # --- è®­ç»ƒHFLæ¨¡å‹ (ä¸¤å±‚ç»“æ„) - ä»…åœ¨æœªæ”¶æ•›æ—¶è®­ç»ƒ ---
    if train_hfl:
        local_hfl = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[user_idx], user_classes=user_classes)
        local_net_hfl.load_state_dict(w_input_hfl)
        w_hfl, loss_hfl = local_hfl.train(net=local_net_hfl.to(args.device))
    else:
        # å¦‚æœå·²æ”¶æ•›ï¼Œç›´æ¥è¿”å›è¾“å…¥æƒé‡å’Œé›¶æŸå¤±
        w_hfl, loss_hfl = copy.deepcopy(w_input_hfl), 0.0

    # è¿”å›ç»“æœï¼ŒåŒ…æ‹¬ user_idx ä»¥ä¾¿åç»­æ’åº
    return (user_idx, 
            copy.deepcopy(w_hfl_random), loss_hfl_random,
            copy.deepcopy(w_hfl_cluster), loss_hfl_cluster, 
            copy.deepcopy(w_hfl), loss_hfl)

def summarize_results(net_glob_hfl_bipartite, net_glob_hfl_random, net_glob_sfl, dataset_train, dataset_test, args,
                     total_comm_overhead_bipartite_upload, total_comm_overhead_bipartite_download,
                     total_comm_overhead_random_upload, total_comm_overhead_random_download,
                     total_comm_overhead_sfl_upload, total_comm_overhead_sfl_download):
    print("=== Final Communication Overhead Summary ===")
    print(f"SFL Total Overhead: {total_comm_overhead_sfl_upload + total_comm_overhead_sfl_download:.6f}s "
          f"(Upload: {total_comm_overhead_sfl_upload:.6f}s, Download: {total_comm_overhead_sfl_download:.6f}s)")
    print(f"HFL Bipartite Total Overhead: {total_comm_overhead_bipartite_upload + total_comm_overhead_bipartite_download:.6f}s "
          f"(Upload: {total_comm_overhead_bipartite_upload:.6f}s, Download: {total_comm_overhead_bipartite_download:.6f}s)")
    print(f"HFL Random Total Overhead: {total_comm_overhead_random_upload + total_comm_overhead_random_download:.6f}s "
          f"(Upload: {total_comm_overhead_random_upload:.6f}s, Download: {total_comm_overhead_random_download:.6f}s)")

def save_results_to_csv(results, filename):
    """Save results to CSV file for three models, including EH-level testing results"""
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['epoch', 'eh_round', 'es_round', 'train_loss', 'test_loss', 'test_acc', 
                     'model_type', 'level', 'eh_idx']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for result in results:
            writer.writerow(result)

if __name__ == '__main__':
    mp.set_start_method('spawn', force=True)
    # parse args
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

    # å…ˆè¿è¡Œå¸¦å®½åˆ†é…ç®—æ³•è·å–å®é™…çš„å®¢æˆ·ç«¯æ•°é‡
    print("æ­£åœ¨åˆ†æç½‘ç»œæ‹“æ‰‘å¹¶ç¡®å®šå®¢æˆ·ç«¯æ•°é‡...")
    bipartite_graph, client_nodes, active_es_nodes, A_design, r_client_to_es, r_es, r_es_to_cloud, r_client_to_cloud = run_bandwidth_allocation(
        graphml_file=args.graphml_file, 
        es_ratio=args.es_ratio, 
        max_capacity=args.max_capacity, 
        visualize=True)
    
    if bipartite_graph is None:
        print("Failed to build bipartite graph, exiting.")
        exit(1)
    
    # æ ¹æ®å®é™…å®¢æˆ·ç«¯æ•°é‡æ›´æ–°args.num_users
    actual_num_users = len(client_nodes)
    print(f"ç½‘ç»œæ‹“æ‰‘åˆ†æå®Œæˆï¼šå®é™…å®¢æˆ·ç«¯æ•°é‡ä¸º {actual_num_users}")
    print(f"åŸå§‹å‚æ•°è®¾ç½®ï¼šargs.num_users = {args.num_users}")
    
    # æ›´æ–°å‚æ•°ä»¥åŒ¹é…å®é™…å®¢æˆ·ç«¯æ•°é‡
    args.num_users = actual_num_users
    print(f"å·²æ›´æ–°å‚æ•°ï¼šargs.num_users = {args.num_users}")

    # ç°åœ¨ä½¿ç”¨æ›´æ–°åçš„å‚æ•°ç”Ÿæˆæ•°æ®åˆ†é…
    dataset_train, dataset_test, dict_users, client_classes = get_data(args)
    
    # éªŒè¯æ•°æ®åˆ†é…çš„å®Œæ•´æ€§
    if not validate_data_distribution(dict_users, dataset_train, args):
        exit("æ•°æ®åˆ†é…éªŒè¯å¤±è´¥ï¼Œç¨‹åºé€€å‡º")

    # æ‰“å° FedRS é…ç½®ä¿¡æ¯
    if args.method == 'fedrs':
        print("\n" + "="*50)
        print("FedRS ç®—æ³•é…ç½®ä¿¡æ¯")
        print("="*50)
        print(f"è”é‚¦å­¦ä¹ æ–¹æ³•: {args.method}")
        print(f"FedRS Alpha å‚æ•°: {args.fedrs_alpha}")
        print(f"æœ€å°æœ¬åœ°è®­ç»ƒè½®æ¬¡: {args.min_le}")
        print(f"æœ€å¤§æœ¬åœ°è®­ç»ƒè½®æ¬¡: {args.max_le}")
        
        # ç»Ÿè®¡å®¢æˆ·ç«¯ç±»åˆ«åˆ†å¸ƒ
        class_counts = {}
        for client_id, classes in client_classes.items():
            num_classes = len(classes)
            class_counts[num_classes] = class_counts.get(num_classes, 0) + 1
        
        print("\nå®¢æˆ·ç«¯ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
        for num_classes, count in sorted(class_counts.items()):
            print(f"  æ‹¥æœ‰ {num_classes} ä¸ªç±»åˆ«çš„å®¢æˆ·ç«¯æ•°é‡: {count}")
        print("="*50 + "\n")
    else:
        print(f"\nä½¿ç”¨è”é‚¦å­¦ä¹ æ–¹æ³•: {args.method}\n")

    net_glob = build_model(args, dataset_train)
    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    print(f"\n=== æ•°æ®ä¸€è‡´æ€§éªŒè¯ ===")
    print(f"å®¢æˆ·ç«¯èŠ‚ç‚¹æ€»æ•°: {len(client_nodes)}")
    print(f"æ´»è·ƒè¾¹ç¼˜æœåŠ¡å™¨èŠ‚ç‚¹æ€»æ•°: {len(active_es_nodes)}")
    print(f"args.num_users: {args.num_users}")
    print(f"dict_usersé”®çš„æ•°é‡: {len(dict_users)}")
    print(f"dict_usersé”®çš„èŒƒå›´: {min(dict_users.keys())} - {max(dict_users.keys())}")
    print("=" * 25)

    net_glob.train()

    # åˆå§‹åŒ–å…¨å±€æƒé‡
    w_glob = net_glob.state_dict()
    num_users = len(client_nodes)
    num_ESs = len(active_es_nodes)
    k2 = args.ES_k2
    k3 = args.EH_k3
    num_processes = args.num_processes

    # # æ•°æ®ä¸€è‡´æ€§å·²åœ¨get_data()åé€šè¿‡validate_data_distribution()éªŒè¯å®Œæˆ
    # # æ­¤å¤„åªéœ€ç¡®è®¤ç½‘ç»œæ‹“æ‰‘ä¸æ•°æ®åˆ†é…çš„ä¸€è‡´æ€§
    # print(f"\n=== ç½‘ç»œæ‹“æ‰‘ä¸æ•°æ®åˆ†é…ä¸€è‡´æ€§ç¡®è®¤ ===")
    # if args.num_users != num_users:
    #     print(f"âŒ å®¢æˆ·ç«¯æ•°é‡ä¸åŒ¹é…: args.num_users={args.num_users}, actual_clients={num_users}")
    #     exit("ç½‘ç»œæ‹“æ‰‘åˆ†æåå®¢æˆ·ç«¯æ•°é‡å‘ç”Ÿå˜åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    # print(f"âœ… ç½‘ç»œæ‹“æ‰‘ä¸æ•°æ®åˆ†é…ä¸€è‡´æ€§ç¡®è®¤é€šè¿‡")
    print(f"   å®¢æˆ·ç«¯æ•°é‡: {args.num_users}")
    print(f"   è¾¹ç¼˜æœåŠ¡å™¨æ•°é‡: {num_ESs}")
    print("=" * 30)

    # A_random = get_A_random(num_users, num_ESs)

    # ä½¿ç”¨è°±èšç±»ç”ŸæˆBçŸ©é˜µï¼ˆæ›¿æ¢åŸæ¥çš„éšæœºBçŸ©é˜µï¼‰
    print("å¼€å§‹åˆå§‹è®­ç»ƒå’Œè°±èšç±»...")

    # 1. è®­ç»ƒåˆå§‹æœ¬åœ°æ¨¡å‹å¹¶èšåˆåˆ°ESå±‚ - éµå¾ªè”é‚¦å­¦ä¹ æœºåˆ¶
    w_locals, client_label_distributions = train_initial_models_with_es_aggregation(
        args, dataset_train, dict_users, net_glob, A_design, args.num_users
    )

    # 2. ä½¿ç”¨è°±èšç±»ç”ŸæˆBçŸ©é˜µï¼ˆw_localsç°åœ¨æ˜¯ESå±‚èšåˆç»“æœï¼‰
    B_cluster = get_B_cluster_from_es_models(
        args, w_locals, A_design, client_label_distributions
    )
    num_EHs = B_cluster.shape[1]
    
    # 3. åŒæ—¶ç”ŸæˆéšæœºBçŸ©é˜µç”¨äºå¯¹æ¯”
    B_random = get_B(num_ESs, num_EHs)
    B_hfl = np.ones((num_ESs, 1))

    # æ„å»ºä¸¤å¥—å±‚çº§ç»“æ„ï¼ˆç”¨äºè”é‚¦å­¦ä¹ èšåˆï¼‰
    C1_hfl, C2_hfl = build_hierarchy(A_design, B_hfl)
    C1_random, C2_random = build_hierarchy(A_design, B_random)
    C1_cluster, C2_cluster = build_hierarchy(A_design, B_cluster)

    # æ„å»ºé€šä¿¡å®é™…çš„å…³è”çŸ©é˜µï¼ˆç”¨äºé€šä¿¡å¼€é”€è®¡ç®—ï¼‰
    # ä¸¤ç§Aå…³è”çŸ©é˜µç›´æ¥å¯ç”¨ï¼ŒBå…³è”çŸ©é˜µä¸ºï¼ˆesï¼Œç°‡ï¼‰å½¢å¼ï¼Œéœ€è½¬åŒ–ä¸ºes-esï¼Œè¿˜éœ€ç”Ÿæˆes-cloud
    p_client = 20.0
    p_es = 50.0
    model_size = get_model_size_in_bits(w_glob)
    B_random_comm, C_random_comm = select_eh_random(B_random)
    B_cluster_comm, C_cluster_comm = select_eh(B_cluster, r_es, r_es_to_cloud, model_size)
    print("C1_random (ä¸€çº§->å®¢æˆ·ç«¯):", C1_random)
    print("C2_random (äºŒçº§->ä¸€çº§):", C2_random)
    print("C1_cluster (ä¸€çº§->å®¢æˆ·ç«¯):", C1_cluster)
    print("C2_cluster (äºŒçº§->ä¸€çº§):", C2_cluster)
    
    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®é‡
    client_data_counts = {}
    for client_id, data_indices in dict_users.items():
        if isinstance(data_indices, set):
            client_data_counts[client_id] = len(data_indices)
        elif isinstance(data_indices, np.ndarray):
            client_data_counts[client_id] = len(data_indices)
        else:
            client_data_counts[client_id] = len(list(data_indices))
    
    print(f"\n=== C1ã€C2å…³è”ç­–ç•¥ä¸‹ESã€EHæ•°æ®é‡ç»Ÿè®¡ ===")
    
    # è®¡ç®—HFLä¸¤å±‚ç»“æ„ä¸‹çš„æ•°æ®é‡åˆ†å¸ƒ
    print("\n--- HFLä¸¤å±‚ç»“æ„ (C1_hfl, C2_hfl) ---")
    es_data_counts_hfl = get_numlist_from_dict_users(C1_hfl, client_data_counts)
    eh_data_counts_hfl = get_numlist_from_dict_users(C2_hfl, es_data_counts_hfl)
    print(f"ESæ•°æ®é‡åˆ—è¡¨: {es_data_counts_hfl}")
    print(f"EHæ•°æ®é‡åˆ—è¡¨: {eh_data_counts_hfl}")
    print(f"ESå¹³å‡æ•°æ®é‡: {np.mean(es_data_counts_hfl):.1f}, æ ‡å‡†å·®: {np.std(es_data_counts_hfl):.1f}")
    print(f"EHå¹³å‡æ•°æ®é‡: {np.mean(eh_data_counts_hfl):.1f}, æ ‡å‡†å·®: {np.std(eh_data_counts_hfl):.1f}")
    
    # è®¡ç®—éšæœºBçŸ©é˜µä¸‹çš„æ•°æ®é‡åˆ†å¸ƒ
    print("\n--- éšæœºBçŸ©é˜µ (C1_random, C2_random) ---")
    es_data_counts_random = get_numlist_from_dict_users(C1_random, client_data_counts)
    eh_data_counts_random = get_numlist_from_dict_users(C2_random, es_data_counts_random)
    print(f"ESæ•°æ®é‡åˆ—è¡¨: {es_data_counts_random}")
    print(f"EHæ•°æ®é‡åˆ—è¡¨: {eh_data_counts_random}")
    print(f"ESå¹³å‡æ•°æ®é‡: {np.mean(es_data_counts_random):.1f}, æ ‡å‡†å·®: {np.std(es_data_counts_random):.1f}")
    print(f"EHå¹³å‡æ•°æ®é‡: {np.mean(eh_data_counts_random):.1f}, æ ‡å‡†å·®: {np.std(eh_data_counts_random):.1f}")
    
    # è®¡ç®—èšç±»BçŸ©é˜µä¸‹çš„æ•°æ®é‡åˆ†å¸ƒ
    print("\n--- èšç±»BçŸ©é˜µ (C1_cluster, C2_cluster) ---")
    es_data_counts_cluster = get_numlist_from_dict_users(C1_cluster, client_data_counts)
    eh_data_counts_cluster = get_numlist_from_dict_users(C2_cluster, es_data_counts_cluster)
    print(f"ESæ•°æ®é‡åˆ—è¡¨: {es_data_counts_cluster}")
    print(f"EHæ•°æ®é‡åˆ—è¡¨: {eh_data_counts_cluster}")
    print(f"ESå¹³å‡æ•°æ®é‡: {np.mean(es_data_counts_cluster):.1f}, æ ‡å‡†å·®: {np.std(es_data_counts_cluster):.1f}")
    print(f"EHå¹³å‡æ•°æ®é‡: {np.mean(eh_data_counts_cluster):.1f}, æ ‡å‡†å·®: {np.std(eh_data_counts_cluster):.1f}")
    
    # print(f"\n=== åŠ æƒå¹³å‡èšåˆé…ç½® ===")
    # print(f"âœ… è”é‚¦å­¦ä¹ èšåˆå°†ä½¿ç”¨åŸºäºæ•°æ®é‡çš„åŠ æƒå¹³å‡")
    # print(f"ğŸ“Š å®¢æˆ·ç«¯æ€»æ•°æ®é‡: {sum(client_data_counts.values())}")
    # print(f"ğŸ“Š å®¢æˆ·ç«¯æ•°æ®é‡åˆ†å¸ƒ: æœ€å°={min(client_data_counts.values())}, æœ€å¤§={max(client_data_counts.values())}, å¹³å‡={np.mean(list(client_data_counts.values())):.1f}")
    # print("=" * 30)
    
    # # æ•°æ®é‡åˆ†å¸ƒå¯¹æ¯”åˆ†æ
    # print(f"\n--- æ•°æ®é‡åˆ†å¸ƒå¯¹æ¯”åˆ†æ ---")
    # print(f"æ€»å®¢æˆ·ç«¯æ•°æ®é‡: {sum(client_data_counts.values())}")
    # print(f"å®¢æˆ·ç«¯æ•°æ®é‡èŒƒå›´: [{min(client_data_counts.values())}, {max(client_data_counts.values())}]")
    # print(f"ESæ•°æ®é‡åˆ†å¸ƒ - HFL: èŒƒå›´[{min(es_data_counts_hfl)}, {max(es_data_counts_hfl)}], å˜å¼‚ç³»æ•°: {np.std(es_data_counts_hfl)/np.mean(es_data_counts_hfl):.3f}")
    # print(f"ESæ•°æ®é‡åˆ†å¸ƒ - éšæœº: èŒƒå›´[{min(es_data_counts_random)}, {max(es_data_counts_random)}], å˜å¼‚ç³»æ•°: {np.std(es_data_counts_random)/np.mean(es_data_counts_random):.3f}")
    # print(f"ESæ•°æ®é‡åˆ†å¸ƒ - èšç±»: èŒƒå›´[{min(es_data_counts_cluster)}, {max(es_data_counts_cluster)}], å˜å¼‚ç³»æ•°: {np.std(es_data_counts_cluster)/np.mean(es_data_counts_cluster):.3f}")
    # print(f"EHæ•°æ®é‡åˆ†å¸ƒ - HFL: èŒƒå›´[{min(eh_data_counts_hfl)}, {max(eh_data_counts_hfl)}], å˜å¼‚ç³»æ•°: {np.std(eh_data_counts_hfl)/np.mean(eh_data_counts_hfl):.3f}")
    # print(f"EHæ•°æ®é‡åˆ†å¸ƒ - éšæœº: èŒƒå›´[{min(eh_data_counts_random)}, {max(eh_data_counts_random)}], å˜å¼‚ç³»æ•°: {np.std(eh_data_counts_random)/np.mean(eh_data_counts_random):.3f}")
    # print(f"EHæ•°æ®é‡åˆ†å¸ƒ - èšç±»: èŒƒå›´[{min(eh_data_counts_cluster)}, {max(eh_data_counts_cluster)}], å˜å¼‚ç³»æ•°: {np.std(eh_data_counts_cluster)/np.mean(eh_data_counts_cluster):.3f}")
    # print("=" * 50)
    
    print("t_client_to_es_random")
    t_client_to_es_random, p_client_to_es_random = calculate_transmission_time(model_size, r_client_to_es, A_design, p_client)
    t_client_to_es_design, p_client_to_es_design = calculate_transmission_time(model_size, r_client_to_es, A_design, p_client)
    t_client_to_es_favg, p_client_to_es_favg = calculate_transmission_time(model_size, r_client_to_es, A_design, p_client)
    t_es_to_eh_random, p_es_to_eh_random = calculate_transmission_time(model_size, r_es, B_random_comm, p_es)
    t_es_to_eh_design, p_es_to_eh_design = calculate_transmission_time(model_size, r_es, B_cluster_comm, p_es)
    t_es_to_cloud_favg, p_es_to_cloud_favg = calculate_transmission_time(model_size, r_es_to_cloud, B_hfl, p_es)
    t_eh_to_cloud_random, p_eh_to_cloud_random = calculate_transmission_time(model_size, r_es_to_cloud, C_random_comm, p_es)
    t_eh_to_cloud_design, p_eh_to_cloud_design = calculate_transmission_time(model_size, r_es_to_cloud, C_cluster_comm, p_es)
    #t_client_to_cloud_sfl, p_client_to_cloud_sfl = calculate_transmission_time(model_size, r_client_to_cloud, np.ones((num_users, 1), dtype=int), p_client)
    print(f"random:{t_client_to_es_random}, {t_es_to_eh_random}, {t_eh_to_cloud_random}")
    print(f"design:{t_client_to_es_design}, {t_es_to_eh_design}, {t_eh_to_cloud_design}")
    print(f"sfl:{t_client_to_es_favg}, {t_es_to_cloud_favg} ")
    print(f"random:{p_client_to_es_random}, {p_es_to_eh_random}, {p_eh_to_cloud_random}")
    print(f"design:{p_client_to_es_design}, {p_es_to_eh_design}, {p_eh_to_cloud_design}")
    print(f"sfl:{p_client_to_es_favg}, {p_es_to_cloud_favg} ")
    t_hfl_random_sig = t_client_to_es_random * k2 + t_es_to_eh_random * k3 + t_eh_to_cloud_random
    t_hfl_design_sig = t_client_to_es_design * k2 + t_es_to_eh_design * k3 + t_eh_to_cloud_design
    t_favg_sig = t_client_to_es_favg * k2 + t_es_to_cloud_favg * k3
    p_hfl_random_sig = p_client_to_es_random * k2 + p_es_to_eh_random * k3 + p_eh_to_cloud_random
    p_hfl_design_sig = p_client_to_es_design * k2 + p_es_to_eh_design * k3 + p_eh_to_cloud_design
    p_favg_sig = p_client_to_es_favg * k2 + p_es_to_cloud_favg * k3
    print(f"hfl_random é¢„è®¡å•è½®é€šä¿¡æ—¶é—´: {t_hfl_random_sig:.6f}s")
    print(f"hfl_design é¢„è®¡å•è½®é€šä¿¡æ—¶é—´: {t_hfl_design_sig:.6f}s")
    print(f"sfl é¢„è®¡å•è½®é€šä¿¡æ—¶é—´: {t_favg_sig:.6f}s")
    print(f"hfl_random é¢„è®¡å•è½®é€šä¿¡èƒ½è€—: {p_hfl_random_sig:.6f}J")
    print(f"hfl_design é¢„è®¡å•è½®é€šä¿¡èƒ½è€—: {p_hfl_design_sig:.6f}J")
    print(f"sfl é¢„è®¡å•è½®é€šä¿¡èƒ½è€—: {p_favg_sig:.6f}J")
    
    # ä¿å­˜é€šä¿¡æ—¶é—´å’Œèƒ½è€—ç»“æœåˆ°CSV
    save_communication_results_to_csv(
        network_scale=num_users,
        hfl_cluster_time=t_hfl_design_sig,
        hfl_random_time=t_hfl_random_sig, 
        sfl_time=t_favg_sig,
        hfl_cluster_power=p_hfl_design_sig,
        hfl_random_power=p_hfl_random_sig,
        sfl_power=p_favg_sig,
        dataset=args.dataset,
        model=args.model,
        lr=args.lr
    )
    # ç”ŸæˆEHä¸“å±æµ‹è¯•é›†
    print("\n--- ç”ŸæˆEHä¸“å±æµ‹è¯•é›† ---")
    print("é‡‡ç”¨æ”¹è¿›çš„èµ„æºåˆ†é…ç­–ç•¥ï¼šå…è®¸æµ‹è¯•æ ·æœ¬åœ¨å¤šä¸ªEHæµ‹è¯•é›†ä¸­é‡å¤å‡ºç°")
    print("è¿™ç¡®ä¿æ¯ä¸ªEHéƒ½èƒ½è·å¾—ä¸å…¶ä¸‹æ¸¸å®¢æˆ·ç«¯åˆ†å¸ƒåŒ¹é…çš„ä¸ªæ€§åŒ–æµ‹è¯•é›†")
    
    # ä¸ºéšæœºBçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†
    print("\nğŸ² ä¸ºéšæœºBçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†...")
    eh_testsets_random, eh_label_distributions_random = EHTestsetGenerator.create_eh_testsets(
        dataset_test, A_design, B_random, C1_random, C2_random, dataset_train, dict_users, visualize=True
    )
    
    # ä¸ºèšç±»BçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†
    print("\nğŸ§© ä¸ºèšç±»BçŸ©é˜µç”ŸæˆEHä¸“å±æµ‹è¯•é›†...")
    eh_testsets_cluster, eh_label_distributions_cluster = EHTestsetGenerator.create_eh_testsets(
        dataset_test, A_design, B_cluster, C1_cluster, C2_cluster, dataset_train, dict_users, visualize=True
    )
    
    # print(f"\nâœ… æµ‹è¯•é›†ç”Ÿæˆå®Œæˆ!")
    # print(f"éšæœºBçŸ©é˜µ: å·²ç”Ÿæˆ {len(eh_testsets_random)} ä¸ªEHä¸“å±æµ‹è¯•é›†")
    # print(f"èšç±»BçŸ©é˜µ: å·²ç”Ÿæˆ {len(eh_testsets_cluster)} ä¸ªEHä¸“å±æµ‹è¯•é›†")
    
    # # æ‰“å°æ¯ä¸ªEHæµ‹è¯•é›†çš„è¯¦ç»†ä¿¡æ¯
    # print(f"\nğŸ“Š éšæœºBçŸ©é˜µ - EHæµ‹è¯•é›†ç»Ÿè®¡:")
    # for eh_idx, testset in eh_testsets_random.items():
    #     unique_samples = len(np.unique(testset))
    #     total_samples = len(testset)
    #     print(f"  EH {eh_idx}: æ€»æ ·æœ¬={total_samples}, å”¯ä¸€æ ·æœ¬={unique_samples}, é‡å¤ç‡={1-unique_samples/total_samples:.1%}")
    
    # print(f"\nğŸ“Š èšç±»BçŸ©é˜µ - EHæµ‹è¯•é›†ç»Ÿè®¡:")
    # for eh_idx, testset in eh_testsets_cluster.items():
    #     unique_samples = len(np.unique(testset))
    #     total_samples = len(testset)
    #     print(f"  EH {eh_idx}: æ€»æ ·æœ¬={total_samples}, å”¯ä¸€æ ·æœ¬={unique_samples}, é‡å¤ç‡={1-unique_samples/total_samples:.1%}")

    # æ‰“å°FedRSé…ç½®ä¿¡æ¯
    print(f"\n--- FedRS Configuration ---")
    print(f"Method: {args.method}")
    if args.method == 'fedrs':
        print(f"FedRS Alpha: {args.fedrs_alpha}")
        print(f"Min Local Epochs: {args.min_le}")
        print(f"Max Local Epochs: {args.max_le}")
        print("FedRS Restricted Softmax: Enabled")
        # æ‰“å°å‰å‡ ä¸ªå®¢æˆ·ç«¯çš„ç±»åˆ«ä¿¡æ¯
        print("Sample Client Class Distributions:")
        for i in range(min(5, len(client_classes))):
            print(f"  Client {i}: Classes {client_classes[i]}")
    else:
        print("FedRS: Disabled (using standard FedAvg)")
    print("-----------------------------\n")

    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    if not os.path.exists('./results'):
        os.makedirs('./results')

    # ç”Ÿæˆå”¯ä¸€çš„æ—¶é—´æˆ³ç”¨äºæ–‡ä»¶åï¼ŒåŒ…å«é‡è¦å‚æ•°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    param_str = f"e{args.epochs}_u{args.num_users}_le{args.local_ep}_{args.dataset}_{args.model}_k2{args.ES_k2}_k3{args.EH_k3}_p{args.num_processes}_lr{args.lr}"
    if not args.iid:
        param_str += f"_beta{args.beta}"
    csv_filename = f'./results/training_results_{param_str}_{timestamp}.csv'

    # åˆå§‹åŒ–ç»“æœè®°å½•åˆ—è¡¨ - æ–°æ ¼å¼æ”¯æŒä¸‰ç§æ¨¡å‹
    results_history = []

    # è®­ç»ƒå‰æµ‹è¯•åˆå§‹æ¨¡å‹
    print("\n--- Testing Initial Global Models ---")
    net_glob.eval()

    # æµ‹è¯•åˆå§‹æ¨¡å‹
    acc_init, loss_init = test_img(net_glob, dataset_test, args)
    print(f"Initial Model - Testing accuracy: {acc_init:.2f}%, Loss: {loss_init:.4f}")

    # è®°å½•åˆå§‹ç»“æœ - ä¸‰ç§æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„åˆå§‹æƒé‡
    for model_name in ['HFL_Random_B', 'HFL_Cluster_B', 'HFL']:
        results_history.append({
            'epoch': -1,
            'eh_round': 0,
            'es_round': 0,
            'train_loss': 0.0,  # åˆå§‹è®­ç»ƒæŸå¤±æš‚æ—¶è®¾ä¸º0
            'test_loss': loss_init,
            'test_acc': acc_init,
            'model_type': model_name,
            'level': 'Global',
            'eh_idx': -1
        })

    # ä¿å­˜åˆå§‹ç»“æœåˆ°CSV
    save_results_to_csv(results_history, csv_filename)

    # training
    # --- åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡å‹ï¼Œç¡®ä¿åˆå§‹æƒé‡ç›¸åŒ ---
    # net_glob_hfl_random æ˜¯ä½¿ç”¨éšæœºBçŸ©é˜µçš„ HFL å…¨å±€æ¨¡å‹
    net_glob_hfl_random = copy.deepcopy(net_glob)
    w_glob_hfl_random = net_glob_hfl_random.state_dict()

    # net_glob_hfl_cluster æ˜¯ä½¿ç”¨èšç±»BçŸ©é˜µçš„ HFL å…¨å±€æ¨¡å‹
    net_glob_hfl_cluster = copy.deepcopy(net_glob)
    w_glob_hfl_cluster = net_glob_hfl_cluster.state_dict()

    # net_glob_hfl æ˜¯ HFL ä¸¤å±‚ç»“æ„çš„å…¨å±€æ¨¡å‹
    net_glob_hfl = copy.deepcopy(net_glob)
    w_glob_hfl = net_glob_hfl.state_dict()

    # --- åˆ†åˆ«è®°å½•ä¸‰ç§æ¨¡å‹çš„æŒ‡æ ‡ ---
    loss_train_hfl_random = []
    loss_train_hfl_cluster = []
    loss_train_hfl = []
    loss_test_hfl_random = []
    loss_test_hfl_cluster = []
    loss_test_hfl = []
    acc_test_hfl_random = []
    acc_test_hfl_cluster = []
    acc_test_hfl = []
    t_hfl_random = 0
    t_hfl_design = 0
    t_hfl = 0

    # è®°å½•å®é™…è¿è¡Œçš„epochæ•°
    final_epoch = args.epochs

    # --- åˆå§‹åŒ–æ”¶æ•›æ£€æŸ¥å™¨ ---
    print("\n=== åˆå§‹åŒ–æ”¶æ•›æ£€æŸ¥å™¨ ===")
    print(f"æ”¶æ•›å‚æ•°è®¾ç½®: æŸå¤±é˜ˆå€¼={args.loss_threshold}, å‡†ç¡®ç‡é˜ˆå€¼={args.acc_threshold}%, è€å¿ƒå€¼={args.convergence_patience}")
    
    # ä¸ºæ¯ä¸ªEHåˆ›å»ºæ”¶æ•›æ£€æŸ¥å™¨ - HFLéšæœºBçŸ©é˜µ
    eh_checkers_random = {}
    for eh_idx in range(num_EHs):
        eh_checkers_random[eh_idx] = ConvergenceChecker(
            patience=args.convergence_patience, 
            loss_threshold=args.loss_threshold, 
            acc_threshold=args.acc_threshold
        )
    
    # ä¸ºæ¯ä¸ªEHåˆ›å»ºæ”¶æ•›æ£€æŸ¥å™¨ - HFLèšç±»BçŸ©é˜µ  
    eh_checkers_cluster = {}
    for eh_idx in range(num_EHs):
        eh_checkers_cluster[eh_idx] = ConvergenceChecker(
            patience=args.convergence_patience, 
            loss_threshold=args.loss_threshold, 
            acc_threshold=args.acc_threshold
        )
    
    # ä¸ºHFLä¸¤å±‚ç»“æ„åˆ›å»ºæ”¶æ•›æ£€æŸ¥å™¨
    hfl_checker = ConvergenceChecker(
        patience=args.convergence_patience, 
        loss_threshold=args.loss_threshold, 
        acc_threshold=args.acc_threshold
    )

    # è®°å½•å„æœºåˆ¶çš„æ”¶æ•›çŠ¶æ€
    converged_hfl_random = False
    converged_hfl_cluster = False
    converged_hfl = False
    
    print(f"å·²ä¸ºHFLéšæœºBçŸ©é˜µåˆ›å»º {len(eh_checkers_random)} ä¸ªEHæ”¶æ•›æ£€æŸ¥å™¨")
    print(f"å·²ä¸ºHFLèšç±»BçŸ©é˜µåˆ›å»º {len(eh_checkers_cluster)} ä¸ªEHæ”¶æ•›æ£€æŸ¥å™¨")
    print(f"å·²ä¸ºHFLä¸¤å±‚ç»“æ„åˆ›å»ºå…¨å±€æ”¶æ•›æ£€æŸ¥å™¨")
    print("=" * 30)

    for epoch in range(args.epochs):
        # HFL éšæœºBçŸ©é˜µæ¨¡å‹æƒé‡åˆ†å‘ (Cloud -> EH)
        EHs_ws_hfl_random = [copy.deepcopy(w_glob_hfl_random) for _ in range(num_EHs)]
        
        # HFL èšç±»BçŸ©é˜µæ¨¡å‹æƒé‡åˆ†å‘ (Cloud -> EH)
        EHs_ws_hfl_cluster = [copy.deepcopy(w_glob_hfl_cluster) for _ in range(num_EHs)]

        # EH å±‚èšåˆ k3 è½®
        for t3 in range(k3):
            # HFLéšæœº: EH å±‚ -> ES å±‚
            ESs_ws_input_hfl_random = [None] * num_ESs
            for EH_idx, ES_indices in C2_random.items():
                for ES_idx in ES_indices:
                    ESs_ws_input_hfl_random[ES_idx] = copy.deepcopy(EHs_ws_hfl_random[EH_idx])
            
            # HFLèšç±»: EH å±‚ -> ES å±‚
            ESs_ws_input_hfl_cluster = [None] * num_ESs
            for EH_idx, ES_indices in C2_cluster.items():
                for ES_idx in ES_indices:
                    ESs_ws_input_hfl_cluster[ES_idx] = copy.deepcopy(EHs_ws_hfl_cluster[EH_idx])
            
            # HFLä¸¤å±‚ç»“æ„: Cloud ç›´æ¥ -> ES å±‚ï¼ˆè·³è¿‡EHå±‚ï¼‰
            ESs_ws_input_hfl = [copy.deepcopy(w_glob_hfl) for _ in range(num_ESs)]

            # ES å±‚èšåˆ k2 è½®
            for t2 in range(k2):
                # --- æœ¬åœ°è®­ç»ƒ (ä¸‰ç§æ¨¡å‹å¹¶è¡Œ) ---
                # HFLéšæœº: ES å±‚ -> Client å±‚
                w_locals_input_hfl_random = [None] * num_users
                for ES_idx, user_indices in C1_random.items():
                    for user_idx in user_indices:
                        w_locals_input_hfl_random[user_idx] = copy.deepcopy(ESs_ws_input_hfl_random[ES_idx])
                
                # HFLèšç±»: ES å±‚ -> Client å±‚
                w_locals_input_hfl_cluster = [None] * num_users
                for ES_idx, user_indices in C1_cluster.items():
                    for user_idx in user_indices:
                        w_locals_input_hfl_cluster[user_idx] = copy.deepcopy(ESs_ws_input_hfl_cluster[ES_idx])
                
                # HFLä¸¤å±‚ç»“æ„: ES å±‚ -> Client å±‚
                w_locals_input_hfl = [None] * num_users
                for ES_idx, user_indices in C1_hfl.items():
                    for user_idx in user_indices:
                        w_locals_input_hfl[user_idx] = copy.deepcopy(ESs_ws_input_hfl[ES_idx])

                # ç”¨äºå­˜å‚¨ä¸‰ç§æ¨¡å‹æœ¬åœ°è®­ç»ƒçš„è¾“å‡º
                w_locals_output_hfl_random = [None] * num_users
                w_locals_output_hfl_cluster = [None] * num_users
                w_locals_output_hfl = [None] * num_users
                loss_locals_hfl_random = []
                loss_locals_hfl_cluster = []
                loss_locals_hfl = []

                # æ˜¾ç¤ºè®­ç»ƒçŠ¶æ€ä¿¡æ¯
                active_models = []
                if not converged_hfl_random:
                    active_models.append("HFL_Random")
                if not converged_hfl_cluster:
                    active_models.append("HFL_Cluster")
                if not converged_hfl:
                    active_models.append("HFL")
                
                if not active_models:
                    print(f"\n[Skip Training] æ‰€æœ‰æ¨¡å‹å·²æ”¶æ•›ï¼Œè·³è¿‡å®¢æˆ·ç«¯è®­ç»ƒ")
                    # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½æ”¶æ•›äº†ï¼Œè·³è¿‡è®­ç»ƒä½†ä»éœ€è¦è¿”å›ç»“æœ
                    results = []
                    for user_idx in range(num_users):
                        results.append((user_idx, 
                                      w_locals_input_hfl_random[user_idx], 0.0,
                                      w_locals_input_hfl_cluster[user_idx], 0.0,
                                      w_locals_input_hfl[user_idx], 0.0))
                else:
                    print(f"\n[Parallel Training] ä¸º {len(active_models)} ç§æ´»è·ƒæ¨¡å‹è®­ç»ƒ {args.num_users} ä¸ªå®¢æˆ·ç«¯")
                    print(f"æ´»è·ƒæ¨¡å‹: {', '.join(active_models)}")
                    print(f"ä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹å¹¶è¡Œè®­ç»ƒ...")

                    # å‡†å¤‡ä¼ é€’ç»™æ¯ä¸ªå­è¿›ç¨‹çš„å‚æ•°
                    tasks = []
                    for user_idx in range(num_users):
                        task_args = (
                            args, user_idx, dataset_train, dict_users,
                            w_locals_input_hfl_random[user_idx], w_locals_input_hfl_cluster[user_idx], 
                            w_locals_input_hfl[user_idx], client_classes,
                            not converged_hfl_random,  # train_hfl_random
                            not converged_hfl_cluster,  # train_hfl_cluster  
                            not converged_hfl  # train_hfl
                        )
                        tasks.append(task_args)
                    print("æˆåŠŸåˆ›å»ºå¤šçº¿ç¨‹ï¼")

                    # åˆ›å»ºè¿›ç¨‹æ± å¹¶åˆ†å‘ä»»åŠ¡
                    # ä½¿ç”¨ with è¯­å¥å¯ä»¥è‡ªåŠ¨ç®¡ç†è¿›ç¨‹æ± çš„å…³é—­
                    with mp.Pool(processes=num_processes) as pool:
                        results = pool.starmap(train_client, tqdm(tasks, desc=f"Epoch {epoch}|{t3 + 1}|{t2 + 1} Training Clients"))

                print("è®­ç»ƒç»“æŸ")
                # æ”¶é›†å¹¶æ•´ç†æ‰€æœ‰å®¢æˆ·ç«¯çš„è®­ç»ƒç»“æœ
                for result in results:
                    u_idx, w_hr, l_hr, w_hc, l_hc, w_h, l_h = result
                    w_locals_output_hfl_random[u_idx] = w_hr
                    loss_locals_hfl_random.append(l_hr)
                    w_locals_output_hfl_cluster[u_idx] = w_hc
                    loss_locals_hfl_cluster.append(l_hc)
                    w_locals_output_hfl[u_idx] = w_h
                    loss_locals_hfl.append(l_h)
                
                print("æ’åºç»“æŸ")
                if active_models:
                    print(f"[Parallel Training] æ‰€æœ‰ {args.num_users} ä¸ªå®¢æˆ·ç«¯å·²å®Œæˆ {len(active_models)} ç§æ¨¡å‹çš„è®­ç»ƒ")
                    print(f"è®­ç»ƒçš„æ¨¡å‹: {', '.join(active_models)}")
                else:
                    print(f"[Skip Training] æ‰€æœ‰æ¨¡å‹å·²æ”¶æ•›ï¼Œæœªè¿›è¡Œå®é™…è®­ç»ƒ")
                print(f'\nEpoch {epoch} | EH_R {t3 + 1}/{k3} | ES_R {t2 + 1}/{k2} | å¼€å§‹èšåˆ')

                # --- HFL èšåˆ (Client -> ES) - åªå¯¹æœªæ”¶æ•›çš„æœºåˆ¶è¿›è¡Œèšåˆ ---
                if not converged_hfl_random:
                    #print(f"  ğŸ“Š [Client->ES] HFLéšæœº: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(client_data_counts)}ä¸ªå®¢æˆ·ç«¯æ•°æ®é‡)")
                    ESs_ws_input_hfl_random = FedAvg_layered(w_locals_output_hfl_random, C1_random, client_data_counts)
                    t_hfl_random += t_client_to_es_random
                else:
                    print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡ESå±‚èšåˆ")
                
                if not converged_hfl_cluster:
                    #print(f"  ğŸ“Š [Client->ES] HFLèšç±»: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(client_data_counts)}ä¸ªå®¢æˆ·ç«¯æ•°æ®é‡)")
                    ESs_ws_input_hfl_cluster = FedAvg_layered(w_locals_output_hfl_cluster, C1_cluster, client_data_counts)
                    t_hfl_design += t_client_to_es_design
                else:
                    print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡ESå±‚èšåˆ")
                
                # --- HFLä¸¤å±‚ç»“æ„èšåˆ (Client -> ES) - ä¸å…¶ä»–æœºåˆ¶åŒæ­¥è¿›è¡ŒESå±‚èšåˆ ---
                if not converged_hfl:
                    #print(f"  ğŸ“Š [Client->ES] HFLä¸¤å±‚: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(client_data_counts)}ä¸ªå®¢æˆ·ç«¯æ•°æ®é‡)")
                    ESs_ws_hfl = FedAvg_layered(w_locals_output_hfl, C1_hfl, client_data_counts)
                    t_hfl += t_client_to_es_favg
                else:
                    print(f"  [Skip] HFLä¸¤å±‚ç»“æ„å·²æ”¶æ•›ï¼Œè·³è¿‡ESå±‚èšåˆ")



                # --- è®°å½•æŸå¤± ---
                # åªä¸ºå®é™…è®­ç»ƒçš„æ¨¡å‹è®¡ç®—å¹³å‡æŸå¤±ï¼Œå·²æ”¶æ•›çš„æ¨¡å‹æŸå¤±ä¸º0
                if not converged_hfl_random:
                    loss_avg_hfl_random = sum(loss_locals_hfl_random) / len(loss_locals_hfl_random) if loss_locals_hfl_random else 0.0
                else:
                    loss_avg_hfl_random = 0.0  # å·²æ”¶æ•›ï¼ŒæŸå¤±ä¸º0
                    
                if not converged_hfl_cluster:
                    loss_avg_hfl_cluster = sum(loss_locals_hfl_cluster) / len(loss_locals_hfl_cluster) if loss_locals_hfl_cluster else 0.0
                else:
                    loss_avg_hfl_cluster = 0.0  # å·²æ”¶æ•›ï¼ŒæŸå¤±ä¸º0
                    
                if not converged_hfl:
                    loss_avg_hfl = sum(loss_locals_hfl) / len(loss_locals_hfl) if loss_locals_hfl else 0.0
                else:
                    loss_avg_hfl = 0.0  # å·²æ”¶æ•›ï¼ŒæŸå¤±ä¸º0
                
                loss_train_hfl_random.append(loss_avg_hfl_random)
                loss_train_hfl_cluster.append(loss_avg_hfl_cluster)
                loss_train_hfl.append(loss_avg_hfl)

                # æ˜¾ç¤ºæŸå¤±ä¿¡æ¯ï¼ŒåŒºåˆ†è®­ç»ƒå’Œæ”¶æ•›çŠ¶æ€
                print(f'\nEpoch {epoch} | EH_R {t3 + 1}/{k3} | ES_R {t2 + 1}/{k2}')
                loss_info = []
                if not converged_hfl_random:
                    loss_info.append(f'HFL_Random Loss: {loss_avg_hfl_random:.4f}')
                else:
                    loss_info.append(f'HFL_Random: å·²æ”¶æ•› âœ…')
                    
                if not converged_hfl_cluster:
                    loss_info.append(f'HFL_Cluster Loss: {loss_avg_hfl_cluster:.4f}')
                else:
                    loss_info.append(f'HFL_Cluster: å·²æ”¶æ•› âœ…')
                    
                if not converged_hfl:
                    loss_info.append(f'HFL Loss: {loss_avg_hfl:.4f}')
                else:
                    loss_info.append(f'HFL: å·²æ”¶æ•› âœ…')
                    
                print(' | '.join(loss_info))

            # HFL èšåˆ (ES -> EH) - åªå¯¹æœªæ”¶æ•›çš„æœºåˆ¶è¿›è¡Œèšåˆ
            if not converged_hfl_random:
                # å°†ESæ•°æ®é‡åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                es_data_weights_random = {i: es_data_counts_random[i] for i in range(len(es_data_counts_random))}
                #print(f"    ğŸ“Š [ES->EH] HFLéšæœº: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(es_data_weights_random)}ä¸ªESæ•°æ®é‡)")
                EHs_ws_hfl_random = FedAvg_layered(ESs_ws_input_hfl_random, C2_random, es_data_weights_random)
                t_hfl_random += t_es_to_eh_random
            else:
                print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHå±‚èšåˆ")
            
            if not converged_hfl_cluster:
                # å°†ESæ•°æ®é‡åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                es_data_weights_cluster = {i: es_data_counts_cluster[i] for i in range(len(es_data_counts_cluster))}
                #print(f"    ğŸ“Š [ES->EH] HFLèšç±»: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(es_data_weights_cluster)}ä¸ªESæ•°æ®é‡)")
                EHs_ws_hfl_cluster = FedAvg_layered(ESs_ws_input_hfl_cluster, C2_cluster, es_data_weights_cluster)
                t_hfl_design += t_es_to_eh_design
            else:
                print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHå±‚èšåˆ")
            
            # --- HFLä¸¤å±‚ç»“æ„å…¨å±€èšåˆ (ES -> Cloud) - åœ¨EHèšåˆæ—¶æœºè¿›è¡ŒESåˆ°Cloudçš„ä¸Šä¼  ---
            if not converged_hfl:
                # HFLä¸¤å±‚ç»“æ„ï¼šESèšåˆç»“æœç›´æ¥ä¸Šä¼ åˆ°Cloudï¼ˆè·³è¿‡EHå±‚ï¼‰
                # print(f"    ğŸ“Š [ES->Cloud] HFLä¸¤å±‚: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(es_data_counts_hfl)}ä¸ªESæ•°æ®é‡)")
                w_glob_hfl = FedAvg(ESs_ws_hfl, es_data_counts_hfl)
                net_glob_hfl.load_state_dict(w_glob_hfl)
                t_hfl += t_es_to_cloud_favg
            else:
                print(f"  [Skip] HFLä¸¤å±‚ç»“æ„å·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€èšåˆ")
            

            
            # --- åœ¨æ¯æ¬¡EHèšåˆåæµ‹è¯•EHæ¨¡å‹åœ¨ä¸“å±æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ ---
            print(f"\n[EH Testing] Epoch {epoch} | EH_Round {t3+1}/{k3} - æµ‹è¯•EHæ¨¡å‹æ€§èƒ½...")
            
            # åˆ›å»ºä¸´æ—¶æ¨¡å‹å¯¹è±¡æ¥åŠ è½½EHæƒé‡å¹¶è¿›è¡Œæµ‹è¯•
            eh_results_random = []
            eh_results_cluster = []
            
            # æµ‹è¯•æ¯ä¸ªEHçš„æ¨¡å‹æ€§èƒ½ï¼ˆåœ¨éšæœºBçŸ©é˜µæƒ…å†µä¸‹ï¼‰- åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶
            if not converged_hfl_random:
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_random):
                    if eh_weights is not None and eh_idx in eh_testsets_random:
                        # åˆ›å»ºä¸´æ—¶æ¨¡å‹å¹¶åŠ è½½æƒé‡
                        temp_model = build_model(args, dataset_train)
                        temp_model.load_state_dict(eh_weights)
                        temp_model.eval()
                        
                        # åœ¨EHä¸“å±æµ‹è¯•é›†ä¸Šæµ‹è¯•æ¨¡å‹
                        eh_acc, eh_loss = test_eh_model(temp_model, dataset_test, eh_testsets_random[eh_idx], args)
                        
                        # è®°å½•ç»“æœ
                        eh_results_random.append({
                            'epoch': epoch,
                            'eh_round': t3 + 1,
                            'es_round': k2,  # ESè½®æ¬¡å·²ç»“æŸ
                            'train_loss': 0.0,  # EHçº§åˆ«æ²¡æœ‰è®­ç»ƒæŸå¤±
                            'test_loss': eh_loss,
                            'test_acc': eh_acc,
                            'model_type': 'HFL_Random_B',
                            'level': 'EH',
                            'eh_idx': eh_idx
                        })
                        
                        print(f"  [Random] EH {eh_idx}: Acc {eh_acc:.2f}%, Loss {eh_loss:.4f}")
            else:
                print("  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHæ¨¡å‹æµ‹è¯•")
            
            # æµ‹è¯•æ¯ä¸ªEHçš„æ¨¡å‹æ€§èƒ½ï¼ˆåœ¨èšç±»BçŸ©é˜µæƒ…å†µä¸‹ï¼‰- åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶
            if not converged_hfl_cluster:
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_cluster):
                    if eh_weights is not None and eh_idx in eh_testsets_cluster:
                        # åˆ›å»ºä¸´æ—¶æ¨¡å‹å¹¶åŠ è½½æƒé‡
                        temp_model = build_model(args, dataset_train)
                        temp_model.load_state_dict(eh_weights)
                        temp_model.eval()
                        
                        # åœ¨EHä¸“å±æµ‹è¯•é›†ä¸Šæµ‹è¯•æ¨¡å‹
                        eh_acc, eh_loss = test_eh_model(temp_model, dataset_test, eh_testsets_cluster[eh_idx], args)
                        
                        # è®°å½•ç»“æœ
                        eh_results_cluster.append({
                            'epoch': epoch,
                            'eh_round': t3 + 1,
                            'es_round': k2,  # ESè½®æ¬¡å·²ç»“æŸ
                            'train_loss': 0.0,  # EHçº§åˆ«æ²¡æœ‰è®­ç»ƒæŸå¤±
                            'test_loss': eh_loss,
                            'test_acc': eh_acc,
                            'model_type': 'HFL_Cluster_B',
                            'level': 'EH',
                            'eh_idx': eh_idx
                        })
                        
                        print(f"  [Cluster] EH {eh_idx}: Acc {eh_acc:.2f}%, Loss {eh_loss:.4f}")
            else:
                print("  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡EHæ¨¡å‹æµ‹è¯•")
            
            # æµ‹è¯•HFLä¸¤å±‚ç»“æ„å…¨å±€æ¨¡å‹ï¼ˆåœ¨å…¨å±€æµ‹è¯•é›†ä¸Šï¼‰- åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶
            if not converged_hfl:
                net_glob_hfl.eval()
                acc_hfl, loss_hfl = test_img(net_glob_hfl, dataset_test, args)
            else:
                print("  [Skip] HFLä¸¤å±‚ç»“æ„å·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€æ¨¡å‹æµ‹è¯•")
                # ä½¿ç”¨ä¸Šä¸€è½®çš„ç»“æœä½œä¸ºå ä½ç¬¦
                acc_hfl, loss_hfl = acc_test_hfl[-1] if acc_test_hfl else 0.0, loss_test_hfl[-1] if loss_test_hfl else 0.0
            
            # è®°å½•HFLæ¨¡å‹ç»“æœ
            hfl_result = {
                'epoch': epoch,
                'eh_round': t3 + 1,
                'es_round': k2,
                'train_loss': 0.0,  # ä½¿ç”¨0.0ä½œä¸ºå ä½ç¬¦
                'test_loss': loss_hfl,
                'test_acc': acc_hfl,
                'model_type': 'HFL',
                'level': 'Global',
                'eh_idx': -1  # å…¨å±€æ¨¡å‹æ²¡æœ‰EHç´¢å¼•
            }
            
            print(f"  [HFL Global]: Acc {acc_hfl:.2f}%, Loss {loss_hfl:.4f}")
            
            # --- æ”¶æ•›æ€§æ£€æŸ¥ ---
            print(f"\n[Convergence Check] Epoch {epoch} | EH_Round {t3+1}/{k3}")
            
            # æ£€æŸ¥HFLéšæœºBçŸ©é˜µçš„æ”¶æ•›æ€§
            if not converged_hfl_random:
                hfl_random_converged_count = 0
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_random):
                    if eh_weights is not None and eh_idx in eh_testsets_random:
                        # æ‰¾åˆ°å¯¹åº”çš„æµ‹è¯•ç»“æœ
                        eh_loss = None
                        eh_acc = None
                        for result in eh_results_random:
                            if result['eh_idx'] == eh_idx:
                                eh_loss = result['test_loss']
                                eh_acc = result['test_acc']
                                break
                        
                        if eh_loss is not None and eh_acc is not None:
                            should_stop, reason = eh_checkers_random[eh_idx].check(eh_loss, eh_acc, epoch)
                            print(f"  [Random] EH {eh_idx}: {reason}")
                            if should_stop:
                                hfl_random_converged_count += 1
                
                # å¦‚æœæ‰€æœ‰EHéƒ½æ”¶æ•›ï¼Œåˆ™æ•´ä¸ªHFLéšæœºBçŸ©é˜µæœºåˆ¶æ”¶æ•›
                active_ehs_random = len([eh for eh in EHs_ws_hfl_random if eh is not None])
                if hfl_random_converged_count == active_ehs_random and active_ehs_random > 0:
                    converged_hfl_random = True
                    print(f"  ğŸ¯ [Random] HFLéšæœºBçŸ©é˜µæœºåˆ¶å·²æ”¶æ•›ï¼æ‰€æœ‰ {active_ehs_random} ä¸ªEHéƒ½æ»¡è¶³æ”¶æ•›æ¡ä»¶")
                else:
                    print(f"  [Random] æ”¶æ•›è¿›åº¦: {hfl_random_converged_count}/{active_ehs_random} EHå·²æ”¶æ•›")
            
            # æ£€æŸ¥HFLèšç±»BçŸ©é˜µçš„æ”¶æ•›æ€§
            if not converged_hfl_cluster:
                hfl_cluster_converged_count = 0
                for eh_idx, eh_weights in enumerate(EHs_ws_hfl_cluster):
                    if eh_weights is not None and eh_idx in eh_testsets_cluster:
                        # æ‰¾åˆ°å¯¹åº”çš„æµ‹è¯•ç»“æœ
                        eh_loss = None
                        eh_acc = None
                        for result in eh_results_cluster:
                            if result['eh_idx'] == eh_idx:
                                eh_loss = result['test_loss']
                                eh_acc = result['test_acc']
                                break
                        
                        if eh_loss is not None and eh_acc is not None:
                            should_stop, reason = eh_checkers_cluster[eh_idx].check(eh_loss, eh_acc, epoch)
                            print(f"  [Cluster] EH {eh_idx}: {reason}")
                            if should_stop:
                                hfl_cluster_converged_count += 1
                
                # å¦‚æœæ‰€æœ‰EHéƒ½æ”¶æ•›ï¼Œåˆ™æ•´ä¸ªHFLèšç±»BçŸ©é˜µæœºåˆ¶æ”¶æ•›
                active_ehs_cluster = len([eh for eh in EHs_ws_hfl_cluster if eh is not None])
                if hfl_cluster_converged_count == active_ehs_cluster and active_ehs_cluster > 0:
                    converged_hfl_cluster = True
                    print(f"  ğŸ¯ [Cluster] HFLèšç±»BçŸ©é˜µæœºåˆ¶å·²æ”¶æ•›ï¼æ‰€æœ‰ {active_ehs_cluster} ä¸ªEHéƒ½æ»¡è¶³æ”¶æ•›æ¡ä»¶")
                else:
                    print(f"  [Cluster] æ”¶æ•›è¿›åº¦: {hfl_cluster_converged_count}/{active_ehs_cluster} EHå·²æ”¶æ•›")
            
            # æ£€æŸ¥HFLä¸¤å±‚ç»“æ„çš„æ”¶æ•›æ€§
            if not converged_hfl:
                should_stop, reason = hfl_checker.check(loss_hfl, acc_hfl, epoch)
                print(f"  [HFL] {reason}")
                if should_stop:
                    converged_hfl = True
                    print(f"  ğŸ¯ [HFL] HFLä¸¤å±‚ç»“æ„æœºåˆ¶å·²æ”¶æ•›ï¼")
            
            # å°†EHæµ‹è¯•ç»“æœæ·»åŠ åˆ°ç»“æœå†å²ä¸­
            results_history.extend(eh_results_random)
            results_history.extend(eh_results_cluster)
            results_history.append(hfl_result)
            
            # æ¯æ¬¡EHæµ‹è¯•åæ›´æ–°CSVæ–‡ä»¶
            save_results_to_csv(results_history, csv_filename)

        # HFL å…¨å±€èšåˆ (EH -> Cloud) - åªå¯¹æœªæ”¶æ•›çš„æœºåˆ¶è¿›è¡Œèšåˆ
        if not converged_hfl_random:
            # print(f"  ğŸ“Š [EH->Cloud] HFLéšæœº: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(eh_data_counts_random)}ä¸ªEHæ•°æ®é‡)")
            w_glob_hfl_random = FedAvg(EHs_ws_hfl_random, eh_data_counts_random)
            net_glob_hfl_random.load_state_dict(w_glob_hfl_random)
            t_hfl_random += t_eh_to_cloud_random
        else:
            print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€èšåˆ")
        
        if not converged_hfl_cluster:
            # print(f"  ğŸ“Š [EH->Cloud] HFLèšç±»: ä½¿ç”¨åŠ æƒå¹³å‡èšåˆ (åŸºäº{len(eh_data_counts_cluster)}ä¸ªEHæ•°æ®é‡)")
            w_glob_hfl_cluster = FedAvg(EHs_ws_hfl_cluster, eh_data_counts_cluster)
            net_glob_hfl_cluster.load_state_dict(w_glob_hfl_cluster)
            t_hfl_design += t_eh_to_cloud_design
        else:
            print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œè·³è¿‡å…¨å±€èšåˆ")

        # --- åœ¨æ¯ä¸ª EPOCH ç»“æŸæ—¶è¿›è¡Œæµ‹è¯• - åªæµ‹è¯•æœªæ”¶æ•›çš„æœºåˆ¶ ---
        print(f"\n[End of Epoch {epoch}] æµ‹è¯•å…¨å±€æ¨¡å‹æ€§èƒ½...")
        
        # è¯„ä¼° HFL éšæœºBæ¨¡å‹
        if not converged_hfl_random:
            net_glob_hfl_random.eval()
            acc_hfl_random, loss_hfl_random = test_img(net_glob_hfl_random, dataset_test, args)
            acc_test_hfl_random.append(acc_hfl_random)
            loss_test_hfl_random.append(loss_hfl_random)
        else:
            # ä½¿ç”¨ä¸Šä¸€è½®ç»“æœä½œä¸ºå ä½ç¬¦
            acc_hfl_random = acc_test_hfl_random[-1] if acc_test_hfl_random else 0.0
            loss_hfl_random = loss_test_hfl_random[-1] if loss_test_hfl_random else 0.0
            acc_test_hfl_random.append(acc_hfl_random)
            loss_test_hfl_random.append(loss_hfl_random)
            print(f"  [Skip] HFLéšæœºBçŸ©é˜µå·²æ”¶æ•›ï¼Œä½¿ç”¨ä¸Šä¸€è½®ç»“æœ")

        # è¯„ä¼° HFL èšç±»Bæ¨¡å‹
        if not converged_hfl_cluster:
            net_glob_hfl_cluster.eval()
            acc_hfl_cluster, loss_hfl_cluster = test_img(net_glob_hfl_cluster, dataset_test, args)
            acc_test_hfl_cluster.append(acc_hfl_cluster)
            loss_test_hfl_cluster.append(loss_hfl_cluster)
        else:
            # ä½¿ç”¨ä¸Šä¸€è½®ç»“æœä½œä¸ºå ä½ç¬¦
            acc_hfl_cluster = acc_test_hfl_cluster[-1] if acc_test_hfl_cluster else 0.0
            loss_hfl_cluster = loss_test_hfl_cluster[-1] if loss_test_hfl_cluster else 0.0
            acc_test_hfl_cluster.append(acc_hfl_cluster)
            loss_test_hfl_cluster.append(loss_hfl_cluster)
            print(f"  [Skip] HFLèšç±»BçŸ©é˜µå·²æ”¶æ•›ï¼Œä½¿ç”¨ä¸Šä¸€è½®ç»“æœ")

        # è¯„ä¼° HFL ä¸¤å±‚ç»“æ„æ¨¡å‹
        if not converged_hfl:
            net_glob_hfl.eval()
            acc_hfl, loss_hfl = test_img(net_glob_hfl, dataset_test, args)
            acc_test_hfl.append(acc_hfl)
            loss_test_hfl.append(loss_hfl)
        else:
            # ä½¿ç”¨ä¸Šä¸€è½®ç»“æœä½œä¸ºå ä½ç¬¦
            acc_hfl = acc_test_hfl[-1] if acc_test_hfl else 0.0
            loss_hfl = loss_test_hfl[-1] if loss_test_hfl else 0.0
            acc_test_hfl.append(acc_hfl)
            loss_test_hfl.append(loss_hfl)
            print(f"  [Skip] HFLä¸¤å±‚ç»“æ„å·²æ”¶æ•›ï¼Œä½¿ç”¨ä¸Šä¸€è½®ç»“æœ")

        # è®°å½•å½“å‰epochçš„ç»“æœ - æ–°æ ¼å¼
        current_epoch_results = [
            {
                'epoch': epoch,
                'eh_round': k3,  # å®Œæ•´çš„EHè½®æ¬¡
                'es_round': k2,  # å®Œæ•´çš„ESè½®æ¬¡
                'train_loss': loss_avg_hfl_random,
                'test_loss': loss_hfl_random,
                'test_acc': acc_hfl_random,
                'model_type': 'HFL_Random_B',
                'level': 'Global',
                'eh_idx': -1
            },
            {
                'epoch': epoch,
                'eh_round': k3,  # å®Œæ•´çš„EHè½®æ¬¡
                'es_round': k2,  # å®Œæ•´çš„ESè½®æ¬¡
                'train_loss': loss_avg_hfl_cluster,
                'test_loss': loss_hfl_cluster,
                'test_acc': acc_hfl_cluster,
                'model_type': 'HFL_Cluster_B',
                'level': 'Global',
                'eh_idx': -1
            },
            {
                'epoch': epoch,
                'eh_round': k3,  # å®Œæ•´çš„EHè½®æ¬¡
                'es_round': k2,  # å®Œæ•´çš„ESè½®æ¬¡
                'train_loss': loss_avg_hfl,
                'test_loss': loss_hfl,
                'test_acc': acc_hfl,
                'model_type': 'HFL',
                'level': 'Global',
                'eh_idx': -1
            }
        ]
        
        results_history.extend(current_epoch_results)

        # ä¿å­˜ç»“æœåˆ°CSV
        save_results_to_csv(results_history, csv_filename)

        # æ‰“å°å½“å‰ EPOCH ç»“æŸæ—¶çš„æµ‹è¯•ç»“æœ
        print(f'\nEpoch {epoch} [END OF EPOCH TEST]')
        print(f'HFL_Random: Acc {acc_hfl_random:.2f}%, Loss {loss_hfl_random:.4f}')
        print(f'HFL_Cluster: Acc {acc_hfl_cluster:.2f}%, Loss {loss_hfl_cluster:.4f}')
        print(f'HFL: Acc {acc_hfl:.2f}%, Loss {loss_hfl:.4f}')

        # --- æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æœºåˆ¶éƒ½å·²æ”¶æ•› ---
        if converged_hfl_random and converged_hfl_cluster and converged_hfl:
            print(f"\nğŸ‰ æ‰€æœ‰è”é‚¦å­¦ä¹ æœºåˆ¶éƒ½å·²æ”¶æ•›ï¼æå‰ç»“æŸè®­ç»ƒã€‚")
            print(f"å®é™…è®­ç»ƒè½®æ¬¡: {epoch + 1}/{args.epochs}")
            final_epoch = epoch + 1
            break
        else:
            print(f"\nğŸ“Š æ”¶æ•›çŠ¶æ€: HFL_Random={'âœ…' if converged_hfl_random else 'âŒ'}, "
                  f"HFL_Cluster={'âœ…' if converged_hfl_cluster else 'âŒ'}, "
                  f"HFL={'âœ…' if converged_hfl else 'âŒ'}")

        net_glob_hfl_random.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
        net_glob_hfl_cluster.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
        net_glob_hfl.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼

    # =====================================================================================
    # Final Testing - æµ‹è¯•ä¸‰ç§æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½
    # =====================================================================================
    print("\n--- Final Model Evaluation ---")
    print(f"æœ€ç»ˆæ”¶æ•›çŠ¶æ€:")
    print(f"  HFL_Random: {'å·²æ”¶æ•› âœ…' if converged_hfl_random else 'æœªæ”¶æ•› âŒ'}")
    print(f"  HFL_Cluster: {'å·²æ”¶æ•› âœ…' if converged_hfl_cluster else 'æœªæ”¶æ•› âŒ'}")
    print(f"  HFL: {'å·²æ”¶æ•› âœ…' if converged_hfl else 'æœªæ”¶æ•› âŒ'}")
    print(f"å®é™…è®­ç»ƒè½®æ¬¡: {final_epoch}/{args.epochs}")
    
    # æµ‹è¯• HFL éšæœºBçŸ©é˜µæ¨¡å‹ï¼ˆä¸‰å±‚ï¼‰
    net_glob_hfl_random.eval()
    acc_train_hfl_random, loss_train_final_hfl_random = test_img(net_glob_hfl_random, dataset_train, args)
    acc_test_final_hfl_random, loss_test_final_hfl_random = test_img(net_glob_hfl_random, dataset_test, args)
    converged_str = "å·²æ”¶æ•›" if converged_hfl_random else "æœªæ”¶æ•›"
    print(f"HFL Model (Random B Matrix, 3-layer) [{converged_str}] - Training accuracy: {acc_train_hfl_random:.2f}%, Testing accuracy: {acc_test_final_hfl_random:.2f}%")
    
    # æµ‹è¯• HFL èšç±»BçŸ©é˜µæ¨¡å‹ï¼ˆä¸‰å±‚ï¼‰
    net_glob_hfl_cluster.eval()
    acc_train_hfl_cluster, loss_train_final_hfl_cluster = test_img(net_glob_hfl_cluster, dataset_train, args)
    acc_test_final_hfl_cluster, loss_test_final_hfl_cluster = test_img(net_glob_hfl_cluster, dataset_test, args)
    converged_str = "å·²æ”¶æ•›" if converged_hfl_cluster else "æœªæ”¶æ•›"
    print(f"HFL Model (Clustered B Matrix, 3-layer) [{converged_str}] - Training accuracy: {acc_train_hfl_cluster:.2f}%, Testing accuracy: {acc_test_final_hfl_cluster:.2f}%")

    # æµ‹è¯• HFL ä¸¤å±‚ç»“æ„æ¨¡å‹
    net_glob_hfl.eval()
    acc_train_hfl, loss_train_final_hfl = test_img(net_glob_hfl, dataset_train, args)
    acc_test_final_hfl, loss_test_final_hfl = test_img(net_glob_hfl, dataset_test, args)
    converged_str = "å·²æ”¶æ•›" if converged_hfl else "æœªæ”¶æ•›"
    print(f"HFL Model (2-layer) [{converged_str}] - Training accuracy: {acc_train_hfl:.2f}%, Testing accuracy: {acc_test_final_hfl:.2f}%")

    # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆæ·»åŠ åˆ°ç»“æœå†å²åˆ—è¡¨ä¸­ï¼‰
    # final_epoch å·²åœ¨å‰é¢å®šä¹‰ï¼Œæ­¤å¤„ä¸éœ€è¦é‡å¤å®šä¹‰
    
    # æ·»åŠ æœ€ç»ˆç»“æœåˆ°ç»“æœå†å²åˆ—è¡¨
    final_results = [
        {
            'epoch': final_epoch,
            'eh_round': k3,
            'es_round': k2,
            'train_loss': loss_train_final_hfl_random,
            'test_loss': loss_test_final_hfl_random,
            'test_acc': acc_test_final_hfl_random,
            'model_type': 'HFL_Random_B',
            'level': 'Final',
            'eh_idx': -1
        },
        {
            'epoch': final_epoch,
            'eh_round': k3,
            'es_round': k2,
            'train_loss': loss_train_final_hfl_cluster,
            'test_loss': loss_test_final_hfl_cluster,
            'test_acc': acc_test_final_hfl_cluster,
            'model_type': 'HFL_Cluster_B',
            'level': 'Final',
            'eh_idx': -1
        },
        {
            'epoch': final_epoch,
            'eh_round': k3,
            'es_round': k2,
            'train_loss': loss_train_final_hfl,
            'test_loss': loss_test_final_hfl,
            'test_acc': acc_test_final_hfl,
            'model_type': 'HFL',
            'level': 'Final',
            'eh_idx': -1
        }
    ]
    
    # å°†æœ€ç»ˆç»“æœæ·»åŠ åˆ°ç»“æœå†å²ä¸­
    results_history.extend(final_results)
    
    # é‡æ–°ä¿å­˜æ•´ä¸ªç»“æœå†å²åˆ°CSVæ–‡ä»¶
    save_results_to_csv(results_history, csv_filename)
    
    # å°†æœ€ç»ˆç»“æœå•ç‹¬ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
    final_summary = [
        {
            'model_type': 'HFL_Random_B',
            'final_train_acc': acc_train_hfl_random,
            'final_train_loss': loss_train_final_hfl_random,
            'final_test_acc': acc_test_final_hfl_random,
            'final_test_loss': loss_test_final_hfl_random
        },
        {
            'model_type': 'HFL_Cluster_B',
            'final_train_acc': acc_train_hfl_cluster,
            'final_train_loss': loss_train_final_hfl_cluster,
            'final_test_acc': acc_test_final_hfl_cluster,
            'final_test_loss': loss_test_final_hfl_cluster
        },
        {
            'model_type': 'HFL',
            'final_train_acc': acc_train_hfl,
            'final_train_loss': loss_train_final_hfl,
            'final_test_acc': acc_test_final_hfl,
            'final_test_loss': loss_test_final_hfl
        }
    ]

    # å°†æœ€ç»ˆæ±‡æ€»ç»“æœè¿½åŠ åˆ°CSVæ–‡ä»¶
    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([])  # ç©ºè¡Œåˆ†éš”
        writer.writerow(['Final Summary'])
        writer.writerow(['model_type', 'final_train_acc', 'final_train_loss', 'final_test_acc', 'final_test_loss'])
        for result in final_summary:
            writer.writerow([result['model_type'], result['final_train_acc'], 
                            result['final_train_loss'], result['final_test_acc'], result['final_test_loss']])

    print(f"\n=== è®­ç»ƒå®Œæˆ ===")
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {csv_filename}")
    
    try:
        print("\næ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        comprehensive_file, summary_file = create_enhanced_visualizations(csv_filename)
        print(f"ç»¼åˆå¯¹æ¯”å›¾: {comprehensive_file}")
        print(f"æ€§èƒ½æ€»ç»“è¡¨: {summary_file}")
    except Exception as e:
        print(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–æ‰‹åŠ¨è¿è¡Œvisualization_tool.py")
    
    # ä¿å­˜é€šä¿¡æ—¶é—´ç»“æœåˆ°å•ç‹¬çš„CSVæ–‡ä»¶
    communication_filename = f"communication_results_{timestamp}.csv"
    communication_csv_path = f'./results/{communication_filename}'
    
    with open(communication_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['model_type', 'total_communication_time', 'epochs', 'avg_communication_per_epoch'])
        writer.writerow(['HFL_Random_B', f"{t_hfl_random:.6f}", final_epoch, f"{t_hfl_random/final_epoch:.6f}"])
        writer.writerow(['HFL_Cluster_B', f"{t_hfl_design:.6f}", final_epoch, f"{t_hfl_design/final_epoch:.6f}"])
        writer.writerow(['HFL', f"{t_hfl:.6f}", final_epoch, f"{t_hfl/final_epoch:.6f}"])
    
    print(f"é€šä¿¡æ—¶é—´ç»“æœå·²ä¿å­˜åˆ°: {communication_csv_path}")

    print(f"\n=== å®éªŒæ€»ç»“ ===")
    print("æœ¬æ¬¡å®éªŒå¯¹æ¯”äº†ä¸‰ç§è”é‚¦å­¦ä¹ æ–¹æ³•:")
    print("1. HFL (Random B Matrix, 3-layer) - ä½¿ç”¨éšæœºç”Ÿæˆçš„ES-EHå…³è”çŸ©é˜µçš„ä¸‰å±‚ç»“æ„")
    print("2. HFL (Clustered B Matrix, 3-layer) - ä½¿ç”¨è°±èšç±»ç”Ÿæˆçš„ES-EHå…³è”çŸ©é˜µçš„ä¸‰å±‚ç»“æ„") 
    print("3. HFL (2-layer) - ä¸¤å±‚è”é‚¦å­¦ä¹ ï¼Œå®¢æˆ·ç«¯-è¾¹ç¼˜æœåŠ¡å™¨-äº‘ç«¯")
    print(f"è®­ç»ƒå‚æ•°: è®¾å®šepochs={args.epochs}, å®é™…epochs={final_epoch}, clients={args.num_users}, local_epochs={args.local_ep}")
    print(f"å±‚çº§å‚æ•°: k2={args.ES_k2} (ESå±‚èšåˆè½®æ•°), k3={args.EH_k3} (EHå±‚èšåˆè½®æ•°)")
    print(f"å¹¶è¡Œå‚æ•°: num_processes={args.num_processes}")
    print(f"æ•°æ®é›†: {args.dataset}, æ¨¡å‹: {args.model}, IID: {args.iid}")
    if not args.iid:
        print(f"éIIDå‚æ•°: beta={args.beta}")
    
    print(f"\n=== é€šä¿¡å¼€é”€åˆ†æ ===")
    print(f"æ€»é€šä¿¡æ—¶é—´å¯¹æ¯”:")
    print(f"  â€¢ HFL_Random (3-layer): {t_hfl_random:.6f}s (å¹³å‡æ¯è½®: {t_hfl_random/final_epoch:.6f}s)")
    print(f"  â€¢ HFL_Cluster (3-layer): {t_hfl_design:.6f}s (å¹³å‡æ¯è½®: {t_hfl_design/final_epoch:.6f}s)")
    print(f"  â€¢ HFL (2-layer): {t_hfl:.6f}s (å¹³å‡æ¯è½®: {t_hfl/final_epoch:.6f}s)")
    
    print(f"\n=== æ”¶æ•›æ€§åˆ†æ ===")
    print(f"æ”¶æ•›æ£€æŸ¥å™¨å‚æ•°: patience=5, min_delta=0.001")
    print(f"æœ€ç»ˆæ”¶æ•›çŠ¶æ€:")
    print(f"  â€¢ HFL_Random (3-layer): {'âœ… å·²æ”¶æ•›' if converged_hfl_random else 'âŒ æœªæ”¶æ•›'}")
    print(f"  â€¢ HFL_Cluster (3-layer): {'âœ… å·²æ”¶æ•›' if converged_hfl_cluster else 'âŒ æœªæ”¶æ•›'}")  
    print(f"  â€¢ HFL (2-layer): {'âœ… å·²æ”¶æ•›' if converged_hfl else 'âŒ æœªæ”¶æ•›'}")
    
    if converged_hfl_random and converged_hfl_cluster and converged_hfl:
        print(f"ğŸ‰ æ‰€æœ‰æœºåˆ¶å‡æ”¶æ•›ï¼Œè®­ç»ƒåœ¨ç¬¬{final_epoch}è½®æå‰ç»“æŸ")
    elif final_epoch < args.epochs:
        print(f"âš ï¸ éƒ¨åˆ†æœºåˆ¶æ”¶æ•›ï¼Œè®­ç»ƒåœ¨ç¬¬{final_epoch}è½®æå‰ç»“æŸ")
    else:
        print(f"â° è®­ç»ƒå®Œæˆè®¾å®šçš„{args.epochs}è½®ï¼Œéƒ¨åˆ†æœºåˆ¶å¯èƒ½æœªå®Œå…¨æ”¶æ•›")
    
    # æ˜¾ç¤ºæœ€ç»ˆæ€§èƒ½å¯¹æ¯”
    try:
        df = pd.read_csv(csv_filename, encoding='utf-8')
        final_results = df[df['epoch'] == df['epoch'].max()]
        print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”:")
        for _, row in final_results.iterrows():
            model_name = row['model_type'].replace('_', ' ')
            print(f"  {model_name}: {row['test_acc']:.2f}%")
    except:
        pass
